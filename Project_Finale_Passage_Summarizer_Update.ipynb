{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Project Finale Passage Summarizer Update.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "fd87f87df6d04005a944c292fd32d0a0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_11f47d598b7a45699344cae1552f748f",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_cebe63a106124624962667640859eaeb",
              "IPY_MODEL_62507284346e447fb2681cff30b499ea"
            ]
          }
        },
        "11f47d598b7a45699344cae1552f748f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "cebe63a106124624962667640859eaeb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_39ab9c2381cf4eb29f590153192030d2",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1621,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1621,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_7f983040806049ea92a55f02d091840d"
          }
        },
        "62507284346e447fb2681cff30b499ea": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_36a5c27f190b4d8fbac81d93cef2725e",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1.62k/1.62k [00:25&lt;00:00, 62.5B/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_69f64b9617ca4183bf05cfc02db77538"
          }
        },
        "39ab9c2381cf4eb29f590153192030d2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "7f983040806049ea92a55f02d091840d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "36a5c27f190b4d8fbac81d93cef2725e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "69f64b9617ca4183bf05cfc02db77538": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "7bf879027cf642aaa7e0ff07a7a72151": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_3739be4905c34d96869e334ff0edb82a",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_383777a27bff479c9c55c9d1af56280a",
              "IPY_MODEL_c7393925efd3402590c58daac5ed0e29"
            ]
          }
        },
        "3739be4905c34d96869e334ff0edb82a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "383777a27bff479c9c55c9d1af56280a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_e546d8dd9fcb484b9ac9b7cffe301249",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1222317369,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1222317369,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_a945596f2a6144ddab88b184f3fa94bb"
          }
        },
        "c7393925efd3402590c58daac5ed0e29": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_b1913ab6917b436f824d478cb0c3a54a",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1.22G/1.22G [00:25&lt;00:00, 47.5MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_959b0df3074a43dbae0a6efb8d191a38"
          }
        },
        "e546d8dd9fcb484b9ac9b7cffe301249": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "a945596f2a6144ddab88b184f3fa94bb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "b1913ab6917b436f824d478cb0c3a54a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "959b0df3074a43dbae0a6efb8d191a38": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "vHyAfBmVhqwY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "63a5b47e-4528-4a81-a81a-3b0deef48de2"
      },
      "source": [
        "pip install -U sentence-transformers\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting sentence-transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4f/fd/e5a1c8602605c31aa09ee6633c6ffee7d451506544637be1b2f10c5f29ca/sentence-transformers-1.0.1.tar.gz (74kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 3.1MB/s \n",
            "\u001b[?25hCollecting transformers<5.0.0,>=3.1.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ed/d5/f4157a376b8a79489a76ce6cfe147f4f3be1e029b7144fa7b8432e8acb26/transformers-4.4.2-py3-none-any.whl (2.0MB)\n",
            "\u001b[K     |████████████████████████████████| 2.0MB 8.5MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: tqdm in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (4.41.1)\n",
            "Requirement already satisfied, skipping upgrade: torch>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.8.0+cu101)\n",
            "Requirement already satisfied, skipping upgrade: numpy in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.19.5)\n",
            "Requirement already satisfied, skipping upgrade: scikit-learn in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (0.22.2.post1)\n",
            "Requirement already satisfied, skipping upgrade: scipy in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.4.1)\n",
            "Requirement already satisfied, skipping upgrade: nltk in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (3.2.5)\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f5/99/e0808cb947ba10f575839c43e8fafc9cc44e4a7a2c8f79c60db48220a577/sentencepiece-0.1.95-cp37-cp37m-manylinux2014_x86_64.whl (1.2MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2MB 29.6MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: packaging in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=3.1.0->sentence-transformers) (20.9)\n",
            "Requirement already satisfied, skipping upgrade: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=3.1.0->sentence-transformers) (3.7.2)\n",
            "Requirement already satisfied, skipping upgrade: requests in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=3.1.0->sentence-transformers) (2.23.0)\n",
            "Requirement already satisfied, skipping upgrade: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=3.1.0->sentence-transformers) (2019.12.20)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/71/23/2ddc317b2121117bf34dd00f5b0de194158f2a44ee2bf5e47c7166878a97/tokenizers-0.10.1-cp37-cp37m-manylinux2010_x86_64.whl (3.2MB)\n",
            "\u001b[K     |████████████████████████████████| 3.2MB 26.2MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 47.1MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: filelock in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=3.1.0->sentence-transformers) (3.0.12)\n",
            "Requirement already satisfied, skipping upgrade: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.6.0->sentence-transformers) (3.7.4.3)\n",
            "Requirement already satisfied, skipping upgrade: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sentence-transformers) (1.0.1)\n",
            "Requirement already satisfied, skipping upgrade: six in /usr/local/lib/python3.7/dist-packages (from nltk->sentence-transformers) (1.15.0)\n",
            "Requirement already satisfied, skipping upgrade: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers<5.0.0,>=3.1.0->sentence-transformers) (2.4.7)\n",
            "Requirement already satisfied, skipping upgrade: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers<5.0.0,>=3.1.0->sentence-transformers) (3.4.1)\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=3.1.0->sentence-transformers) (2020.12.5)\n",
            "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=3.1.0->sentence-transformers) (1.24.3)\n",
            "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=3.1.0->sentence-transformers) (2.10)\n",
            "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=3.1.0->sentence-transformers) (3.0.4)\n",
            "Requirement already satisfied, skipping upgrade: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers<5.0.0,>=3.1.0->sentence-transformers) (7.1.2)\n",
            "Building wheels for collected packages: sentence-transformers, sacremoses\n",
            "  Building wheel for sentence-transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sentence-transformers: filename=sentence_transformers-1.0.1-cp37-none-any.whl size=114179 sha256=09f5ca48c5f07769fca243a8def93466fee375534ac9b4d143878ae32c65fd78\n",
            "  Stored in directory: /root/.cache/pip/wheels/e8/92/70/49ac4d76a2374f6dac8b1e5cfd69c088af43aee54c2fb2b9f4\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp37-none-any.whl size=893262 sha256=02c1be4c27a9bc654ffcb4d6e193bbc6e53b17a9a0b88a176eee0e4edd4d75bf\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sentence-transformers sacremoses\n",
            "Installing collected packages: tokenizers, sacremoses, transformers, sentencepiece, sentence-transformers\n",
            "Successfully installed sacremoses-0.0.43 sentence-transformers-1.0.1 sentencepiece-0.1.95 tokenizers-0.10.1 transformers-4.4.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fmMVBizNTj_8",
        "outputId": "68d44d03-f41e-4933-c938-938bb10858aa"
      },
      "source": [
        "pip install transformers torch"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.4.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (1.8.0+cu101)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.43)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (3.7.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch) (3.7.4.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2020.12.5)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pY-nLZWwb6r_",
        "outputId": "c428644c-4096-496e-e9f0-b6cde0530fed"
      },
      "source": [
        "pip install bert-extractive-summarizer"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting bert-extractive-summarizer\n",
            "  Downloading https://files.pythonhosted.org/packages/1a/07/fdb05f9e18b6f641499ef56737126fbd2fafe1cdc1a04ba069d5aa205901/bert_extractive_summarizer-0.7.1-py3-none-any.whl\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (from bert-extractive-summarizer) (4.4.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from bert-extractive-summarizer) (0.22.2.post1)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.7/dist-packages (from bert-extractive-summarizer) (2.2.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers->bert-extractive-summarizer) (20.9)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers->bert-extractive-summarizer) (1.19.5)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers->bert-extractive-summarizer) (3.7.2)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers->bert-extractive-summarizer) (0.10.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers->bert-extractive-summarizer) (3.0.12)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers->bert-extractive-summarizer) (0.0.43)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers->bert-extractive-summarizer) (2.23.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers->bert-extractive-summarizer) (2019.12.20)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers->bert-extractive-summarizer) (4.41.1)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->bert-extractive-summarizer) (1.4.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->bert-extractive-summarizer) (1.0.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy->bert-extractive-summarizer) (54.0.0)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy->bert-extractive-summarizer) (7.4.0)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy->bert-extractive-summarizer) (0.4.1)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy->bert-extractive-summarizer) (1.1.3)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy->bert-extractive-summarizer) (3.0.5)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy->bert-extractive-summarizer) (1.0.5)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy->bert-extractive-summarizer) (1.0.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy->bert-extractive-summarizer) (2.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy->bert-extractive-summarizer) (1.0.5)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy->bert-extractive-summarizer) (0.8.2)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers->bert-extractive-summarizer) (2.4.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers->bert-extractive-summarizer) (3.4.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers->bert-extractive-summarizer) (3.7.4.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers->bert-extractive-summarizer) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers->bert-extractive-summarizer) (7.1.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers->bert-extractive-summarizer) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers->bert-extractive-summarizer) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers->bert-extractive-summarizer) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers->bert-extractive-summarizer) (3.0.4)\n",
            "Installing collected packages: bert-extractive-summarizer\n",
            "Successfully installed bert-extractive-summarizer-0.7.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W2Nyax5cst-o",
        "outputId": "2fb96b49-21e3-4542-a5e2-18d0e03a6ec5"
      },
      "source": [
        "pip install sumy"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting sumy\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/61/20/8abf92617ec80a2ebaec8dc1646a790fc9656a4a4377ddb9f0cc90bc9326/sumy-0.8.1-py2.py3-none-any.whl (83kB)\n",
            "\r\u001b[K     |████                            | 10kB 14.4MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 20kB 10.3MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 30kB 6.7MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 40kB 3.1MB/s eta 0:00:01\r\u001b[K     |███████████████████▌            | 51kB 3.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 61kB 4.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▍    | 71kB 4.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▎| 81kB 4.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 92kB 3.4MB/s \n",
            "\u001b[?25hCollecting breadability>=0.1.20\n",
            "  Downloading https://files.pythonhosted.org/packages/ad/2d/bb6c9b381e6b6a432aa2ffa8f4afdb2204f1ff97cfcc0766a5b7683fec43/breadability-0.1.20.tar.gz\n",
            "Collecting pycountry>=18.2.23\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/76/73/6f1a412f14f68c273feea29a6ea9b9f1e268177d32e0e69ad6790d306312/pycountry-20.7.3.tar.gz (10.1MB)\n",
            "\u001b[K     |████████████████████████████████| 10.1MB 17.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests>=2.7.0 in /usr/local/lib/python3.7/dist-packages (from sumy) (2.23.0)\n",
            "Requirement already satisfied: nltk>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from sumy) (3.2.5)\n",
            "Requirement already satisfied: docopt<0.7,>=0.6.1 in /usr/local/lib/python3.7/dist-packages (from sumy) (0.6.2)\n",
            "Requirement already satisfied: chardet in /usr/local/lib/python3.7/dist-packages (from breadability>=0.1.20->sumy) (3.0.4)\n",
            "Requirement already satisfied: lxml>=2.0 in /usr/local/lib/python3.7/dist-packages (from breadability>=0.1.20->sumy) (4.2.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.7.0->sumy) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.7.0->sumy) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.7.0->sumy) (1.24.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk>=3.0.2->sumy) (1.15.0)\n",
            "Building wheels for collected packages: breadability, pycountry\n",
            "  Building wheel for breadability (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for breadability: filename=breadability-0.1.20-py2.py3-none-any.whl size=21680 sha256=b8a3e2c22eecee67192d5fbc67a20cafe7e3aa660b520e458437783d65fb979f\n",
            "  Stored in directory: /root/.cache/pip/wheels/5a/4d/a1/510b12c5e65e0b2b3ce539b2af66da0fc57571e528924f4a52\n",
            "  Building wheel for pycountry (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pycountry: filename=pycountry-20.7.3-py2.py3-none-any.whl size=10746863 sha256=db4cf23200737c2235a59bb2d91f5a0a37e4cf663afaef4579b2880ade24c77b\n",
            "  Stored in directory: /root/.cache/pip/wheels/33/4e/a6/be297e6b83567e537bed9df4a93f8590ec01c1acfbcd405348\n",
            "Successfully built breadability pycountry\n",
            "Installing collected packages: breadability, pycountry, sumy\n",
            "Successfully installed breadability-0.1.20 pycountry-20.7.3 sumy-0.8.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bedBcREu5wru",
        "outputId": "6753c4e5-f044-4072-891c-a1a3f058a60f"
      },
      "source": [
        "pip install line-profiler"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting line-profiler\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/da/ad/30ef6e8c8f3d4b4e08a197c7bf4154f001d14684fba2b693d813c2b4c692/line_profiler-3.1.0-cp37-cp37m-manylinux2010_x86_64.whl (63kB)\n",
            "\r\u001b[K     |█████▏                          | 10kB 16.3MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 20kB 12.2MB/s eta 0:00:01\r\u001b[K     |███████████████▌                | 30kB 8.4MB/s eta 0:00:01\r\u001b[K     |████████████████████▊           | 40kB 7.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▉      | 51kB 4.2MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 61kB 4.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 71kB 3.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: IPython in /usr/local/lib/python3.7/dist-packages (from line-profiler) (5.5.0)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.7/dist-packages (from IPython->line-profiler) (0.8.1)\n",
            "Requirement already satisfied: pexpect; sys_platform != \"win32\" in /usr/local/lib/python3.7/dist-packages (from IPython->line-profiler) (4.8.0)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.7/dist-packages (from IPython->line-profiler) (54.0.0)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from IPython->line-profiler) (0.7.5)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from IPython->line-profiler) (4.4.2)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.7/dist-packages (from IPython->line-profiler) (5.0.5)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.7/dist-packages (from IPython->line-profiler) (1.0.18)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from IPython->line-profiler) (2.6.1)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.7/dist-packages (from pexpect; sys_platform != \"win32\"->IPython->line-profiler) (0.7.0)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.7/dist-packages (from traitlets>=4.2->IPython->line-profiler) (0.2.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->IPython->line-profiler) (0.2.5)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->IPython->line-profiler) (1.15.0)\n",
            "Installing collected packages: line-profiler\n",
            "Successfully installed line-profiler-3.1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DQXFOAPhTg1F"
      },
      "source": [
        "from nltk.cluster.util import cosine_distance\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import networkx as nx\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import requests\n",
        "from bs4 import BeautifulSoup"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EViJhEP6vjFX"
      },
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.metrics.pairwise import euclidean_distances"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I8kF--VcYU9t"
      },
      "source": [
        "import requests\n",
        "#import RAKE\n",
        "import operator\n",
        "import timeit\n",
        "import time\n",
        "API_KEY = \"AIzaSyAvZlAZHcE3lXigI2ZJxWQvCoCpF6qFiyY\"\n",
        "SEARCH_ENGINE_ID = \"bf6585a79f0c6d8bb\""
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "99rlbZ2UYhDc"
      },
      "source": [
        "def google_search(api_key, search_engine, query, page_number=1):\n",
        "  start = (page_number - 1) * 10 + 1\n",
        "  url = f\"https://www.googleapis.com/customsearch/v1?key={api_key}&cx={search_engine}&q={query}&start={start}\"\n",
        "  data = requests.get(url).json()\n",
        "  return data\n",
        "  "
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6d1RNGg2YxG6"
      },
      "source": [
        "def results(data):\n",
        "  search_items = data.get(\"items\")\n",
        "  for i, search_item in enumerate(search_items, start=1):\n",
        "    title = search_item.get(\"title\")\n",
        "    snippet = search_item.get(\"snippet\")\n",
        "    link = search_item.get(\"link\")\n",
        "    print(\"Title:\", title)\n",
        "    print(\"Description:\", snippet)\n",
        "    print(\"URL:\", link, \"\\n\")\n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oY-GhGTrQRjT"
      },
      "source": [
        "def only_link(data):\n",
        "  search_items = data.get(\"items\")\n",
        "  link=[]\n",
        "  for i, search_item in enumerate(search_items, start=1):\n",
        "    link.append(search_item.get(\"link\")) \n",
        "    #print(\"URL:\", link, \"\\n\")\n",
        "  return  link"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_rvSnzdLY3z4"
      },
      "source": [
        "def Sort (info):\n",
        "  info.sort(key = lambda x: x[1], reverse=True)\n",
        "  return info"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3HisRiSGFKIp"
      },
      "source": [
        "def read_file(files):\n",
        "  myfile = open(files, \"r\")\n",
        "  readfiles=myfile.readlines()\n",
        "  text =\"\"\n",
        "  for line in range(len(readfiles)):\n",
        "    text = text + \" \"+ readfiles[line]\n",
        "  return text \n",
        "\n"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sdQEzUr2i-R4"
      },
      "source": [
        "#building the similarity matrix\n",
        "def sim_matrix(sentences, stop_words):\n",
        "    length =len(sentences)\n",
        "    sim_mat = np.zeros((length, length))\n",
        "    for sen1 in range(length):\n",
        "        for sen2 in range(length):\n",
        "    # Create an empty similarity matrix\n",
        "            if sen1 == sen2: \n",
        "                continue \n",
        "            sim_mat[sen1][sen2] = sentence_similarity(sentences[sen1], sentences[sen2], stop_words)\n",
        "\n",
        "    return sim_mat"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vsJdaCzBiwXR"
      },
      "source": [
        "def sentence_similarity(sent1, sent2, stopwords=None):\n",
        "    if stopwords is None:\n",
        "        stopwords = []\n",
        " \n",
        "    sent1 = [w.lower() for w in sent1]\n",
        "    sent2 = [w.lower() for w in sent2]\n",
        "    all_words = list(set(sent1 + sent2))\n",
        "    vector1 = [0] * len(all_words)\n",
        "    vector2 = [0] * len(all_words)\n",
        " \n",
        "    # build the vector for the first sentence\n",
        "    for w in sent1:\n",
        "        if w in stopwords:\n",
        "            continue\n",
        "        vector1[all_words.index(w)] += 1\n",
        " \n",
        "    # build the vector for the second sentence\n",
        "    for w in sent2:\n",
        "        if w in stopwords:\n",
        "            continue\n",
        "        vector2[all_words.index(w)] += 1\n",
        " \n",
        "    return 1 - cosine_distance(vector1, vector2)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gd4gsONpi3Oy"
      },
      "source": [
        "def generate_summary(sentences,stop_words,top_n=5):\n",
        "    summarize_text = []\n",
        "    summarized_text =\"\"\n",
        "    sentence_similarity_martix = sim_matrix(sentences, stop_words)\n",
        "\n",
        "    sentence_similarity_graph = nx.from_numpy_array(sentence_similarity_martix)\n",
        "    scores = nx.pagerank(sentence_similarity_graph)\n",
        "    print(sentence_similarity_graph)\n",
        "    ranked_sentence = sorted(((scores[i],s) for i,s in enumerate(sentences)), reverse=True)    \n",
        "   # print(\"Indexes of top ranked_sentence order are \", ranked_sentence)    \n",
        "\n",
        "    for i in range(top_n):\n",
        "      summarized_text= summarized_text +\" \"+ ranked_sentence[i][1]\n",
        "\n",
        "    return summarized_text"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MXIvmJ2zokE0"
      },
      "source": [
        "def long_function():\n",
        "    print('function start')\n",
        "    #time.sleep(5)\n",
        "    generate_summary( sentencesevol,stopwords ,3)\n",
        "    print('function end')"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-siPMoAhW0aW",
        "outputId": "c53c958c-f7ec-4128-b7ff-085ae6d0a1aa"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KHoKucDf1Tc9",
        "outputId": "ca977620-69a1-4551-e36f-a342d4c8f854"
      },
      "source": [
        "textkkkk=read_file(\"kkkk.txt\")\n",
        "textatten = read_file(\"Attention.txt\")\n",
        "textevol=read_file(\"Evolution.txt\")\n",
        "texthS = read_file(\"humSym.txt\")\n",
        "textmorals=read_file(\"morals.txt\")\n",
        "textmus = read_file(\"music.txt\")\n",
        "textweb=read_file(\"webscraper.txt\")\n",
        "\n",
        "#print(text)\n",
        "sentenceskkk=sent_tokenize(textkkkk)\n",
        "sentencesAtten=sent_tokenize(textatten)\n",
        "sentencesevol=sent_tokenize(textevol)\n",
        "sentenceshS=sent_tokenize(texthS)\n",
        "sentencesmorals=sent_tokenize(textmorals)\n",
        "sentencesmus=sent_tokenize(textmus)\n",
        "sentencesweb=sent_tokenize(textweb)\n",
        "\n",
        "stopwords =\"SmartStoplist.txt\"\n",
        "print(len(sentencesAtten))"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "294\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NehVq7C7XwO5",
        "outputId": "6d6823b9-58a1-46df-d2a0-013c7f2998eb"
      },
      "source": [
        "print(sentencesAtten)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[' ATTENTION IS ALL YOU NEED IN SPEECH SEPARATION\\n Cem Subakan1\\n , Mirco Ravanelli1\\n , Samuele Cornell2\\n , Mirko Bronzi1\\n , Jianyuan Zhong3\\n 1Mila-Quebec AI Institute, Canada,\\n 2Universita Politecnica delle Marche, Italy `\\n 3University of Rochester, USA\\n ABSTRACT\\n Recurrent Neural Networks (RNNs) have long been the dominant\\n architecture in sequence-to-sequence learning.', 'RNNs, however, are\\n inherently sequential models that do not allow parallelization of their\\n computations.', 'Transformers are emerging as a natural alternative to\\n standard RNNs, replacing recurrent computations with a multi-head\\n attention mechanism.', 'In this paper, we propose the SepFormer, a novel RNN-free\\n Transformer-based neural network for speech separation.', 'The SepFormer learns short and long-term dependencies with a multi-scale\\n approach that employs transformers.', 'The proposed model achieves\\n state-of-the-art (SOTA) performance on the standard WSJ0-2/3mix\\n datasets.', 'It reaches an SI-SNRi of 22.3 dB on WSJ0-2mix and an\\n SI-SNRi of 19.5 dB on WSJ0-3mix.', 'The SepFormer inherits the\\n parallelization advantages of Transformers and achieves a competitive performance even when downsampling the encoded representation by a factor of 8.', 'It is thus significantly faster and it is less\\n memory-demanding than the latest speech separation systems with\\n comparable performance.', 'Index Terms— speech separation, source separation, transformer, attention, deep learning.', '1.', 'INTRODUCTION\\n RNNs are a crucial component of modern audio processing systems and they are used in many different domains, including speech\\n recognition, synthesis, enhancement, and separation, just to name a\\n few.', 'Especially when coupled with multiplicative gate mechanisms\\n (like LSTM [1] and GRU [2, 3]), their recurrent connections are essential to learn long-term dependencies and properly manage speech\\n contexts.', 'Nevertheless, the inherently sequential nature of RNNs\\n impairs an effective parallelization of the computations.', 'This bottleneck is particularly evident when processing large datasets with\\n long sequences.', 'On the other hand, Transformers [4] completely\\n avoid this bottleneck by eliminating recurrence and replacing it with\\n a fully attention-based mechanism.', 'By attending to the whole sequence at once, a direct connection can be established between distant elements allowing Transformers to learn long-term dependencies more easily [5].', 'For this reason, Transformers are gaining considerable popularity for speech processing and recently showed competitive performance in speech recognition [6], synthesis [7], enhancement [8], diarization [9], as well as speaker recognition [10].', 'Little research has been done so far on Transformer-based models for monaural audio source separation.', 'The field has been revolutionized by the adoption of deep learning techniques [11–16], and\\n with recent works [17–23] achieving impressive results by adopting an end-to-end approach.', 'Most of the current speech separation\\n techniques [14, 15, 17–22] require effective modeling of long input\\n x Encoder h Masking Net Decoder\\n sˆ1\\n sˆ2\\n m1\\n m2\\n Fig.', '1.', 'The high-level description of our system: The encoder block\\n estimates a learned-representation for the input signal, while the\\n masking network estimates optimal masks to separate the sources\\n present in the mixtures.', 'The decoder finally reconstructs the estimated sources in the time domain using the masks provided by the\\n masking network.', 'sequences to perform well.', 'Current systems rely, in large part, on the\\n learned-domain masking strategy popularized by Conv-TasNet [15].', 'In this framework, an overcomplete set of analysis and synthesis filters is learned directly from the data, and separation is performed\\n by estimating a mask for each source in this learned-domain.', 'Building on this, Dual-Path RNN (DPRNN) [17] has demonstrated that\\n better long-term modeling is crucial to improve the separation performance.', 'This is achieved by splitting the input sequence into multiple chunks that are processed locally and globally with different\\n RNNs.', 'Nevertheless, due to the use of RNNs, DPRNN still suffers\\n from the aforementioned limitations of recurrent connections, especially regarding the global processing step.', 'An attempt to integrate\\n transformers into the speech separation pipeline has been recently\\n done in [22] where the proposed Dual-Path Transformer Network\\n (DPTNet) is shown to outperform the standard DPRNN.', 'Such an architecture, however, still embeds an RNN, effectively negating the\\n parallelization capability of pure-attention models.', 'In this paper, we propose a novel model called SepFormer (Separation Transformer), which is mainly composed of multi-head attention and feed-forward layers.', 'We adopt the dual-path framework introduced by DPRNN and we replace RNNs with a multiscale pipeline composed of transformers that learn both short and\\n long-term dependencies.', 'The dual-path framework enables to mitigate the quadratic complexity of transformers, as transformers in the\\n dual-path framework process smaller chunks.', 'To the best of our knowledge, this is the first work showing\\n that we can obtain state-of-the-art performance in separation with an\\n RNN-free Transformer-based architecture.', 'The SepFormer achieves\\n an SI-SNRi of 22.3 dB on the standard WSJ0-2mix dataset.', 'It also\\n achieves the SOTA performance of 19.5 dB SI-SNRi on the WSJ0-\\n 3mix dataset.', 'The SepFormer not only processes all the time steps in\\n parallel but also achieves competitive performance when downsampling the encoded representation by a factor of 8.', 'This makes the\\n proposed architecture significantly faster and less memory demandarXiv:2010.13154v2 [eess.AS] 8 Mar 2021\\n ing than the latest RNN-based separation models.', '2.', 'THE MODEL\\n The proposed model is based on the learned-domain masking approach [14, 15, 17–22] and employs an encoder, a decoder, and a\\n masking network, as shown in Figure 1.', 'The encoder is fully convolutional, while the masking network employs two Transformers\\n embedded inside the dual-path processing block proposed in [17].', 'The decoder finally reconstructs the separated signals in the time\\n domain by using the masks predicted by the masking network.', 'To\\n foster reproducibility, the SepFormer will be made available within\\n the SpeechBrain toolkit1\\n .', '2.1.', 'Encoder\\n The encoder takes in the time-domain mixture-signal x ∈ R\\n T\\n as\\n input, which contains audio from multiple speakers.', 'It learns an\\n STFT-like representation h ∈ R\\n F ×T\\n 0\\n using a single convolutional\\n layer:\\n h = ReLU(conv1d(x)).', '(1)\\n As we will describe in Sec.', '4, the stride factor of this convolution\\n impacts significantly on the performance, speed, and memory of the\\n model.', '2.2.', 'Masking Network\\n Figure 2 (top) shows the detailed architecture of the masking network (Masking Net).', 'The masking network is fed by the encoded\\n representations h ∈ R\\n F ×T\\n 0\\n and estimates a mask {m1, .', '.', '.', ', mNs}\\n for each of the Ns speakers in the mixture.', 'As in [15], the encoded input h is normalized with layer normalization [24] and processed by a linear layer (with dimensionality F).', 'We then create overlapping chunks of size C by chopping up h on\\n the time axis with an overlap factor of 50%.', 'We denote the output of\\n the chunking operation with h\\n 0 ∈ R\\n F ×C×Nc, where C is the length\\n of each chunk, and Nc is the resulting number of chunks.', 'The representation h\\n 0\\n feeds the SepFormer block, which is the\\n main component of the masking network.', 'This block, which will be\\n described in detail in Sec.', '2.3, employs a pipeline composed of two\\n transformers able to learn short and long-term dependencies.', 'The output of the SepFormer h\\n 00 ∈ R\\n F ×C×Nc is processed by\\n PReLU activations followed by a linear layer.', 'We denote the output\\n of this module h\\n 000 ∈ R\\n (F ×Ns)×C×Nc, where Ns is the number of\\n speakers.', 'Afterwards we apply the overlap-add scheme described\\n in [17] and obtain h\\n 0000 ∈ R\\n F ×Ns×T\\n 0\\n .', 'We pass this representation\\n through two feed-forward layers and a ReLU activation at the end to\\n finally obtain the mask mk for each of the speakers.', '2.3.', 'SepFormer Block\\n Figure 2 (Middle) shows the architecture of the SepFormer block.', 'The SepFormer block is designed to model both short and longterm dependencies with the dual-scale approach of DPRNNs [17].', 'In our model, the transformer block which models the short-term\\n dependencies is named IntraTransformer (IntraT), and the block for\\n longer-term dependencies is named InterTransformer (InterT).', 'IntraT processes the second dimension of h\\n 0\\n , and thus acts on each\\n chunk independently, modeling the short-term dependencies within\\n 1speechbrain.github.io/\\n each chunk.', 'Next, we permute the last two dimensions (which we\\n denote with P), and the InterT is applied to model the transitions\\n across chunks.', 'This scheme enables effective modelling of longterm dependencies across the chunks.', 'The overall transformation of\\n the SepFormer is therefore defined as follows:\\n h\\n 00 = finter(P(fintra(h\\n 0\\n ))), (2)\\n where we denote the IntraT and InterT with finter(.', '), and fintra(.', '),\\n respectively.', 'The overall SepFormer block is repeated N times.', '2.3.1.', 'Intra and Inter Transformers\\n Figure 2 (Bottom) shows the architecture of the Transformers used\\n for both the IntraT and InterT blocks.', 'It closely resembles the original one defined in [4].', 'We use the variable z to denote the input\\n to the Transformer.', 'First of all, sinusoidal positional encoding e is\\n added to the input z, such that,\\n z\\n 0 = z + e. (3)\\n Positional encoding injects information on the order of the various\\n elements composing the sequence, thus improving the separation\\n performance.', 'We follow the positional encoding definition in [4].', 'We then apply multiple Transformer layers.', 'Inside each Transformer layer g(.', '), we first apply layer normalization, followed by\\n multi-head attention (MHA):\\n z\\n 00 = MultiHeadAttention(LayerNorm(z\\n 0\\n )).', '(4)\\n As proposed in [4], each attention head computes the scaled dotproduct attention between all the elements of the sequence.', 'The\\n Transformer finally employs a feed-forward network (FFW), which\\n is applied to each position independently:\\n z\\n 000 = FeedForward(LayerNorm(z\\n 00 + z\\n 0\\n )) + z\\n 00 + z\\n 0\\n .', '(5)\\n The overall transformer block is therefore defined as follows:\\n f(z) = g\\n K(z + e) + z, (6)\\n where g\\n K(.)', 'denotes K layers of transformer layer g(.).', 'We use\\n K = N intra layers for the IntraT, and K = N inter layers for the\\n InterT.', 'As shown in Figure 2 (Bottom) and Eq.', '(6), we add residual\\n connections across the transformer layers, and across the transformer\\n architecture to improve gradient backpropagation.', '2.4.', 'Decoder\\n The decoder simply uses a transposed convolution layer, with the\\n same stride and kernel size of the encoder.', 'The input to the decoder is the element-wise multiplication between the mask mk of\\n the source k and the output of the encoder h. The transformation of\\n the decoder can therefore be expressed as follows:\\n sbk = conv1d-transpose(mk ∗ h), (7)\\n where sbk ∈ R\\n T\\n denotes the separated source k.\\n 3.', 'EXPERIMENTAL SETUP\\n 3.1.', 'Dataset\\n We use the popular WSJ0-2mix and WSJ0-3mix datasets [11] for\\n source separation, where mixtures of two speakers and three speakers are created by randomly mixing utterances in the WSJ0 corpus.', 'The relative levels for the sources are sampled uniformly between 0\\n dB to 5 dB.', 'Respectively, 30, 10, 5 hours of speech is used for training, validation, and test.', 'The training and test sets are created with\\n different sets of speakers.', 'The waveforms are sampled at 8 kHz.', 'h Norm+Linear Chunking SepFormer PReLU+Linear OverlapAdd FFW+ReLU\\n m1\\n m2\\n h\\n 0 h\\n 00 h\\n 000 h\\n 0000\\n Repeat N times\\n h\\n 0\\n IntraTransformer Permute InterTransformer h\\n 00\\n Repeat K times\\n z\\n e\\n LayerNorm MHA LayerNorm FFW z\\n 000 f(z)\\n z\\n 0 z\\n 00\\n Fig.', '2.', '(Top) The overall architecture proposed for the masking network.', '(Middle) The SepFormer Block.', '(Bottom) The transformer\\n architecture f(.)', 'that is used both in the IntraTransformer block and in the InterTransformer block.', '3.2.', 'Architecture and Training Details\\n The encoder is based on 256 convolutional filters with a kernel size\\n of 16 samples and a stride factor of 8 samples.', 'The decoder uses the\\n same kernel size and the stride factors of the encoder.', 'In our best models, the SepFormer masking network processes\\n chunks of size C = 250 with a 50 % overlap between them and\\n employs 8 layers of transformers in both IntraT and InterT.', 'The\\n IntraT-InterT dual-path processing pipeline is repeated N = 2 times.', 'We used 8 parallel attention heads, and 1024-dimensional positional\\n feed-forward networks within each Transformer layer.', 'The model\\n has a total of 26 million parameters.', 'We explored the use of dynamic mixing (DM) data augmentation [23] which consists in on-the-fly creation of new mixtures from\\n single speaker sources.', 'In this work we expanded this powerful technique by applying also speed perturbation on the sources before mixing them.', 'The speed randomly changes between 95 % slow-down\\n and 105 % speed-up.', 'We used the Adam algorithm [25] as optimizer, with a learning rate of 15e\\n −5\\n .', 'After epoch 65 (after epoch 100 with DM), the\\n learning rate is annealed by halving it if we do not observe any improvement of the validation performance for 3 successive epochs\\n (5 epoch for DM).', 'Gradient clipping is employed to limit the L2\\n norm of the gradients to 5.', 'During training, we used a batch size of\\n 1, and used the scale-invariant signal-to-noise Ratio (SI-SNR) [26]\\n via utterance-level permutation invariant loss [13], with clipping at\\n 30dB [23].', 'We used automatic mixed-precision to speed up training.', 'The system is trained for a maximum of 200 epochs.', 'Each epoch\\n takes approximately 1.5 hours on a single NVIDIA V100 GPU with\\n 32 GB of memory.', '4.', 'RESULTS\\n 4.1.', 'Results on WSJ0-2mix\\n Table 1 compares the performance achieved by the proposed SepFormer with the best results reported in the literature on the WSJ0-\\n 2mix dataset.', 'The SepFormer achieves an SI-SNR improvement (SISNRi) of 22.3 dB and a Signal-to-Distortion Ratio [30] (SDRi) improvement of 22.4 dB on the test-set with dynamic mixing.', 'When\\n using dynamic mixing, the proposed architecture achieves state-ofthe-art performance.', 'The SepFormer outperforms previous systems\\n without using dynamic mixing except Wavesplit, which uses speaker\\n identity as additional information.', 'Table 1.', 'Best results on the WSJ0-2mix dataset (test-set).', 'DM\\n stands for dynamic mixing.', 'Model SI-SNRi SDRi # Param Stride\\n Tasnet [27] 10.8 11.1 n.a 20\\n SignPredictionNet [28] 15.3 15.6 55.2M 8\\n ConvTasnet [15] 15.3 15.6 5.1M 10\\n Two-Step CTN [29] 16.1 n.a.', '8.6M 10\\n DeepCASA [18] 17.7 18.0 12.8M 1\\n FurcaNeXt [19] n.a.', '18.4 51.4M n.a.', 'DualPathRNN [17] 18.8 19.0 2.6M 1\\n sudo rm -rf [21] 18.9 n.a.', '2.6M 10\\n VSUNOS [20] 20.1 20.4 7.5M 2\\n DPTNet* [22] 20.2 20.6 2.6M 1\\n Wavesplit** [23] 21.0 21.2 29M 1\\n Wavesplit** + DM [23] 22.2 22.3 29M 1\\n SepFormer 20.4 20.5 26M 8\\n SepFormer + DM 22.3 22.4 26M 8\\n *only SI-SNR and SDR (without improvement) are reported.', '**uses speaker-ids as additional info.', 'Table 2.', 'Ablation of the SepFormer on WSJ0-2Mix (validation set).', 'SI-SNRi N N intra N inter # Heads DFF PosEnc DM\\n 22.3 2 8 8 8 1024 Yes Yes\\n 20.5 2 8 8 8 1024 Yes No\\n 20.4 2 4 4 16 2048 Yes No\\n 20.2 2 4 4 8 2048 Yes No\\n 19.9 2 4 4 8 2048 Yes No\\n 19.8 3 4 4 8 2048 Yes No\\n 19.4 2 4 4 8 2048 No No\\n 19.2 2 4 1 8 2048 Yes No\\n 19.1 2 3 3 8 2048 Yes No\\n 19.0 2 3 3 8 2048 No No\\n 4.2.', 'Ablation Study\\n Hereafter we study the effect of various hyperparameters and data\\n augmentation on the performance of the SepFormer using WSJ0-\\n 2mix dataset.', 'The results are summarized in Table 2.', 'The reported\\n performance in this table is calculated on the validation set.', 'We observe that the number of InterT and IntraT blocks has an\\n important impact on the performance.', 'The best results are achieved\\n with 8 layers for both blocks replicated two times.', 'We also would\\n like to point out that a respectable performance of 19.2 dB is obtained even when we use a single layer transformer for the Inter-\\n 0 10 20 30 40\\n Training time (hours)\\n 8\\n 10\\n 12\\n 14\\n 16\\n 18\\n 20\\n SI-SNRi (dB)\\n Training Speed on WSJ0-2Mix\\n SepFormer\\n DP-RNN\\n DPTNet\\n 1.0 2.0 3.0 4.0 5.0\\n Input sequence length in seconds\\n 32.0\\n 55.0\\n 95.0\\n 166.0\\n 288.0\\n 501.0\\n miliseconds\\n Average Forward-Pass Time\\n SepFormer\\n DP-RNN\\n DPTNet\\n Wavesplit\\n 1.0 2.0 3.0 4.0 5.0\\n Input sequence length in seconds\\n 2.0\\n 3.0\\n 6.0\\n 11.0\\n 21.0\\n 40.0\\n GBytes\\n Memory Usage\\n SepFormer\\n DP-RNN\\n DPTNet\\n Wavesplit\\n Fig.', '3.', '(Left) The traning curves of SepFormer, DPRNN, and DPTNeT on the WSJ0-2mix dataset.', '(Middle & Right) The comparison of\\n forward-pass speed and memory usage in the GPU on inputs ranging 1-5 seconds long sampled at 8kHz.', 'Table 3.', 'Best results on the WSJ0-3mix dataset.', 'Model SI-SNRi SDRi # Param\\n ConvTasnet [15] 12.7 13.1 5.1M\\n DualPathRNN [17] 14.7 n.a 2.6M\\n VSUNOS [20] 16.9 n.a 7.5M\\n Wavesplit [23] 17.3 17.6 29M\\n Wavesplit [23] + DM 17.8 18.1 29M\\n Sepformer 17.6 17.9 26M\\n Sepformer + DM 19.5 19.7 26M\\n Transformer.', 'This suggests that the IntraTransformer, and thus local processing, has a greater influence on the performance.', 'It also\\n emerges that positional encoding is helpful (e.g.', 'see lines 3 and 5 of\\n Table 2).', 'A similar outcome has been observed in [31] for speech\\n enhancement.', 'As for the number of attention heads, we observe a\\n slight performance difference between 8 and 16 heads.', 'Finally, it\\n can be observed that dynamic mixing helps the performance significantly.', '4.3.', 'Results on WSJ0-3mix\\n Table 3 showcases the best performing models on the WSJ0-3mix\\n dataset.', 'SepFormer obtains the state-of-the-art performance with an\\n SI-SNRi of 19.5 dB and an SDRi of 19.7 dB.', 'We used here the best\\n architecture found for the WSJ0-2mix dataset.', 'The only difference is\\n that the decoder has now three outputs.', 'It is worth noting that on this\\n corpus the SepFormer outperforms all previously proposed systems.', 'Our results on WSJ0-2mix and WSJ0-3mix show that it is possible to achieve state-of-the-art performance in separation with an\\n RNN-free Transformer-based model.', 'The big advantage of SepFormer over RNN-based systems like [17,20,22] is the possibility to\\n parallelize the computations over different time steps.', 'This leads to\\n faster training and inference, as described in the following section.', '4.4.', 'Speed and Memory Comparison\\n We now compare the training and inference speed of our model with\\n DPRNN [17] and DPTNet [22].', 'Figure 3 (left) shows the training\\n curves of the aforementioned models on the WSJ0-2mix dataset.', 'We plot the performance achieved on the validation set in the first\\n 48 hours of training versus the wall-clock time.', 'For a fair comparison, we used the same machine with the same GPU (a single\\n NVIDIA V100-32GB) for all the models.', 'Moreover, all the systems\\n are trained with a batch size of 1 and employ automatic mixed precision.', 'We observe that the SepFormer is faster than DPRNN and\\n DPTNeT.', 'Figure 3 (left), highlights that SepFormer reaches above\\n 17dB levels only after a full day of training, whereas the DPRNN\\n model requires two days of training to achieve the same level of performance.', 'Figure 3 (middle&right) compares the average computation time\\n (in ms) and the total memory allocation (in GB) during inference\\n when single precision is used.', 'We analyze the speed of our best\\n model for both WSJ0-2Mix and WSJ0-3Mix datasets.', 'We compare\\n our models against DP-RNN, DPTNeT, and Wavesplit.', 'All the models are stored in the same NVIDIA RTX8000-48GB GPU and we\\n performed this analysis using the PyTorch profiler [32].', 'For Wavesplit we used the implementation in [33].', 'From this analysis, it emerges that the SepFormer is not only\\n faster but also less memory demanding than DPTNet, DPRNN, and\\n Wavesplit.', 'We observed the same behavior using the CPU for inference also.', 'Such a level of computational efficiency is achieved even\\n though the proposed SepFormer employs more parameters than the\\n other RNN-based methods (see Table 1).', 'This is not only due to the\\n superior parallelization capabilities of the proposed model, but also\\n because the best performance is achieved with a stride factor of 8\\n samples, against a stride of 1 for DPRNN and DPTNet.', 'Increasing\\n the stride of the encoder results in downsampling the input sequence,\\n and therefore the model processes less data.', 'In [17], the authors\\n showed that the DPRNN performance degrades when increasing the\\n stride factor.', 'The SepFormer, instead, reaches competitive results\\n even with a relatively large stride, leading to the aforementioned\\n speed and memory advantages.', '5.', 'CONCLUSIONS\\n In this paper, we proposed a novel neural model for speech separation called SepFormer (Separation Transformer).', 'The SepFormer\\n is an RNN-free architecture that employs a masking network composed of transformers only.', 'The masking network learns both short\\n and long-term dependencies using a multi-scale approach.', 'Our results, reported on the WSJ0-2mix and WSJ0-3mix datasets, highlight that we can reach state-of-the-art performances in source separation without using RNNs in the network design.', 'This way, computations over different time-steps can be parallelized.', 'Moreover,\\n our model achieves a competitive performance even when subsampling the encoded representation by a factor of 8.', 'These two properties lead to a significant speed-up at training/inference time and\\n a drastic reduction of memory usage, especially when compared to\\n recent models such as DPRNN, DPTNet, and Wavesplit.', 'As future\\n work, we would like to explore different transformer architectures\\n that could potentially further improve performance, speed, and memory usage.', '6.', 'REFERENCES\\n [1] S. Hochreiter and J. Schmidhuber, “Long short-term memory,”\\n Neural Computation, vol.', '9, no.', '8, pp.', '1735–1780, Nov. 1997.', '[2] K. Cho, B. van Merrienboer, D. Bahdanau, and Y. Bengio, “On ¨\\n the properties of neural machine translation: Encoder–decoder\\n approaches,” in Proc.', 'of SSST, 2014, pp.', '103–111.', '[3] M. Ravanelli, P. Brakel, M. Omologo, and Y. Bengio, “Light\\n gated recurrent units for speech recognition,” IEEE Transactions on Emerging Topics in Computational Intelligence, vol.', '2, no.', '2, pp.', '92–102, April 2018.', '[4] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones,\\n A. N. Gomez, L. Kaiser, and I. Polosukhin, “Attention is all\\n you need,” CoRR, vol.', 'abs/1706.03762, 2017.', '[5] G. Kerg, B. Kanuparthi, A. Goyal, K. Goyette, Y. Bengio, and\\n G. Lajoie, “Untangling tradeoffs between recurrence and selfattention in neural networks,” CoRR, vol.', 'abs/2006.09471,\\n 2020.', '[6] S. Karita, N. Chen, T. Hayashi, T. Hori, H. Inaguma, Z. Jiang,\\n M. Someki, N. E. Y. Soplin, R. Yamamoto, X. Wang, S. Watanabe, T. Yoshimura, and W. Zhang, “A comparative study on\\n transformer vs rnn in speech applications,” in Proc.', 'of ASRU,\\n 2019, pp.', '449–456.', '[7] N. Li, S. Liu, Y. Liu, S. Zhao, and M. Liu, “Neural speech\\n synthesis with transformer network,” in Proc.', 'of AAAI, 2019,\\n pp.', '6706–6713.', '[8] J. Kim, M. El-Khamy, and J. Lee, “T-gsa: Transformer with\\n gaussian-weighted self-attention for speech enhancement,” in\\n Proc.', 'of ICASSP, 2020, pp.', '6649–6653.', '[9] Q. Li, F. L. Kreyssig, C. Zhang, and P. C. Woodland, “Discriminative neural clustering for speaker diarisation,” CoRR,\\n vol.', 'abs/1910.09703, 2019.', '[10] X. Chang, W. Zhang, Y. Qian, J.', 'Le Roux, and S. Watanabe, “End-to-end multi-speaker speech recognition with transformer,” in Proc.', 'of ICASSP, 2020, pp.', '6134–6138.', '[11] J. R. Hershey, Z. Chen, J.', 'Le Roux, and S. Watanabe, “Deep\\n clustering: Discriminative embeddings for segmentation and\\n separation,” in Proc.', 'of ICASSP, 2016, pp.', '31–35.', '[12] D. Yu, M. Kolbæk, Z. Tan, and J. Jensen, “Permutation invariant training of deep models for speaker-independent multitalker speech separation,” in Proc.', 'of ICASSP, 2017, pp.', '241–\\n 245.', '[13] M. Kolbæk, D. Yu, Z.-H. Tan, and J. Jensen, “Multitalker\\n speech separation with utterance-level permutation invariant\\n training of deep recurrent neural networks,” IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol.', '25,\\n no.', '10, pp.', '1901–1913, 2017.', '[14] Shrikant Venkataramani, Jonah Casebeer, and Paris Smaragdis,\\n “End-to-end source separation with adaptive front-ends,” in\\n Proc.', 'of ACSSC, 2018, pp.', '684–688.', '[15] Y. Luo and N. Mesgarani, “Conv-TasNet: Surpassing Ideal\\n Time–Frequency Magnitude Masking for Speech Separation,”\\n vol.', '27, no.', '8, pp.', '1256–1266, Aug. 2019.', '[16] P. Huang, M. Kim, M. Hasegawa-Johnson, and P. Smaragdis,\\n “Deep learning for monoaural source separation,” in Proc.', 'of\\n ICASSP, 2014, pp.', '1562–1566.', '[17] Y. Luo, Z. Chen, and T. Yoshioka, “Dual-path rnn: efficient long sequence modeling for time-domain single-channel\\n speech separation,” in Proc.', 'of ICASSP, 2020, pp.', '46–50.', '[18] Y. Liu and D. Wang, “Divide and conquer: A deep casa\\n approach to talker-independent monaural speaker separation,”\\n IEEE/ACM Transactions on audio, speech, and language processing, vol.', '27, no.', '12, 2019.', '[19] Z. Shi, H. Lin, L. Liu, R. Liu, J. Han, and A. Shi, “Furcanext:\\n End-to-end monaural speech separation with dynamic gated dilated temporal convolutional networks,” in MultiMedia Modeling, 2020, pp.', '653–665.', '[20] E. Nachmani, Y. Adi, and L. Wolf, “Voice separation with\\n an unknown number of multiple speakers,” ICML, pp.', '7164–\\n 7175, 2020.', '[21] E. Tzinis, Z. Wang, and P. Smaragdis, “Sudo rm -rf: Efficient\\n networks for universal audio source separation,” in MLSP,\\n 2020, pp.', '1–6.', '[22] J. Chen, Q. Mao, and D. Liu, “Dual-Path Transformer\\n Network: Direct Context-Aware Modeling for End-to-End\\n Monaural Speech Separation,” in Proc.', 'of Interspeech 2020,\\n 2020, pp.', '2642–2646.', '[23] N. Zeghidour and D. Grangier, “Wavesplit: End-to-end\\n speech separation by speaker clustering,” arXiv preprint\\n arXiv:2002.08933, 2020.', '[24] L. J. Ba, J. R. Kiros, and G. E. Hinton, “Layer normalization,”\\n CoRR, vol.', 'abs/1607.06450, 2016.', '[25] D. P. Kingma and J. Ba, “Adam: A method for stochastic optimization,” arXiv preprint arXiv:1412.6980, 2014.', '[26] J.', 'Le Roux, S. Wisdom, H. Erdogan, and J. R. Hershey, “Sdr–\\n half-baked or well done?,” in Proc.', 'of ICASSP.', 'IEEE, 2019,\\n pp.', '626–630.', '[27] Y. Luo and N. Mesgarani, “TasNet: time-domain audio separation network for real-time, single-channel speech separation,”\\n CoRR, vol.', 'abs/1711.00541, 2017.', '[28] Zhong-Qiu Wang, Ke Tan, and DeLiang Wang, “Deep learning\\n based phase reconstruction for speaker separation: A trigonometric perspective,” in Proc.', 'of ICASSP, 2019, pp.', '71–75.', '[29] E. Tzinis, S. Venkataramani, Z. Wang, C. Subakan, and\\n P. Smaragdis, “Two-step sound source separation: Training on\\n learned latent targets,” in Proc.', 'of ICASSP, 2020, pp.', '31–35.', '[30] E. Vincent, R. Gribonval, and C Fevotte, “Performance mea- ´\\n surement in blind audio source separation,” IEEE transactions\\n on audio, speech, and language processing, vol.', '14, no.', '4, pp.', '1462–1469, 2006.', '[31] J. Kim, M. El-Khamy, and J. Lee, “T-gsa: Transformer with\\n gaussian-weighted self-attention for speech enhancement,” in\\n Proc.', 'of ICASSP, 2020, pp.', '6649–6653.', '[32] Pytorch, “Profiler,” https://pytorch.org/\\n tutorials/recipes/recipes/profiler.html,\\n 2020, Accessed: 2020-10-21.', '[33] M. Pariente, S. Cornell, J. Cosentino, S. Sivasankaran, E. Tzinis, J. Heitkaemper, M. Olvera, F.-R. Stoter, M. Hu, J. M. ¨\\n Martın-Donas, D. Ditter, A. Frank, A. Deleforge, and E. Vin- ˜\\n cent, “Asteroid: the PyTorch-based audio source separation\\n toolkit for researchers,” in Proc.', 'of Interspeech, 2020, pp.', '2637–2641.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "id": "pskYMyFAig0l",
        "outputId": "69b783ec-bf3f-4b1e-f940-f36de93b0e29"
      },
      "source": [
        "generate_summary( sentenceskkk,stopwords ,3)\n"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "' Performance of several techniques has been already reported on this set, including N-gram models, LSA-based model [32], log-bilinear model [24] and a combination of recurrent neural networks that currently holds the state of the art performance of 55.4% accuracy on this benchmark [19]. Finally, we found that when we train high dimensional word vectors on a large amount of data, the resulting vectors can be used to answer very subtle semantic relationships between words, such as a city and the country it belongs to, e.g. However, several practical solutions were proposed for avoiding it; either using hierarchical versions of the softmax [25, 23, 18], or avoiding normalized models completely by using models that are not normalized during training [4, 9].'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 387
        },
        "id": "wD8nOi1pVn9c",
        "outputId": "638b57cf-9ca8-4e19-c713-fb332959572b"
      },
      "source": [
        "generate_summary( sentencesAtten,stopwords ,3)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/nltk/cluster/util.py:133: RuntimeWarning: invalid value encountered in true_divide\n",
            "  sqrt(numpy.dot(u, u)) * sqrt(numpy.dot(v, v))))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "PowerIterationFailedConvergence",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mPowerIterationFailedConvergence\u001b[0m           Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-27-92dd7eb4a342>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgenerate_summary\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0msentencesAtten\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstopwords\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-17-880baa675911>\u001b[0m in \u001b[0;36mgenerate_summary\u001b[0;34m(sentences, stop_words, top_n)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0msentence_similarity_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence_similarity_martix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpagerank\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence_similarity_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence_similarity_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mranked_sentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreverse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<decorator-gen-430>\u001b[0m in \u001b[0;36mpagerank\u001b[0;34m(G, alpha, personalization, max_iter, tol, nstart, weight, dangling)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/networkx/utils/decorators.py\u001b[0m in \u001b[0;36m_not_implemented_for\u001b[0;34m(not_implement_for_func, *args, **kwargs)\u001b[0m\n\u001b[1;32m     76\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mnx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNetworkXNotImplemented\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mnot_implement_for_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_not_implemented_for\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/networkx/algorithms/link_analysis/pagerank_alg.py\u001b[0m in \u001b[0;36mpagerank\u001b[0;34m(G, alpha, personalization, max_iter, tol, nstart, weight, dangling)\u001b[0m\n\u001b[1;32m    155\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0merr\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mN\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtol\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 157\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mnx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPowerIterationFailedConvergence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mPowerIterationFailedConvergence\u001b[0m: (PowerIterationFailedConvergence(...), 'power iteration failed to converge within 100 iterations')"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 154
        },
        "id": "vlCbjyUxVoB9",
        "outputId": "74bba4af-b780-4fac-ac5c-d818212b6a07"
      },
      "source": [
        "generate_summary( sentencesevol,stopwords ,3)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "' Intriguingly the three\\n day 127 clones that accumulated between 52-79 mutations carried a G180D DnaQ mutation,\\n while the five that accumulated over 275 mutations by day 127 suffered a different mutation\\n within DnaQ (E110K). Despite their acquiring a substantial deleterious burden, clones with very high mutation\\n rates can persist for prolonged periods of time, alongside clones with lower mutation\\n rates\\n Downloaded from https://academic.oup.com/mbe/advance-article/doi/10.1093/molbev/msab067/6178007 by guest on 19 March 2021\\n 12\\n We find that ratios of the rates of non-synonymous to synonymous substitutions accumulated\\n by non-mutators, initial mutators and population 4 putative enhanced mutators are all\\n significantly higher than one (Table 2). Downloaded from https://academic.oup.com/mbe/advance-article/doi/10.1093/molbev/msab067/6178007 by guest on 19 March 2021\\n 17\\n Calculating numbers of synonymous and non-synonymous sites within the E. coli K12\\n MG1655 genome\\n DNA sequences of all protein-coding genes of E. coli K12 MG1655 were downloaded from\\n the NCBI database.'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pCaCN6G47I3q"
      },
      "source": [
        "# Performance Mesure on Generate Summary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ftDpvn6Ymidt",
        "outputId": "fcc01411-7a93-4f9f-b926-e1b9022e3d13"
      },
      "source": [
        "print(timeit.Timer().timeit(number=2))"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.1520005500642583e-06\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PCUvzALW1qil",
        "outputId": "18a36473-0755-4ecb-f464-3d5235652471"
      },
      "source": [
        "print(getrusage(RUSAGE_SELF))"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "resource.struct_rusage(ru_utime=1083.297522, ru_stime=6.991109, ru_maxrss=1942724, ru_ixrss=0, ru_idrss=0, ru_isrss=0, ru_minflt=1477001, ru_majflt=1063, ru_nswap=0, ru_inblock=352704, ru_oublock=98832, ru_msgsnd=0, ru_msgrcv=0, ru_nsignals=0, ru_nvcsw=67413, ru_nivcsw=634710)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G_84QxUL7rNY",
        "outputId": "94947e9c-134f-413f-a993-b03fbbb3d270"
      },
      "source": [
        "from line_profiler import LineProfiler\n",
        "lp = LineProfiler()\n",
        "lp_wrapper = lp(generate_summary)\n",
        "lp_wrapper(sentencesevol,stopwords ,3)\n",
        "lp.print_stats()"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Timer unit: 1e-06 s\n",
            "\n",
            "Total time: 166.022 s\n",
            "File: <ipython-input-17-880baa675911>\n",
            "Function: generate_summary at line 1\n",
            "\n",
            "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
            "==============================================================\n",
            "     1                                           def generate_summary(sentences,stop_words,top_n=5):\n",
            "     2         1          2.0      2.0      0.0      summarize_text = []\n",
            "     3         1          1.0      1.0      0.0      summarized_text =\"\"\n",
            "     4         1  142410703.0 142410703.0     85.8      sentence_similarity_martix = sim_matrix(sentences, stop_words)\n",
            "     5                                           \n",
            "     6         1    2093950.0 2093950.0      1.3      sentence_similarity_graph = nx.from_numpy_array(sentence_similarity_martix)\n",
            "     7         1   21515762.0 21515762.0     13.0      scores = nx.pagerank(sentence_similarity_graph)\n",
            "     8         1        697.0    697.0      0.0      print(sentence_similarity_graph)\n",
            "     9         1        558.0    558.0      0.0      ranked_sentence = sorted(((scores[i],s) for i,s in enumerate(sentences)), reverse=True)    \n",
            "    10                                              # print(\"Indexes of top ranked_sentence order are \", ranked_sentence)    \n",
            "    11                                           \n",
            "    12         4          5.0      1.2      0.0      for i in range(top_n):\n",
            "    13         3          5.0      1.7      0.0        summarized_text= summarized_text +\" \"+ ranked_sentence[i][1]\n",
            "    14                                           \n",
            "    15         1          1.0      1.0      0.0      return summarized_text\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 154
        },
        "id": "rlH8PtVnkLxG",
        "outputId": "9c65afb1-1599-4c10-dc8c-20b8f647082e"
      },
      "source": [
        "\n",
        "\n",
        "generate_summary(sentenceshS,stopwords ,3)"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "' Full size image\\n Shared symptoms indicate shared protein interactions\\n To further assess whether shared symptoms indicate not only shared genetic associations, but also close interaction of the corresponding proteins, we integrated five publicly available PPI databases (Supplementary Methods) and constructed disease networks in which two diseases are linked if they have shared 1st and 2nd order PPI interactions, respectively: shared 1st order PPI means that two diseases have associated proteins that directly interact within the PPI network, while shared 2nd order PPI means that they are connected by a path of length two (Fig. Full size image\\n Results\\n Construction of the HSDN\\n We extracted 7,109,429 (about 35.5% in over twenty million records) PubMed bibliographic records with one or more disease/symptom terms in the MeSH metadata field (see Methods), yielding a total of 4,442 disease terms and 322 symptom terms (Supplementary Data 1 and 2). figure2\\n (a) The twenty most frequent disease terms in the MeSH fields of PubMed records, containing eight types of cancers (for example, breast neoplasms, lung neoplasms), four types of vascular diseases (for example, hypertension, myocardial infarction and coronary diseases), HIV infections, asthma, obesity, pain, rheumatoid arthritis, type 2 diabetes and two mental diseases.'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "id": "RQo0_AeokL-y",
        "outputId": "6d927ddb-5600-4621-f8d7-98b8a8b73b0f"
      },
      "source": [
        "generate_summary(sentencesmorals,stopwords ,3)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "' The ethical dimension of the decision leads me to think about myself and recognise, say, that I have certain talents, or that I would like to maximise my work-life balance. That is, they must recognise that duties can be ranked in a hierarchy (for example, to stop at an accident to render assistance trumps the promise of meeting for coffee); in a similar way, consequences can be ranked too. In moral decisions, in which the importance of others and their actual situation in the world, is recognised, community decisions are based on dialogue between all those on whom the decision impacts.'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "id": "NFOkVaC8kMJC",
        "outputId": "267fb721-2b4c-45f6-d6ab-39331217a329"
      },
      "source": [
        "#https://www.catholicculture.org/commentary/short-course-in-music-and-morals/\n",
        "generate_summary(sentencesmus,stopwords ,3)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\" Cole keeps up with what we're doing at CatholicCulture.org; recently he surmised from some of my previous remarks on music that I might be interested in his dissertation, which had been published as a book in 1993. Even so, the book provides significant food for thought on a difficult subject, and the notes and bibliography demonstrate how thoroughly the author researched every aspect of it. Even those interested in studying highly specialized works by particular philosophers, musicians and researchers would be wise to put them off until they’ve read Music & Morals—a book that brings order out of the chaos of mere opinion concerning the moral dimensions of music.\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "id": "_nwu4az6kMQb",
        "outputId": "01539b55-8e43-4122-dcf3-38de42bfccce"
      },
      "source": [
        "generate_summary(sentencesweb,stopwords ,3)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "' You can think of Selenium as a slimmed-down browser that executes the JavaScript code for you before passing on the rendered HTML response to your script. You learned how to:\\n \\n Inspect the HTML structure of your target site with your browser’s developer tools\\n Gain insight into how to decipher the data encoded in URLs\\n Download the page’s HTML content using Python’s requests library\\n Parse the downloaded HTML with Beautiful Soup to extract relevant information\\n With this general pipeline in mind and powerful libraries in your toolkit, you can go out and see what other websites you can scrape! Note: Keep in mind that it’s helpful to periodically switch back to your browser and interactively explore the page using developer tools.'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "53rSorj4ZRlA"
      },
      "source": [
        "Original document link\n",
        "\n",
        "https://arxiv.org/pdf/1301.3781.pdf"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZKXD4UQwZErx",
        "outputId": "b648db2f-69cf-473a-d2e3-a2701769d53d"
      },
      "source": [
        "myquery= input(\"Please enter the information to be searched \")\n",
        "data =google_search(API_KEY,SEARCH_ENGINE_ID,myquery)\n",
        "results(data)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Please enter the information to be searched Performance of several techniques has been already reported on this set, including N-gram models, LSA-based model [32], log-bilinear model [24] and a combination of recurrent neural networks that currently holds the state of the art performance of 55.4% accuracy on this benchmark [19]. Finally, we found that when we train high dimensional word vectors on a large amount of data, the resulting vectors can be used to answer very subtle semantic relationships between words, such as a city and the country it belongs to, e.g. However, several practical solutions were proposed for avoiding it; either using hierarchical versions of the softmax [25, 23, 18], or avoiding normalized models completely by using models that are not normalized during training [4, 9]\n",
            "Title: (PDF) Efficient Estimation of Word Representations in Vector Space\n",
            "Description: Oct 21, 2014 ... Furthermore, we show that these vectors provide state-of-the-art perfor- ... \n",
            "Recurrent neural network based language model has been proposed to ... been \n",
            "already reported on this set, including N-gram models, LSA-based model [32], \n",
            "log-bilinear ... performance of 55.4% accuracy on this benchmark [19].\n",
            "URL: https://www.researchgate.net/publication/234131319_Efficient_Estimation_of_Word_Representations_in_Vector_Space \n",
            "\n",
            "Title: Efficient Estimation of Word Representations in Vector Space\n",
            "Description: Sep 7, 2013 ... Furthermore, we show that these vectors provide state-of-the-art perfor- ... \n",
            "However, the simple techniques are at their limits in many tasks. ... Recurrent \n",
            "neural network based language model has been ... been already reported on this \n",
            "set, including N-gram models, LSA-based model [32], log-bilinear model ...\n",
            "URL: https://arxiv.org/pdf/1301.3781 \n",
            "\n",
            "Title: 论文学习：Word2Vec第一篇：Efficient Estimation of Word ...\n",
            "Description: 2021年3月1日 ... 对比了NNLM、RNNLM模型提出了CBOW和Skip-gram两种模型架构 ... best \n",
            "performing techniques based on different types of neural networks. ... that these \n",
            "vectors provide state-of-the-art performance on our test set ... An example is the \n",
            "popular N-gram model used for statistical language modeling - today, ...\n",
            "URL: https://zhuanlan.zhihu.com/p/353609023 \n",
            "\n",
            "Title: Representation and Transfer Learning Using Information-Theoretic ...\n",
            "Description: May 15, 2020 ... ideas that took shape after discussions with Lizhong and it has been an ... 4.8 \n",
            "Neural network classifier's output LLR for the example dataset in Figure 4.6 ... 2.1 \n",
            "Performance comparison between our model and other single architecture ... \n",
            "Clustering is one of many important techniques in unsupervised ...\n",
            "URL: https://dspace.mit.edu/bitstream/handle/1721.1/127008/1191230439-MIT.pdf?sequence=1&isAllowed=y \n",
            "\n",
            "Title: Robust and comprehensive joint image-text representations\n",
            "Description: This thesis investigates the joint modeling of visual and textual content of \n",
            "multimedia ... suffers from several deficiencies that may hinder the performance \n",
            "of cross-modal tasks ... Evaluations show that our approaches achieve state-of-\n",
            "the-art ... The research on multimedia retrieval and classification has been very \n",
            "active in the.\n",
            "URL: https://www.theses.fr/2017CNAM1096.pdf \n",
            "\n",
            "Title: Compressive Cross-Language Text Summarization\n",
            "Description: Mar 20, 2019 ... Analyzing these problems, we propose a neural network model that combines \n",
            "recurrent and convolutional neural networks to estimate the ...\n",
            "URL: https://hal.archives-ouvertes.fr/tel-02003886v2/document \n",
            "\n",
            "Title: Proceedings of the Eighteenth Conference on Computational ...\n",
            "Description: Jun 29, 2012 ... Probabilistic Modeling of Joint-context in Distributional Similarity. Oren Melamud, \n",
            "Ido ... performance on the test section of the Wall Street ... a series of experiments \n",
            "comparing state-of-the-art ... 1In many fields, including NLP, it has become good \n",
            "prac- ... Resources and benchmarks for parsing the language.\n",
            "URL: https://www.aclweb.org/anthology/W14-16.pdf \n",
            "\n",
            "Title: Download book PDF\n",
            "Description: Model Checking Games for a Fair Branching-Time Temporal Epistemic ... Texture \n",
            "Detection Using Neural Networks Trained on Examples of One ... Based on the \n",
            "market setting we describe above, we now define the market policies to ... use N-\n",
            "PP as pricing policy, which has been observed as a well-performing policy in.\n",
            "URL: https://link.springer.com/content/pdf/10.1007%2F978-3-642-10439-8.pdf \n",
            "\n",
            "Title: Proceedings of the 54th Annual Meeting of the Association for ...\n",
            "Description: Aug 8, 2016 ... 32. Improving Statistical Machine Translation Performance by ... Nonparametric \n",
            "Spherical Topic Modeling with Word Embeddings ... Deep Neural Networks for \n",
            "Syntactic Parsing of Morphologically Rich ... shallow approaches have been tried \n",
            "in the past. ... bag-of-n-grams vector (up to trigrams) and has di-.\n",
            "URL: https://www.aclweb.org/anthology/P16-2.pdf \n",
            "\n",
            "Title: standard 12-lead electrocardiogram: Topics by Science.gov\n",
            "Description: This new hand held smart phone 12 lead ECG recorder needs further ... The 12-\n",
            "lead Electrocardiogram (ECG) has been used to detect cardiac ... The QRS \n",
            "pattern matching model shows stable performance for verification of 10 to ... The \n",
            "second set was a test set (n=116 ECGs) in which the J-wave status (present/\n",
            "absent) ...\n",
            "URL: https://www.science.gov/topicpages/s/standard+12-lead+electrocardiogram.html \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RO87jkxSrm9N",
        "outputId": "e302fa9e-5bb2-48ae-8983-ba96b46907a7"
      },
      "source": [
        "myquery= input(\"Please enter the information to be searched \")\n",
        "data =google_search(API_KEY,SEARCH_ENGINE_ID,myquery)\n",
        "results(data)"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Please enter the information to be searched  Intriguingly the three  day 127 clones that accumulated between 52-79 mutations carried a G180D DnaQ mutation,  while the five that accumulated over 275 mutations by day 127 suffered a different mutation  within DnaQ (E110K). Despite their acquiring a substantial deleterious burden, clones with very high mutation  rates can persist for prolonged periods of time, alongside clones with lower mutation  rates  Downloaded from https://academic.oup.com/mbe/advance-article/doi/10.1093/molbev/msab067/6178007 by guest on 19 March 2021  12  We find that ratios of the rates of non-synonymous to synonymous substitutions accumulated  by non-mutators, initial mutators and population 4 putative enhanced mutators are all  significantly higher than one (Table 2). Downloaded from https://academic.oup.com/mbe/advance-article/doi/10.1093/molbev/msab067/6178007 by guest on 19 March 2021  17  Calculating numbers of synonymous and non-synonymous sites within the E. coli K12  MG1655 genome  DNA sequences of all protein-coding genes of E. coli K12 MG1655 were downloaded from  the NCBI database.\n",
            "Title: Dynamics of adaptation during three years of evolution under long ...\n",
            "Description: clones that vary greatly in their mutation rates tend to co-exist within their ... day \n",
            "127 clones that accumulated between 52-79 mutations carried a G180D DnaQ \n",
            "mutation, while the five that accumulated over 275 mutations by day 127 suffered \n",
            "a different ... the 'initial' mutators (4.8*10-6), but lower than that of the E110K \n",
            "DnaQ ...\n",
            "URL: https://academic.oup.com/mbe/advance-article-pdf/doi/10.1093/molbev/msab067/36638362/msab067.pdf \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "InSTDmhVr-ax",
        "outputId": "f7406573-3dcb-4f42-9a5f-7efb403eb818"
      },
      "source": [
        "myquery= input(\"Please enter the information to be searched \")\n",
        "data =google_search(API_KEY,SEARCH_ENGINE_ID,myquery)\n",
        "results(data)"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Please enter the information to be searched  The ethical dimension of the decision leads me to think about myself and recognise, say, that I have certain talents, or that I would like to maximise my work-life balance. That is, they must recognise that duties can be ranked in a hierarchy (for example, to stop at an accident to render assistance trumps the promise of meeting for coffee); in a similar way, consequences can be ranked too. In moral decisions, in which the importance of others and their actual situation in the world, is recognised, community decisions are based on dialogue between all those on whom the decision impacts.\n",
            "Title: You say morals, I say ethics – what's the difference?\n",
            "Description: Sep 17, 2014 ... Certain customs or behaviours are recognised as good and others as bad, ... (for \n",
            "example, to stop at an accident to render assistance trumps the promise ... The \n",
            "ethical dimension of the decision leads me to think about myself and ... I have \n",
            "certain talents, or that I would like to maximise my work-life balance.\n",
            "URL: https://theconversation.com/you-say-morals-i-say-ethics-whats-the-difference-30913 \n",
            "\n",
            "Title: 10 strategies to support students and help them learn during the ...\n",
            "Description: Mar 17, 2020 ... Mays Imad offers 10 teaching strategies to support students and help ... to \n",
            "balance you. ... to take time off to be with his family, I would later work with him to \n",
            "help ... being in class has always offered me a sanctuary where I could tune ... I \n",
            "would've liked my teachers to do had I been a student who was sent ...\n",
            "URL: https://www.insidehighered.com/advice/2020/03/17/10-strategies-support-students-and-help-them-learn-during-coronavirus-crisis \n",
            "\n",
            "Title: What Every Leader Needs to Know About Followers\n",
            "Description: It's long overdue for leaders to acknowledge the importance of understanding \n",
            "their ... These distinctions have critical implications for how leaders should lead \n",
            "and ... followers according to their rank: They are low in the hierarchy and have \n",
            "less ... drives their subordinates can be a great help to themselves, their followers\n",
            ", and ...\n",
            "URL: https://hbr.org/2007/12/what-every-leader-needs-to-know-about-followers \n",
            "\n",
            "Title: A professor gives three reasons why he doesn't allow cellphones in ...\n",
            "Description: Jan 28, 2020 ... Kevin Brown gives three reasons he decided to make it a policy in his classroom. \n",
            "... have a wide variety of reasons to want students to have their cellphones. Some \n",
            "believe that students should learn to self-regulate (or even sink ... That said, \n",
            "something changed in my classes and me that has led me to take ...\n",
            "URL: https://www.insidehighered.com/advice/2020/01/28/professor-gives-three-reasons-why-he-doesnt-allow-cellphones-his-classes-opinion \n",
            "\n",
            "Title: Sarcasm, Self-Deprecation, and Inside Jokes: A User's Guide to ...\n",
            "Description: Some leaders use humor instinctively; many more could wield it purposefully.\n",
            "URL: https://hbr.org/2020/07/sarcasm-self-deprecation-and-inside-jokes-a-users-guide-to-humor-at-work \n",
            "\n",
            "Title: Employee Engagement & Loyalty Statistics: The Ultimate Collection\n",
            "Description: Jan 7, 2019 ... Up to two-thirds of employees say they may leave their jobs in 2020 (Achievers) \n",
            "... recognized have a lower engagement and are twice as likely to say they'll ... 77\n",
            "% of companies focus on employee experience to increase retention ... they \n",
            "seriously consider how a position will affect their work-life balance, ...\n",
            "URL: https://blog.accessperks.com/employee-engagement-loyalty-statistics-the-ultimate-collection \n",
            "\n",
            "Title: Leadership, Roles, and Problem Solving in Groups\n",
            "Description: How do groups solve problems and make decisions in order to accomplish their \n",
            "task? ... Whether you consider yourself a leader or not, all members of a group \n",
            "can ... out leadership positions not because they possess leadership skills and \n",
            "have ... identify and begin to address any interpersonal or communication issues\n",
            " ...\n",
            "URL: https://2012books.lardbucket.org/books/a-primer-on-communication-studies/s14-leadership-roles-and-problem-s.html \n",
            "\n",
            "Title: 9 key issues with Amazon's corporate culture\n",
            "Description: Aug 19, 2015 ... \"A lot of people who work there feel this tension: It's the greatest place I ... For \n",
            "some people it doesn't work.\" ... From its early years, Mr. Bezos led Amazon to \n",
            "resist the forces he thought ... \"You can work long, hard or smart, but at Amazon.\n",
            "com you can't ... Disregarding employees' need for work-life balance.\n",
            "URL: https://www.beckershospitalreview.com/hospital-management-administration/9-key-issues-with-amazon-s-corporate-culture.html \n",
            "\n",
            "Title: Shaping an Ethical Workplace Culture\n",
            "Description: of a competent, licensed professional should be sought. ... It will also provide \n",
            "specific suggestions for shaping your workplace culture in a ... By developing a \n",
            "more ethical culture, HR professionals can help unlock ... Management, HRM´s \n",
            "Role in Corporate Social and Environmental ... workplace cultures, they would \n",
            "have.\n",
            "URL: https://www.shrm.org/hr-today/trends-and-forecasting/special-reports-and-expert-views/Documents/Ethical-Workplace-Culture.pdf \n",
            "\n",
            "Title: Stories From Experts About the Impact of Digital Life | Pew Research ...\n",
            "Description: Jul 3, 2018 ... Fifty-fifty anecdotes: How digital life has been both positive and negative ... This \n",
            "makes it sound as though my answer should have been that these ... I am, at last, \n",
            "surrounded by a large number of people like myself, but with an ... It helps me \n",
            "make quicker, more-informed decisions and it can connect me to ...\n",
            "URL: https://www.pewresearch.org/internet/2018/07/03/fifty-fifty-anecdotes-how-digital-life-has-been-both-positive-and-negative/ \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cNb11MsWr-o9",
        "outputId": "e8ae927d-5c71-4ed7-81f5-77fefc0735cf"
      },
      "source": [
        "myquery= input(\"Please enter the information to be searched \")\n",
        "data =google_search(API_KEY,SEARCH_ENGINE_ID,myquery)\n",
        "results(data)"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Please enter the information to be searched  Full size image  Shared symptoms indicate shared protein interactions  To further assess whether shared symptoms indicate not only shared genetic associations, but also close interaction of the corresponding proteins, we integrated five publicly available PPI databases (Supplementary Methods) and constructed disease networks in which two diseases are linked if they have shared 1st and 2nd order PPI interactions, respectively: shared 1st order PPI means that two diseases have associated proteins that directly interact within the PPI network, while shared 2nd order PPI means that they are connected by a path of length two (Fig. Full size image  Results  Construction of the HSDN  We extracted 7,109,429 (about 35.5% in over twenty million records) PubMed bibliographic records with one or more disease/symptom terms in the MeSH metadata field (see Methods), yielding a total of 4,442 disease terms and 322 symptom terms (Supplementary Data 1 and 2). figure2  (a) The twenty most frequent disease terms in the MeSH fields of PubMed records, containing eight types of cancers (for example, breast neoplasms, lung neoplasms), four types of vascular diseases (for example, hypertension, myocardial infarction and coronary diseases), HIV infections, asthma, obesity, pain, rheumatoid arthritis, type 2 diabetes and two mental diseases.\n",
            "Title: Human symptoms–disease network | Nature Communications\n",
            "Description: Jun 26, 2014 ... Here, we use a large-scale biomedical literature database to construct ... shared \n",
            "symptoms and shared genes or protein–protein interactions of two ... (c) \n",
            "Integrating both disease–gene associations and PPI databases to obtain ... To \n",
            "further assess whether shared symptoms indicate not only shared genetic ...\n",
            "URL: https://www.nature.com/articles/ncomms5212 \n",
            "\n",
            "Title: (PDF) Human symptoms–disease network\n",
            "Description: Jun 26, 2014 ... ... shared symptoms and. shared genes or protein–protein interactions of two \n",
            "diseases could ... protein interactions.To. further assess whether shared \n",
            "symptoms indicate not only shared. genetic associations, but also close \n",
            "interaction of the corre-. sponding proteins, we integrated ﬁve publicly available \n",
            "PPI.\n",
            "URL: https://www.researchgate.net/publication/263430196_Human_symptoms-disease_network \n",
            "\n",
            "Title: Unraveling human protein interaction networks underlying co ...\n",
            "Description: Apr 14, 2014 ... While distinct diseases share pathological symptoms and various ... C is the \n",
            "threshold of D(p n , p m ), which indicates sufficient proximity between two \n",
            "proteins. We ... To produce valid PPI networks, we only used protein interactions \n",
            "with ... and a total of 317 genes related to five diseases (obesity, T2DM, ...\n",
            "URL: https://translational-medicine.biomedcentral.com/articles/10.1186/1479-5876-12-99 \n",
            "\n",
            "Title: Protein-protein interactions underlying the behavioral and ...\n",
            "Description: Jan 17, 2020 ... In this study, we explored the molecular interactions between AD and BPSD \n",
            "using protein-protein interaction (PPI) networks built from OMIM ...\n",
            "URL: https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0226021 \n",
            "\n",
            "Title: Enriching Human Interactome with Functional Mutations to Detect ...\n",
            "Description: Nov 15, 2019 ... mutations into the protein–protein interaction (PPI) network to ... genomic \n",
            "variations will be available soon [7]. ... proteins and responsible for the disease \n",
            "phenotype. ... When a full-length PPI could not be modeled, we only modeled the \n",
            "... bipolar disorder, that shared symptoms also shared functional ...\n",
            "URL: https://www.mdpi.com/2073-4425/10/11/933/pdf \n",
            "\n",
            "Title: Protein-protein interactions underlying the behavioral and ...\n",
            "Description: Jan 17, 2020 ... We first chose causative genes related to these symptoms based on prior ... Red \n",
            "color indicates the shared proteins between AD with one BPSD ... It has \n",
            "integrated known, experimental, and predicted PPIs for five ... To increase the \n",
            "data reliability of protein interactions, all predicted ... If not, e(u,v) is zero.\n",
            "URL: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6968845/ \n",
            "\n",
            "Title: Predicting whole genome protein interaction networks from primary ...\n",
            "Description: Sep 10, 2013 ... The large-scale identification of physical protein-protein interactions (PPIs) is an \n",
            "... We also find that the predicted interactions are biologically meaningful, ... of \n",
            "publicly-available software that would enable domain-based PPI prediction ... (A) \n",
            "The frequency of shared KEGG pathways and (B) the mean GO ...\n",
            "URL: https://link.springer.com/article/10.1186/1471-2164-14-608 \n",
            "\n",
            "Title: Network Based Integrated Analysis of Phenotype-Genotype Data for ...\n",
            "Description: Then the disease-gene associations and protein-protein interactions are ... \n",
            "symptoms, which are not recorded in the benchmark data set, but have been ... \n",
            "The availability of large-scale data of phenotype-genotype associations like ... of \n",
            "shared genetic associations and the extent to which their associated proteins \n",
            "interact.\n",
            "URL: https://www.hindawi.com/journals/bmri/2014/435853/ \n",
            "\n",
            "Title: Genome-wide prioritization of disease genes and identification of ...\n",
            "Description: Sep 3, 2009 ... We integrate 16 genomic features to construct an evidence-weighted ... evidence \n",
            "for functional association, such as protein-protein physical-interaction data ... not \n",
            "only whether diseases share associated genes, but also whether gene ... of \n",
            "common known disease genes or common pathological symptoms.\n",
            "URL: https://genomebiology.biomedcentral.com/articles/10.1186/gb-2009-10-9-r91 \n",
            "\n",
            "Title: Pathway and network embedding methods for prioritizing psychiatric ...\n",
            "Description: uncovered shared genetic modules across multiple psychiatric disorders — \n",
            "providing an ... interaction networks and embedding-based methods, we build a \n",
            "pipeline to ... we demonstrate that gene-expression-derived pathway features \n",
            "can ... node2vec PPI network embeddings to assess proximity between drug and \n",
            "disease ...\n",
            "URL: https://psb.stanford.edu/psb-online/proceedings/psb20/Pershad.pdf \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VFS_-Owpr-yP",
        "outputId": "0d467df9-9848-445a-88c2-19656275b4fc"
      },
      "source": [
        "myquery= input(\"Please enter the information to be searched \")\n",
        "data =google_search(API_KEY,SEARCH_ENGINE_ID,myquery)\n",
        "results(data)"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Please enter the information to be searched  Cole keeps up with what we're doing at CatholicCulture.org; recently he surmised from some of my previous remarks on music that I might be interested in his dissertation, which had been published as a book in 1993. Even so, the book provides significant food for thought on a difficult subject, and the notes and bibliography demonstrate how thoroughly the author researched every aspect of it. Even those interested in studying highly specialized works by particular philosophers, musicians and researchers would be wise to put them off until they’ve read Music & Morals—a book that brings order out of the chaos of mere opinion concerning the moral dimensions of music.\n",
            "Title: Flannery O'Connor's Art And The French Renouveau Catholique: A ...\n",
            "Description: Over the ten years it has taken to complete this dissertation, I have been keenly \n",
            "aware that no ... carries not a hint of the ressentiment Nietzsche so despised; it is \n",
            "his glory. In his ... could not have written any of them without the impact he made \n",
            "on my life from my ... Appropriately, the most recent of these books, Daniel.\n",
            "URL: https://scholarcommons.sc.edu/cgi/viewcontent.cgi?article=5513&context=etd \n",
            "\n",
            "Title: MISSED IDENTITY: COLLECTIVE MEMORY, ADINA DE ZAVALA ...\n",
            "Description: haunted me because I so believed her story must be told. ... passionate \n",
            "cheerleader and reader has been my mother, retired judge ... surname were \n",
            "limited in racially divided San Antonio. ... has been doing its part to make that \n",
            "happen,” she wrote about Adina De Zavala, ... The recent emphasis on historical \n",
            "memory and, in.\n",
            "URL: https://rc.library.uta.edu/uta-ir/bitstream/handle/10106/11820/Cottraux_uta_2502M_12162.pdf?sequence=1 \n",
            "\n",
            "Title: EVELYN WAUGH, GRAHAM GREENE, AND CATHOLICISM: 1928 ...\n",
            "Description: My girlfriends have also been great supporters – they have kept my spirits high, \n",
            "and have always ... early writings published prior to their so-called 'Catholic' \n",
            "novels. ... 30 Loewenstein argues that Greene's anti-Semitic thoughts were 'the ... \n",
            "My thesis engages with all the novels and travel books written by Waugh and \n",
            "Greene.\n",
            "URL: https://core.ac.uk/download/pdf/5222987.pdf \n",
            "\n",
            "Title: Martin Scorsese and Film Culture: Radically Contextualizing the ...\n",
            "Description: Martin Scorsese and his films have been analyzed extensively since he began \n",
            "making films over four ... Scorsese the auteur is less significant to this work than \n",
            "... seventies, Movie had shifted interest to contemporary American film. Starting \n",
            "with ... Stylistic breaks with the past were usually brief and predominantly tied to \n",
            "story.\n",
            "URL: https://central.bac-lac.gc.ca/.item?id=NR47489&op=pdf&app=Library&oclc_number=697931817 \n",
            "\n",
            "Title: 10.1007/978-3-642-22558-1.pdf\n",
            "Description: even in the absence of a specific statement, that such names are exempt from the \n",
            "... Do it now! Write the book of your life! Everything you have done so far is just \n",
            "preparation. Sorry if ... He also met my mother, Jeanette Anderson, a music major \n",
            "... There has never been any previous written account of our unique Boulder High\n",
            ".\n",
            "URL: http://link.springer.com/content/pdf/10.1007%2F978-3-642-22558-1.pdf \n",
            "\n",
            "Title: Journal of Christian Legal Thought\n",
            "Description: Protestant, Roman Catholic, and Orthodox perspectives are welcome as within \n",
            "the broad stream of Christianity. However, articles and essays do not necessarily\n",
            " ...\n",
            "URL: https://www.christianlegalsociety.org/sites/default/files/2020-11/CLSJournal_2020%20No2_web.pdf \n",
            "\n",
            "Title: Blake and Conflict\n",
            "Description: This book is printed on paper suitable for recycling and made from fully managed \n",
            "and ... At present he is completing a study of Coleridge and the fine arts with the \n",
            "aid of an Andrew ... If Blake explores the idea of conflict in his work, then it also \n",
            "seems to ... Recent years have witnessed a remarkable surge of interest in \n",
            "Blake's.\n",
            "URL: https://link.springer.com/content/pdf/10.1057%2F9780230584280.pdf \n",
            "\n",
            "Title: Old Yankee Women: Life Histories and Cultural Significance\n",
            "Description: Tydings, my husband, says he doesn't recall now how we stepped around piles ... \n",
            "his kindness in being so available to help, this would have been a much more \n",
            "difficult ... that women up to recent times seemed to be missing persons in \n",
            "gerontology. ... without a second thought, lending him some rare books from my \n",
            "library.\n",
            "URL: https://drum.lib.umd.edu/bitstream/handle/1903/10815/Tydings_umd_0117E_11462.pdf?sequence=1 \n",
            "\n",
            "Title: Download PDF\n",
            "Description: Harrison's ritualistic interpretation of religion has been attributed to many ... \n",
            "professional life, academic appointments, publication of major books, travels, and \n",
            "... Chapter 6 will then turn back to the rituals of the High Church to illustrate how \n",
            "they ... these myths were then picked up in the scholarship that followed (129-160\n",
            ").\n",
            "URL: https://fsu.digital.flvc.org/islandora/object/fsu:168356/datastream/PDF/download/citation.pdf \n",
            "\n",
            "Title: Examining Biblical Certainty of Salvation Among the Elderly in ...\n",
            "Description: One thing can be certain: those aged 80 and older have already eclipsed the ... \n",
            "questionnaire, individual follow-up, and a self-assessment were provided for \n",
            "each ... heart may fail, but God is the strength of my heart and my portion forever. \n",
            "... Thesis Statement . ... When Jesus saw his mother and the disciple whom he \n",
            "loved.\n",
            "URL: https://digitalcommons.liberty.edu/cgi/viewcontent.cgi?article=3611&context=doctoral \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WDQE8HYVsGjG"
      },
      "source": [
        "myquery= input(\"Please enter the information to be searched \")\n",
        "data =google_search(API_KEY,SEARCH_ENGINE_ID,myquery)\n",
        "results(data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7BfH1ihIsG1x"
      },
      "source": [
        "myquery= input(\"Please enter the information to be searched \")\n",
        "data =google_search(API_KEY,SEARCH_ENGINE_ID,myquery)\n",
        "results(data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lY_dZSLIMMuo"
      },
      "source": [
        "URL_link =only_link(data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K-ZywY8BZ61E"
      },
      "source": [
        "First link found by the cosine similarity is the original. Will try with other document"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IOvSNlYepvHE"
      },
      "source": [
        "#Method 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HdgDCCUSp_zC",
        "outputId": "b2b795e2-16d2-473d-9c73-0b749b0820d0"
      },
      "source": [
        "from gensim.summarization.summarizer import summarize\n",
        "# Summarize text using gensim\n",
        "gen_summary=summarize(textkkkk,word_count=50)\n",
        "print(gen_summary)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The new architectures directly follow those proposed in our earlier work [13, 14], where it was found that neural network language model can be successfully trained in two steps: first, continuous word vectors are learned using simple model, and then the N-gram NNLM is trained on top of these distributed representations of words.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DiEkYpXg1HWT",
        "outputId": "fd4cad85-2664-4da3-ed75-b36ae92e6c60"
      },
      "source": [
        "summarize(textevol,word_count=50)\n",
        "print(timeit.Timer().timeit(number=2))"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.6969997886917554e-06\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U6YdjiGDxgQm",
        "outputId": "747e9a30-8084-4b3e-9f54-c7ef07cd6a78"
      },
      "source": [
        "from resource import getrusage, RUSAGE_SELF\n",
        "summarize(textevol,word_count=50)\n",
        "print(getrusage(RUSAGE_SELF))"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "resource.struct_rusage(ru_utime=1012.034078, ru_stime=6.106212, ru_maxrss=1942724, ru_ixrss=0, ru_idrss=0, ru_isrss=0, ru_minflt=1251938, ru_majflt=1063, ru_nswap=0, ru_inblock=352704, ru_oublock=98832, ru_msgsnd=0, ru_msgrcv=0, ru_nsignals=0, ru_nvcsw=64661, ru_nivcsw=396824)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GNJ_pdf37nqD",
        "outputId": "6e402966-3894-437c-f941-5afa542325ca"
      },
      "source": [
        "lp = LineProfiler()\n",
        "lp_wrapper = lp(summarize)\n",
        "lp_wrapper(textevol,word_count=50)\n",
        "lp.print_stats()"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Timer unit: 1e-06 s\n",
            "\n",
            "Total time: 13.1246 s\n",
            "File: /usr/local/lib/python3.7/dist-packages/gensim/summarization/summarizer.py\n",
            "Function: summarize at line 376\n",
            "\n",
            "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
            "==============================================================\n",
            "   376                                           def summarize(text, ratio=0.2, word_count=None, split=False):\n",
            "   377                                               \"\"\"Get a summarized version of the given text.\n",
            "   378                                           \n",
            "   379                                               The output summary will consist of the most representative sentences\n",
            "   380                                               and will be returned as a string, divided by newlines.\n",
            "   381                                           \n",
            "   382                                               Note\n",
            "   383                                               ----\n",
            "   384                                               The input should be a string, and must be longer than :const:`~gensim.summarization.summarizer.INPUT_MIN_LENGTH`\n",
            "   385                                               sentences for the summary to make sense.\n",
            "   386                                               The text will be split into sentences using the split_sentences method in the :mod:`gensim.summarization.texcleaner`\n",
            "   387                                               module. Note that newlines divide sentences.\n",
            "   388                                           \n",
            "   389                                           \n",
            "   390                                               Parameters\n",
            "   391                                               ----------\n",
            "   392                                               text : str\n",
            "   393                                                   Given text.\n",
            "   394                                               ratio : float, optional\n",
            "   395                                                   Number between 0 and 1 that determines the proportion of the number of\n",
            "   396                                                   sentences of the original text to be chosen for the summary.\n",
            "   397                                               word_count : int or None, optional\n",
            "   398                                                   Determines how many words will the output contain.\n",
            "   399                                                   If both parameters are provided, the ratio will be ignored.\n",
            "   400                                               split : bool, optional\n",
            "   401                                                   If True, list of sentences will be returned. Otherwise joined\n",
            "   402                                                   strings will bwe returned.\n",
            "   403                                           \n",
            "   404                                               Returns\n",
            "   405                                               -------\n",
            "   406                                               list of str\n",
            "   407                                                   If `split` **OR**\n",
            "   408                                               str\n",
            "   409                                                   Most representative sentences of given the text.\n",
            "   410                                           \n",
            "   411                                               \"\"\"\n",
            "   412                                               # Gets a list of processed sentences.\n",
            "   413         1     322375.0 322375.0      2.5      sentences = _clean_text_by_sentences(text)\n",
            "   414                                           \n",
            "   415                                               # If no sentence could be identified, the function ends.\n",
            "   416         1          4.0      4.0      0.0      if len(sentences) == 0:\n",
            "   417                                                   logger.warning(\"Input text is empty.\")\n",
            "   418                                                   return [] if split else u\"\"\n",
            "   419                                           \n",
            "   420                                               # If only one sentence is present, the function raises an error (Avoids ZeroDivisionError).\n",
            "   421         1          1.0      1.0      0.0      if len(sentences) == 1:\n",
            "   422                                                   raise ValueError(\"input must have more than one sentence\")\n",
            "   423                                           \n",
            "   424                                               # Warns if the text is too short.\n",
            "   425         1          1.0      1.0      0.0      if len(sentences) < INPUT_MIN_LENGTH:\n",
            "   426                                                   logger.warning(\"Input text is expected to have at least %d sentences.\", INPUT_MIN_LENGTH)\n",
            "   427                                           \n",
            "   428         1      60904.0  60904.0      0.5      corpus = _build_corpus(sentences)\n",
            "   429                                           \n",
            "   430         1   12739818.0 12739818.0     97.1      most_important_docs = summarize_corpus(corpus, ratio=ratio if word_count is None else 1)\n",
            "   431                                           \n",
            "   432                                               # If couldn't get important docs, the algorithm ends.\n",
            "   433         1          3.0      3.0      0.0      if not most_important_docs:\n",
            "   434                                                   logger.warning(\"Couldn't get relevant sentences.\")\n",
            "   435                                                   return [] if split else u\"\"\n",
            "   436                                           \n",
            "   437                                               # Extracts the most important sentences with the selected criterion.\n",
            "   438         1       1499.0   1499.0      0.0      extracted_sentences = _extract_important_sentences(sentences, corpus, most_important_docs, word_count)\n",
            "   439                                           \n",
            "   440                                               # Sorts the extracted sentences by apparition order in the original text.\n",
            "   441         1          9.0      9.0      0.0      extracted_sentences.sort(key=lambda s: s.index)\n",
            "   442                                           \n",
            "   443         1          7.0      7.0      0.0      return _format_results(extracted_sentences, split)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jRjyG-Tavg5r",
        "outputId": "2d7d3504-9aff-43cc-b744-7311acbf07c7"
      },
      "source": [
        "gen_summary=summarize(textevol,word_count=50)\n",
        "print(gen_summary)"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "non-mutator clones extracted from the same time point and population (Table S3, discussed\n",
            "accumulated up to three years under LTSP, within non-mutator clones, are likely adaptive.\n",
            "In population 3, mutators and non-mutator clones coexist at high frequencies up to three years\n",
            "non-mutator and mutator population 3 clones extracted from the final time point (day 1095),\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jxigQa8F3Z9b",
        "outputId": "4bb103a3-f6ee-44ee-a025-5be990cef9da"
      },
      "source": [
        "myquery= input(\"Please enter the information to be searched \")\n",
        "data =google_search(API_KEY,SEARCH_ENGINE_ID,myquery)\n",
        "results(data)"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Please enter the information to be searched non-mutator clones extracted from the same time point and population (Table S3, discussed accumulated up to three years under LTSP, within non-mutator clones, are likely adaptive. In population 3, mutators and non-mutator clones coexist at high frequencies up to three years non-mutator and mutator population 3 clones extracted from the final time point\n",
            "Title: Dynamics of adaptation during three years of evolution under long ...\n",
            "Description: non-mutator clones extracted from the same time point and population (Table S3, \n",
            "... Mutations accumulated up to three years under LTSP accumulate in an ... In \n",
            "population 3, mutators and non-mutator clones coexist at high frequencies up to ...\n",
            "URL: https://academic.oup.com/mbe/advance-article-pdf/doi/10.1093/molbev/msab067/36638362/msab067.pdf \n",
            "\n",
            "Title: Rapid Genetic Adaptation during the First Four Months of Survival ...\n",
            "Description: Mar 28, 2017 ... E. coli cells can be maintained in LTSP for a large number of years, without any ... \n",
            "Mean number of viable cells with time in five LTSP populations ... number of \n",
            "mutations than nonmutator clones, extracted at the same time points (P ... Yet, \n",
            "even nonmutator clones accumulate on average eight mutations per ...\n",
            "URL: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5455981/ \n",
            "\n",
            "Title: the of and to a in for is on s that by this with i you it not or be are from ...\n",
            "Description: the of and to a in for is on s that by this with i you it not or be are from at as your ... \n",
            "top people had list n name just over state year day into email two health world \n",
            "next ... mr live large gallery table register ve june however october november \n",
            "market ... ist unemployment fa verizon enhancement clone scored dicks \n",
            "newcastle cab ...\n",
            "URL: https://web.mit.edu/adamrose/Public/googlelist \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "phsoDqFfvhK1",
        "outputId": "417e476c-ac79-4d3a-d75e-dcfe0945d673"
      },
      "source": [
        "gen_summary=summarize(texthS,word_count=50)\n",
        "print(gen_summary)\n"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Here, we use a large-scale biomedical literature database to construct a symptom-based human disease network and investigate the connection between clinical manifestations of diseases and their underlying molecular interactions.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EAXiAUTrvhT1",
        "outputId": "3e93fe42-69f3-4b64-9f8f-0f730f054b23"
      },
      "source": [
        "gen_summary=summarize(textmorals,word_count=50)\n",
        "print(gen_summary)\n"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "In moral decisions, in which the importance of others and their actual situation in the world, is recognised, community decisions are based on dialogue between all those on whom the decision impacts.\n",
            "Collecting the facts precedes any ethical or moral decision-making.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3TtJemiPvhaR",
        "outputId": "c13c0fde-91e7-4cc6-a89f-84d89afc74aa"
      },
      "source": [
        "gen_summary=summarize(textmus,word_count=50)\n",
        "print(gen_summary)"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Happily, Fr. Cole keeps up with what we're doing at CatholicCulture.org; recently he surmised from some of my previous remarks on music that I might be interested in his dissertation, which had been published as a book in 1993.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LAONvY6xvhj7",
        "outputId": "53c78dbe-9704-420e-a2b6-1e263d6699a5"
      },
      "source": [
        "gen_summary=summarize(textweb,word_count=50)\n",
        "print(gen_summary)"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "This is a powerful project because you’ll be able to apply the same process and the same tools to any static website out there on the World Wide Web. You can download the source code for the project and all examples in this tutorial by clicking on the link below:\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ori8wvzw6669"
      },
      "source": [
        "method 3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YB6imJJVV7J5"
      },
      "source": [
        "Using Pipeline- It crashes when sentences are longer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z5CIxZwVGPVV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 468,
          "referenced_widgets": [
            "fd87f87df6d04005a944c292fd32d0a0",
            "11f47d598b7a45699344cae1552f748f",
            "cebe63a106124624962667640859eaeb",
            "62507284346e447fb2681cff30b499ea",
            "39ab9c2381cf4eb29f590153192030d2",
            "7f983040806049ea92a55f02d091840d",
            "36a5c27f190b4d8fbac81d93cef2725e",
            "69f64b9617ca4183bf05cfc02db77538",
            "7bf879027cf642aaa7e0ff07a7a72151",
            "3739be4905c34d96869e334ff0edb82a",
            "383777a27bff479c9c55c9d1af56280a",
            "c7393925efd3402590c58daac5ed0e29",
            "e546d8dd9fcb484b9ac9b7cffe301249",
            "a945596f2a6144ddab88b184f3fa94bb",
            "b1913ab6917b436f824d478cb0c3a54a",
            "959b0df3074a43dbae0a6efb8d191a38"
          ]
        },
        "outputId": "4fd23f93-bd72-4cd7-bc61-f0239cdcd034"
      },
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# using pipeline API for summarization task\n",
        "summarization = pipeline(\"summarization\")\n",
        "\n",
        "summary_text = summarization(sentences)\n",
        "print(\"Summary:\", summary_text)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "fd87f87df6d04005a944c292fd32d0a0",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=1621.0, style=ProgressStyle(description…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7bf879027cf642aaa7e0ff07a7a72151",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=1222317369.0, style=ProgressStyle(descr…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-37-7eaadaebab92>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# using pipeline API for summarization task\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0msummarization\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"summarization\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0msummary_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msummarization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/pipelines/__init__.py\u001b[0m in \u001b[0;36mpipeline\u001b[0;34m(task, model, config, tokenizer, framework, revision, use_fast, **kwargs)\u001b[0m\n\u001b[1;32m    338\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_default_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtargeted_task\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframework\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtask_options\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    339\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 340\u001b[0;31m     \u001b[0mframework\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mframework\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mget_framework\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    342\u001b[0m     \u001b[0mtask_class\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtargeted_task\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"impl\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargeted_task\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mframework\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/pipelines/base.py\u001b[0m in \u001b[0;36mget_framework\u001b[0;34m(model, revision)\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m                 \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrevision\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrevision\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m                 \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTFAutoModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrevision\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrevision\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/auto/modeling_auto.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    773\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mMODEL_MAPPING\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    774\u001b[0m             return MODEL_MAPPING[type(config)].from_pretrained(\n\u001b[0;32m--> 775\u001b[0;31m                 \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mmodel_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    776\u001b[0m             )\n\u001b[1;32m    777\u001b[0m         raise ValueError(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   1032\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1033\u001b[0m         \u001b[0;31m# Instantiate model.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1034\u001b[0;31m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mmodel_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mmodel_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1035\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1036\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstate_dict\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfrom_tf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/bart/modeling_bart.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m   1089\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshared\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEmbedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0md_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1090\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1091\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBartEncoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshared\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1092\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBartDecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshared\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1093\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/bart/modeling_bart.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, config, embed_tokens)\u001b[0m\n\u001b[1;32m    676\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    677\u001b[0m         )\n\u001b[0;32m--> 678\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModuleList\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mBartEncoderLayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder_layers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    679\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayernorm_embedding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLayerNorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membed_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    680\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/bart/modeling_bart.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    676\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    677\u001b[0m         )\n\u001b[0;32m--> 678\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModuleList\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mBartEncoderLayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder_layers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    679\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayernorm_embedding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLayerNorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membed_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    680\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/bart/modeling_bart.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m    280\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivation_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mACT2FN\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivation_function\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivation_dropout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivation_dropout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 282\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder_ffn_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    283\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder_ffn_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinal_layer_norm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLayerNorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, in_features, out_features, bias)\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister_parameter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'bias'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreset_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mreset_parameters\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreset_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m         \u001b[0minit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkaiming_uniform_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m             \u001b[0mfan_in\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_calculate_fan_in_and_fan_out\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/init.py\u001b[0m in \u001b[0;36mkaiming_uniform_\u001b[0;34m(tensor, a, mode, nonlinearity)\u001b[0m\n\u001b[1;32m    377\u001b[0m     \u001b[0mbound\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3.0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mstd\u001b[0m  \u001b[0;31m# Calculate uniform bounds from standard deviation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    378\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 379\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muniform_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mbound\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbound\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    380\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xq3V3W-mJp1R"
      },
      "source": [
        "myquery= input(\"Please enter the information to be searched \")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YhLHD4qAT16i"
      },
      "source": [
        "Using Summarizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gJ_BFCoOZdHm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "22377bbc-c5f5-4630-e5ca-ce88d47c6409"
      },
      "source": [
        "from summarizer import Summarizer\n",
        "model = Summarizer()\n",
        "result = model(textevol, num_sentences=2)\n",
        "#full = \"\".join(result)\n",
        "print(result)"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1\n",
            " Dynamics of adaptation during three years of evolution under long-term stationary\n",
            " phase\n",
            " Sophia Katz1,*, Sarit Avrani2,*, Meitar Yavneh1\n",
            " , Sabrin Hilau1\n",
            " , Jonathan Gross1 and Ruth\n",
            " Hershberg1, †\n",
            " Affiliations:\n",
            " 1\n",
            " Rachel & Menachem Mendelovitch Evolutionary Processes of Mutation & Natural Selection\n",
            " Research Laboratory, Department of Genetics and Developmental Biology, the Ruth and Bruce\n",
            " Rappaport Faculty of Medicine, Technion-Israel Institute of Technology, Haifa 31096, Israel. This in turn provides greater resolution in determining the\n",
            " genetic changes associated with the observed evolutionary processes. In population 4 a\n",
            " majority of mutators carried a mutation within mutL, while a minority carried a mutation\n",
            " within the gene mutH. Interestingly, the three populations in which mutator clones were\n",
            " observed by day 64 our experiments, continued to include such clones, at observable\n",
            " frequencies, up to three years into the experiment (Figure 4A-C).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Lezxsjl62Ri",
        "outputId": "dd93c149-e372-4e46-cbe7-93dd4e9c60b5"
      },
      "source": [
        "print(timeit.Timer().timeit(number=2))"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.2600003174156882e-06\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ULWsNWn26-af",
        "outputId": "a99b1734-72c7-4fb5-be98-bb2255c0fc19"
      },
      "source": [
        "from resource import getrusage, RUSAGE_SELF\n",
        "summarize(textevol,word_count=50)\n",
        "print(getrusage(RUSAGE_SELF))"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "resource.struct_rusage(ru_utime=766.659009, ru_stime=30.724321, ru_maxrss=7796300, ru_ixrss=0, ru_idrss=0, ru_isrss=0, ru_minflt=2521375, ru_majflt=1406, ru_nswap=0, ru_inblock=2373952, ru_oublock=4473112, ru_msgsnd=0, ru_msgrcv=0, ru_nsignals=0, ru_nvcsw=35159, ru_nivcsw=532736)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 407
        },
        "id": "bayJK9sX8Qc_",
        "outputId": "af7e4204-c7f7-4242-ba81-45818d3361ad"
      },
      "source": [
        "lp = LineProfiler()\n",
        "lp_wrapper = lp(model)\n",
        "lp_wrapper(textkkkk, num_sentences=2)\n",
        "lp.print_stats()"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/line_profiler/line_profiler.py:79: UserWarning: Could not extract a code object for the object <summarizer.model_processors.Summarizer object at 0x7fda27094a50>\n",
            "  self.add_function(func)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-46-dab20dd18f9c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mlp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLineProfiler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mlp_wrapper\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mlp_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtextkkkk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_sentences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_stats\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/line_profiler/line_profiler.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, func)\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_coroutine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m             \u001b[0mwrapper\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrap_coroutine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m         \u001b[0;32melif\u001b[0m \u001b[0mis_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m             \u001b[0mwrapper\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrap_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/line_profiler/line_profiler.py\u001b[0m in \u001b[0;36mis_generator\u001b[0;34m(f)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \"\"\" Return True if a function is a generator.\n\u001b[1;32m     66\u001b[0m     \"\"\"\n\u001b[0;32m---> 67\u001b[0;31m     \u001b[0misgen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__code__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mco_flags\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0mCO_GENERATOR\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0misgen\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'Summarizer' object has no attribute '__code__'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jaPn_fv4IjSJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e7769e81-daad-4080-b476-dcabc78d6a62"
      },
      "source": [
        "myquery= input(\"Please enter the information to be searched \")\n",
        "data =google_search(API_KEY,SEARCH_ENGINE_ID,myquery)\n",
        "results(data)"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Please enter the information to be searched Dynamics of adaptation during three years of evolution under long-term stationary  phase  Sophia Katz1,*, Sarit Avrani2,*, Meitar Yavneh1  , Sabrin Hilau1  , Jonathan Gross1 and Ruth  Hershberg1,\n",
            "Title: Dynamics of adaptation during three years of evolution under long ...\n",
            "Description: Dynamics of adaptation during three years of evolution under long-term \n",
            "stationary phase. Sophia Katz1,*,Sarit Avrani2,*, Meitar Yavneh1, Sabrin Hilau1, \n",
            "Jonathan Gross1 and Ruth. Hershberg1, †. Affiliations: ... growth period, followed \n",
            "by a short stationary phase in which cell numbers remain constant. Shortly \n",
            "thereafter ...\n",
            "URL: https://academic.oup.com/mbe/advance-article-pdf/doi/10.1093/molbev/msab067/36638362/msab067.pdf \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "veXf8plLYl8p"
      },
      "source": [
        "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
        "model = T5ForConditionalGeneration.from_pretrained(\"t5-base\")\n",
        "tokenizer = T5Tokenizer.from_pretrained(\"t5-base\")"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4YTGRXzVafPy"
      },
      "source": [
        "inputs = tokenizer.encode(\"summarize: \" + textevol, return_tensors=\"pt\", max_length=1000, truncation=True)"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-t-T9Lw4azOy",
        "outputId": "079a37d9-0a1f-4359-e1ef-4fc51da7ba6f"
      },
      "source": [
        "outputs = model.generate(inputs, max_length=150, min_length=40, length_penalty=1.0, num_beams=4, early_stopping=True)\n",
        "#print(outputs)\n",
        "print(tokenizer.decode(outputs[0]))"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<pad> authors describe the dynamics of adaptation during the first three years spent under long-term stationary phase (LTSP) they show that during this time E. coli continuously adapts genetically, through the accumulation of mutations. short generation times enable investigators to rapidly observe bacterial evolution over many generations of growth.</s>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cTCKcud7_QLj",
        "outputId": "0010bd7a-66f4-4270-80a4-01ae6f135142"
      },
      "source": [
        "print(timeit.Timer().timeit(number=2))"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.2700002116616815e-06\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eCutf3hF_QWC",
        "outputId": "780eda3d-a5f9-4b5a-8ae0-f8b2f53bf253"
      },
      "source": [
        "print(getrusage(RUSAGE_SELF))"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "resource.struct_rusage(ru_utime=807.368318, ru_stime=46.728265, ru_maxrss=7796300, ru_ixrss=0, ru_idrss=0, ru_isrss=0, ru_minflt=2797691, ru_majflt=1460, ru_nswap=0, ru_inblock=4126344, ru_oublock=4473112, ru_msgsnd=0, ru_msgrcv=0, ru_nsignals=0, ru_nvcsw=38681, ru_nivcsw=536030)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "29UduNd89GHd",
        "outputId": "d8bd7f93-df29-4b6c-888e-0aa98c7f5988"
      },
      "source": [
        "lp = LineProfiler()\n",
        "lp_wrapper = lp(model.generate)\n",
        "lp_wrapper(inputs, max_length=150, min_length=40, length_penalty=1.0, num_beams=4, early_stopping=True)\n",
        "lp.print_stats()"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Timer unit: 1e-06 s\n",
            "\n",
            "Total time: 44.441 s\n",
            "File: /usr/local/lib/python3.7/dist-packages/torch/autograd/grad_mode.py\n",
            "Function: decorate_context at line 24\n",
            "\n",
            "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
            "==============================================================\n",
            "    24                                                   @functools.wraps(func)\n",
            "    25                                                   def decorate_context(*args, **kwargs):\n",
            "    26         1         52.0     52.0      0.0              with self.__class__():\n",
            "    27         1   44440937.0 44440937.0    100.0                  return func(*args, **kwargs)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "43VfKtrwQ4BI",
        "outputId": "035343e9-d6cc-445f-daac-779722aa86ad"
      },
      "source": [
        "myquery= input(\"Please enter the information to be searched \")\n",
        "data =google_search(API_KEY,SEARCH_ENGINE_ID,myquery)\n",
        "results(data)"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Please enter the information to be searched authors describe the dynamics of adaptation during the first three years spent under long-term stationary phase (LTSP) they show that during this time E. coli continuously adapts genetically, through the accumulation of mutations. short generation times enable investigators to rapidly observe bacterial evolution over many generations of growth.\n",
            "Title: Dynamics of adaptation during three years of evolution under long ...\n",
            "Description: Here we describe the dynamics of E. coli adaptation during the first ... of \n",
            "mutations accumulated appear to be adaptive under LTSP, reflected in an \n",
            "extremely convergent ... Short generation times enable investigators to rapidly \n",
            "observe ... Long-term stationary phase (LTSP) populations maintain constant \n",
            "viability over the.\n",
            "URL: https://academic.oup.com/mbe/advance-article-pdf/doi/10.1093/molbev/msab067/36638362/msab067.pdf \n",
            "\n",
            "Title: Long-Term Survival During Stationary Phase: Evolution and the ...\n",
            "Description: Evolution in Long-Term Stationary-Phase Batch Culture: Emergence of Divergent \n",
            "... Additional mutations accumulate in these Growth Advantage in Stationary ... of \n",
            "E. coli adaptation during the first 4 months (127 days) of survival under LTSP (2). \n",
            "... ... We show that maintenance of genetic variation under LTSP enables LTSP ...\n",
            "URL: https://www.researchgate.net/publication/7356327_Long-Term_Survival_During_Stationary_Phase_Evolution_and_the_GASP_Phenotype \n",
            "\n",
            "Title: Rapid Genetic Adaptation during the First Four Months of Survival ...\n",
            "Description: Mar 28, 2017 ... Mutations to these three sites are strongly antagonistically pleiotropic, in that they \n",
            "... bacteria show a remarkable ability to maintain themselves in spent media ... E. \n",
            "coli cells can be maintained in LTSP for a large number of years, ... under LTSP \n",
            "for prolonged periods, following a short initial phases of growth.\n",
            "URL: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5455981/ \n",
            "\n",
            "Title: (PDF) Adaptation of Escherichia coli to Long-Term Serial Passage in ...\n",
            "Description: Dec 6, 2020 ... PDF | Experimental evolution of bacterial populations in the ... years without the \n",
            "addition of nutrients (17–20). ... constant generation time and are presumably \n",
            "under very-low-stress to ... (three long-term serial passages) in rich medium, the \n",
            "LTSP survival ... Long-term serially passaged E. coli adapts to LTSP.\n",
            "URL: https://www.researchgate.net/publication/314294750_Adaptation_of_Escherichia_coli_to_Long-Term_Serial_Passage_in_Complex_Medium_Evidence_of_Parallel_Evolution \n",
            "\n",
            "Title: Adaptations Accumulated under Prolonged Resource Exhaustion ...\n",
            "Description: Aug 12, 2020 ... KEYWORDS: LTSP, bacterial evolution, long-term stationary phase, ... the \n",
            "dynamics of E. coli adaptation during the first 4 months (127 days) of survival \n",
            "under LTSP (2). ... From now on we will refer to mutations falling within these \n",
            "three sites ... Despite the fact that E. coli populations adapt rapidly in a highly ...\n",
            "URL: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7426164/ \n",
            "\n",
            "Title: Oxygen Consumption Rates of Bacteria under Nutrient-Limited ...\n",
            "Description: Following the death phase, during the long-term stationary phase (LTSP), QO2 ... \n",
            "The decline in oxygen consumption rates with time in both studies suggests that \n",
            "... Therefore, we studied how adaptations to starvation, cryptic growth, and the ... \n",
            "Experiments were conducted using three bacterial strains: Escherichia coli K-12\n",
            " ...\n",
            "URL: https://aem.asm.org/content/79/16/4921 \n",
            "\n",
            "Title: Conference Program Book\n",
            "Description: Session 3: Tutorial: Network Dynamics and Cell ... Building keys and select \n",
            "university phone numbers and addresses on the back ... Bioinformatics Facility \n",
            "Phase 1 ... computer models with these data, and finally to ... necessary “first-step\n",
            "” for any Systems Biology or Omics ... can be described in terms of equivalent \n",
            "groups.\n",
            "URL: https://www.cpe.vt.edu/icsb2017/ICSBConferenceProgramBook.pdf \n",
            "\n",
            "Title: 2018 Abstracts\n",
            "Description: Feb 14, 2018 ... the other with some studies showing differences between source material and ... \n",
            "Participants first read a vignette describing a negotiation situation with ... mutation\n",
            ", it is very efficient and will not require integration of foreign DNA. ... In Long-Term \n",
            "Stationary Phase (LTSP) of the Escherichia coli life cycle, cells ...\n",
            "URL: https://www.csudh.edu/Assets/csudh-sites/gsr/docs/Student-Research-Day/Abstract_SRD_2018_FINAL.pdf \n",
            "\n",
            "Title: 2018 Poster Authors and Abstracts – CSU Annual Biotechnology ...\n",
            "Description: In the first phase of the project, chalcones were tested on individual ... rate under \n",
            "nitrogen fixing conditions at 92°C and with a fastest doubling time of 3 hours. ... \n",
            "Abstract: YdeH is one of the nineteen diguanylate cyclases in Escherichia coli ... \n",
            "and Biochemistry, California State University, Long Beach, Presenting Author\n",
            "URL: http://www.csuperb.org/symposium/2018-poster-authors-and-abstracts/ \n",
            "\n",
            "Title: PROJECT SUMMARIES CSU Statewide Student Research ...\n",
            "Description: contribute to stage conversion during Toxoplasma pathogenesis? ... Initial step in \n",
            "building energy efficiency improvement is to predict its consumption. ... \n",
            "agricultural soil contaminated with E. coli O157:H7, Salmonella, and Listeria ... \n",
            "Our research's long-term goal is to learn enough about how organisms are \n",
            "naturally.\n",
            "URL: https://www.csueastbay.edu/csr/brief-project-descriptions%2C-names%2C-project-titles.pdf \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ZIHB8QlrcH7"
      },
      "source": [
        "from sumy.parsers.plaintext import PlaintextParser\n",
        "from sumy.nlp.tokenizers import Tokenizer"
      ],
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g7Ah6cDVqYpe",
        "outputId": "ab3d889b-d05d-4d23-9629-ef41dc7f106e"
      },
      "source": [
        "from sumy.summarizers.lsa import LsaSummarizer\n",
        "parser = PlaintextParser.from_string(textevol,Tokenizer(\"english\"))\n",
        "summarizer_lsa = LsaSummarizer()\n",
        "# Summarize using sumy LSA\n",
        "summary =summarizer_lsa(parser.document,2)\n",
        "lsa_summary=\"\"\n",
        "for sentence in summary:\n",
        "    lsa_summary+=str(sentence)  \n",
        "print(lsa_summary)"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accumulated mutations were very strongly enriched for functional categories, which combined with the high levels of convergence observed suggest that mutation accumulation was governed by strong positive selection (Avrani, et al. 2017).While overall mutation accumulation rates within mutators are substantially higher, the mutations accumulated tend to be enriched for transitions, which have a lower likelihood of disrupting protein sequences.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ClSYuezOCxpJ",
        "outputId": "ccf52faa-cf6a-46e7-8414-bfcc832b3c3c"
      },
      "source": [
        "print(timeit.Timer().timeit(number=2))"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.7000002117129043e-06\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8pfjbn3VDvQ2",
        "outputId": "5fc07da5-dd2e-4bd6-9520-e266af05aa96"
      },
      "source": [
        "print(getrusage(RUSAGE_SELF))"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "resource.struct_rusage(ru_utime=843.283514, ru_stime=59.786855, ru_maxrss=7796300, ru_ixrss=0, ru_idrss=0, ru_isrss=0, ru_minflt=2798391, ru_majflt=1472, ru_nswap=0, ru_inblock=4135416, ru_oublock=4473112, ru_msgsnd=0, ru_msgrcv=0, ru_nsignals=0, ru_nvcsw=40520, ru_nivcsw=538667)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 407
        },
        "id": "vU7Tsf7oD4EQ",
        "outputId": "d48bedbb-af3c-4f9d-ede3-72ece8842dbd"
      },
      "source": [
        "lp = LineProfiler()\n",
        "lp_wrapper = lp(summarizer_lsa)\n",
        "lp_wrapper(parser.document,2)\n",
        "lp.print_stats()"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/line_profiler/line_profiler.py:79: UserWarning: Could not extract a code object for the object <sumy.summarizers.lsa.LsaSummarizer object at 0x7fda41e183d0>\n",
            "  self.add_function(func)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-68-03b92e249045>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mlp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLineProfiler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mlp_wrapper\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msummarizer_lsa\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mlp_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdocument\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_stats\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/line_profiler/line_profiler.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, func)\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_coroutine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m             \u001b[0mwrapper\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrap_coroutine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m         \u001b[0;32melif\u001b[0m \u001b[0mis_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m             \u001b[0mwrapper\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrap_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/line_profiler/line_profiler.py\u001b[0m in \u001b[0;36mis_generator\u001b[0;34m(f)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \"\"\" Return True if a function is a generator.\n\u001b[1;32m     66\u001b[0m     \"\"\"\n\u001b[0;32m---> 67\u001b[0;31m     \u001b[0misgen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__code__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mco_flags\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0mCO_GENERATOR\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0misgen\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'LsaSummarizer' object has no attribute '__code__'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 380
        },
        "id": "YNhE4EVquno4",
        "outputId": "c5bbe927-7152-41e6-f90a-c8b906ebe0c4"
      },
      "source": [
        "myquery= input(\"Please enter the information to be searched \")\n",
        "\n",
        "somethin =\"Accumulated mutations were very strongly enriched for functional categories, which combined with the high levels of convergence observed suggest that mutation accumulation was governed by strong positive selection (Avrani, et al. 2017).While overall mutation accumulation rates within mutators are substantially higher, the mutations accumulated tend to be enriched for transitions, which have a lower likelihood of disrupting protein sequences\"\n",
        "data =google_search(API_KEY,SEARCH_ENGINE_ID,somethin)\n",
        "results(data)"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Please enter the information to be searched kkk\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-74-08e06f25812c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0msomethin\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;34m\"Accumulated mutations were very strongly enriched for functional categories, which combined with the high levels of convergence observed suggest that mutation accumulation was governed by strong positive selection (Avrani, et al. 2017).While overall mutation accumulation rates within mutators are substantially higher, the mutations accumulated tend to be enriched for transitions, which have a lower likelihood of disrupting protein sequences\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0mgoogle_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mAPI_KEY\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mSEARCH_ENGINE_ID\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msomethin\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mresults\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-10-e49897839d01>\u001b[0m in \u001b[0;36mresults\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m   \u001b[0msearch_items\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"items\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m   \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msearch_item\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msearch_items\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mtitle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msearch_item\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"title\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0msnippet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msearch_item\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"snippet\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not iterable"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NngzcB6e6Zb1"
      },
      "source": [
        "#Some websites have been ranked ie Paperswithcode, wikipedia, Researchgate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "id": "Rc018oeqIfvc",
        "outputId": "7d55172a-2ba6-47d6-be8f-b94404930de7"
      },
      "source": [
        "generate_summary( sentences,stopwords ,3)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "' Performance of several techniques has been already reported on this set, including N-gram models, LSA-based model [32], log-bilinear model [24] and a combination of recurrent neural networks that currently holds the state of the art performance of 55.4% accuracy on this benchmark [19]. Finally, we found that when we train high dimensional word vectors on a large amount of data, the resulting vectors can be used to answer very subtle semantic relationships between words, such as a city and the country it belongs to, e.g. However, several practical solutions were proposed for avoiding it; either using hierarchical versions of the softmax [25, 23, 18], or avoiding normalized models completely by using models that are not normalized during training [4, 9].'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nb54dfOe7PkM",
        "outputId": "170a11ac-745d-4595-82a1-7d8a9de863d9"
      },
      "source": [
        "myquery= input(\"Please enter the information to be searched \")\n",
        "data =google_search(API_KEY,SEARCH_ENGINE_ID,myquery)\n",
        "results(data)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Please enter the information to be searched Performance of several techniques has been already reported on this set, including N-gram models, LSA-based model [32], log-bilinear model [24] and a combination of recurrent neural networks that currently holds the state of the art performance of 55.4% accuracy on this benchmark [19]. Finally, we found that when we train high dimensional word vectors on a large amount of data, the resulting vectors can be used to answer very subtle semantic relationships between words, such as a city and the country it belongs to, e.g. However, several practical solutions were proposed for avoiding it; either using hierarchical versions of the softmax [25, 23, 18], or avoiding normalized models completely by using models that are not normalized during training [4, 9].\n",
            "Title: (PDF) Efficient Estimation of Word Representations in Vector Space\n",
            "Description: Oct 21, 2014 ... Furthermore, we show that these vectors provide state-of-the-art perfor- ... \n",
            "Recurrent neural network based language model has been proposed to ... been \n",
            "already reported on this set, including N-gram models, LSA-based model [32], \n",
            "log-bilinear ... performance of 55.4% accuracy on this benchmark [19].\n",
            "URL: https://www.researchgate.net/publication/234131319_Efficient_Estimation_of_Word_Representations_in_Vector_Space \n",
            "\n",
            "Title: Efficient Estimation of Word Representations in Vector Space\n",
            "Description: Sep 7, 2013 ... Furthermore, we show that these vectors provide state-of-the-art perfor- ... \n",
            "However, the simple techniques are at their limits in many tasks. ... Recurrent \n",
            "neural network based language model has been ... been already reported on this \n",
            "set, including N-gram models, LSA-based model [32], log-bilinear model ...\n",
            "URL: https://arxiv.org/pdf/1301.3781 \n",
            "\n",
            "Title: 论文学习：Word2Vec第一篇：Efficient Estimation of Word ...\n",
            "Description: 2021年3月1日 ... 对比了NNLM、RNNLM模型提出了CBOW和Skip-gram两种模型架构 ... best \n",
            "performing techniques based on different types of neural networks. ... that these \n",
            "vectors provide state-of-the-art performance on our test set ... An example is the \n",
            "popular N-gram model used for statistical language modeling - today, ...\n",
            "URL: https://zhuanlan.zhihu.com/p/353609023 \n",
            "\n",
            "Title: Representation and Transfer Learning Using Information-Theoretic ...\n",
            "Description: May 15, 2020 ... ideas that took shape after discussions with Lizhong and it has been an ... 4.8 \n",
            "Neural network classifier's output LLR for the example dataset in Figure 4.6 ... 2.1 \n",
            "Performance comparison between our model and other single architecture ... \n",
            "Clustering is one of many important techniques in unsupervised ...\n",
            "URL: https://dspace.mit.edu/bitstream/handle/1721.1/127008/1191230439-MIT.pdf?sequence=1&isAllowed=y \n",
            "\n",
            "Title: Robust and comprehensive joint image-text representations\n",
            "Description: This thesis investigates the joint modeling of visual and textual content of \n",
            "multimedia ... suffers from several deficiencies that may hinder the performance \n",
            "of cross-modal tasks ... Evaluations show that our approaches achieve state-of-\n",
            "the-art ... The research on multimedia retrieval and classification has been very \n",
            "active in the.\n",
            "URL: https://www.theses.fr/2017CNAM1096.pdf \n",
            "\n",
            "Title: Compressive Cross-Language Text Summarization\n",
            "Description: Mar 20, 2019 ... A word has a specific meaning based on its previous and its following words in a \n",
            "sentence. Therefore, we propose a new NN model that ...\n",
            "URL: https://hal.archives-ouvertes.fr/tel-02003886v2/document \n",
            "\n",
            "Title: Proceedings of the Eighteenth Conference on Computational ...\n",
            "Description: Jun 29, 2012 ... Probabilistic Modeling of Joint-context in Distributional Similarity. Oren Melamud, \n",
            "Ido ... performance on the test section of the Wall Street ... a series of experiments \n",
            "comparing state-of-the-art ... 1In many fields, including NLP, it has become good \n",
            "prac- ... Resources and benchmarks for parsing the language.\n",
            "URL: https://www.aclweb.org/anthology/W14-16.pdf \n",
            "\n",
            "Title: Download book PDF\n",
            "Description: Model Checking Games for a Fair Branching-Time Temporal Epistemic ... Texture \n",
            "Detection Using Neural Networks Trained on Examples of One ... Based on the \n",
            "market setting we describe above, we now define the market policies to ... use N-\n",
            "PP as pricing policy, which has been observed as a well-performing policy in.\n",
            "URL: https://link.springer.com/content/pdf/10.1007%2F978-3-642-10439-8.pdf \n",
            "\n",
            "Title: Proceedings of the 54th Annual Meeting of the Association for ...\n",
            "Description: Aug 8, 2016 ... 32. Improving Statistical Machine Translation Performance by ... Nonparametric \n",
            "Spherical Topic Modeling with Word Embeddings ... Deep Neural Networks for \n",
            "Syntactic Parsing of Morphologically Rich ... shallow approaches have been tried \n",
            "in the past. ... bag-of-n-grams vector (up to trigrams) and has di-.\n",
            "URL: https://www.aclweb.org/anthology/P16-2.pdf \n",
            "\n",
            "Title: standard 12-lead electrocardiogram: Topics by Science.gov\n",
            "Description: This new hand held smart phone 12 lead ECG recorder needs further ... The 12-\n",
            "lead Electrocardiogram (ECG) has been used to detect cardiac ... The QRS \n",
            "pattern matching model shows stable performance for verification of 10 to ... The \n",
            "second set was a test set (n=116 ECGs) in which the J-wave status (present/\n",
            "absent) ...\n",
            "URL: https://www.science.gov/topicpages/s/standard+12-lead+electrocardiogram.html \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pXAL1nZv76zh",
        "outputId": "7307227d-00dc-494a-cd17-abf6ad696c8c"
      },
      "source": [
        "myquery= input(\"Please enter the information to be searched \")\n",
        "data =google_search(API_KEY,SEARCH_ENGINE_ID,myquery)\n",
        "results(data)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Please enter the information to be searched python\n",
            "Title: Welcome to Python.org\n",
            "Description: The official home of the Python Programming Language.\n",
            "URL: https://www.python.org/ \n",
            "\n",
            "Title: Python (programming language) - Wikipedia\n",
            "Description: Python is an interpreted, high-level and general-purpose programming language\n",
            ". Python's design philosophy emphasizes code readability with its notable use ...\n",
            "URL: https://en.wikipedia.org/wiki/Python_(programming_language) \n",
            "\n",
            "Title: Download Python | Python.org\n",
            "Description: Python was created in the early 1990s by Guido van Rossum at Stichting \n",
            "Mathematisch Centrum in the Netherlands as a successor of a language called \n",
            "ABC.\n",
            "URL: https://www.python.org/downloads/ \n",
            "\n",
            "Title: Python Docs\n",
            "Description: keep this under your pillow. Language Reference describes syntax and \n",
            "language elements. Python Setup and Usage how to use Python on different \n",
            "platforms.\n",
            "URL: https://docs.python.org/ \n",
            "\n",
            "Title: Python Courses & Tutorials | Codecademy\n",
            "Description: Python · Great first language · Large programming community · Excellent online \n",
            "documentation · Endless libraries and packages · World-wide popularity · \n",
            "Powerful ...\n",
            "URL: https://www.codecademy.com/catalog/language/python \n",
            "\n",
            "Title: The Python Tutorial — Python 3.9.2 documentation\n",
            "Description: Python is an easy to learn, powerful programming language. ... Python's elegant \n",
            "syntax and dynamic typing, together with its interpreted nature, make it an ideal ...\n",
            "URL: https://docs.python.org/3/tutorial/ \n",
            "\n",
            "Title: Python Tutorial\n",
            "Description: Well organized and easy to understand Web building tutorials with lots of \n",
            "examples of how to use HTML, CSS, JavaScript, SQL, PHP, Python, Bootstrap, \n",
            "Java ...\n",
            "URL: https://www.w3schools.com/python/ \n",
            "\n",
            "Title: Python - Visual Studio Marketplace\n",
            "Description: 2 days ago ... Install the Python extension for Visual Studio Code. Step 3. Open or create a \n",
            "Python file and start coding! Set up your environment. Select your ...\n",
            "URL: https://marketplace.visualstudio.com/items?itemName=ms-python.python \n",
            "\n",
            "Title: The Python Standard Library — Python 3.9.2 documentation\n",
            "Description: Python's standard library is very extensive, offering a wide range of facilities as \n",
            "indicated by the long table of contents listed below. The library contains built-in ...\n",
            "URL: https://docs.python.org/3/library/ \n",
            "\n",
            "Title: PyPI · The Python Package Index\n",
            "Description: The Python Package Index (PyPI) is a repository of software for the Python \n",
            "programming language.\n",
            "URL: https://pypi.org/ \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JEpBVJDG8sqd"
      },
      "source": [
        "Scraping the sites"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hqusCWGLNYh0",
        "outputId": "8a62ee14-df5e-45d2-de4b-8d49290b1702"
      },
      "source": [
        "# Make a request\n",
        "page = requests.get(\"https://www.python.org/'\")\n",
        "soup = BeautifulSoup(page.content, 'html.parser')\n",
        "page_title = soup.title.text\n",
        "page_body = soup.body\n",
        "page_head = soup.head\n",
        "print(page_body, page_head)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<body class=\"python default-page fourohfour\">\n",
            "<div id=\"touchnav-wrapper\">\n",
            "<div class=\"do-not-print\" id=\"nojs\">\n",
            "<p><strong>Notice:</strong> While Javascript is not essential for this website, your interaction with the content will be limited. Please turn Javascript on for the full experience. </p>\n",
            "</div>\n",
            "<!--[if lte IE 8]>\n",
            "        <div id=\"oldie-warning\" class=\"do-not-print\">\n",
            "            <p>\n",
            "                <strong>Notice:</strong> Your browser is <em>ancient</em>. Please\n",
            "                <a href=\"http://browsehappy.com/\">upgrade to a different browser</a> to experience a better web.\n",
            "            </p>\n",
            "        </div>\n",
            "        <![endif]-->\n",
            "<!-- Sister Site Links -->\n",
            "<div class=\"top-bar do-not-print\" id=\"top\">\n",
            "<nav class=\"meta-navigation container\" role=\"navigation\">\n",
            "<div class=\"skip-link screen-reader-text\">\n",
            "<a href=\"#content\" title=\"Skip to content\">Skip to content</a>\n",
            "</div>\n",
            "<a aria-hidden=\"true\" class=\"jump-link\" href=\"#python-network\" id=\"close-python-network\">\n",
            "<span aria-hidden=\"true\" class=\"icon-arrow-down\"><span>▼</span></span> Close\n",
            "                </a>\n",
            "<ul class=\"menu\" role=\"tree\">\n",
            "<li class=\"python-meta \">\n",
            "<a href=\"/\" title=\"The Python Programming Language\">Python</a>\n",
            "</li>\n",
            "<li class=\"psf-meta \">\n",
            "<a href=\"/psf-landing/\" title=\"The Python Software Foundation\">PSF</a>\n",
            "</li>\n",
            "<li class=\"docs-meta \">\n",
            "<a href=\"https://docs.python.org\" title=\"Python Documentation\">Docs</a>\n",
            "</li>\n",
            "<li class=\"pypi-meta \">\n",
            "<a href=\"https://pypi.org/\" title=\"Python Package Index\">PyPI</a>\n",
            "</li>\n",
            "<li class=\"jobs-meta \">\n",
            "<a href=\"/jobs/\" title=\"Python Job Board\">Jobs</a>\n",
            "</li>\n",
            "<li class=\"shop-meta \">\n",
            "<a href=\"/community-landing/\">Community</a>\n",
            "</li>\n",
            "</ul>\n",
            "<a aria-hidden=\"true\" class=\"jump-link\" href=\"#top\" id=\"python-network\">\n",
            "<span aria-hidden=\"true\" class=\"icon-arrow-up\"><span>▲</span></span> The Python Network\n",
            "                </a>\n",
            "</nav>\n",
            "</div>\n",
            "<!-- Header elements -->\n",
            "<header class=\"main-header\" role=\"banner\">\n",
            "<div class=\"container\">\n",
            "<h1 class=\"site-headline\">\n",
            "<a href=\"/\"><img alt=\"python™\" class=\"python-logo\" src=\"/static/img/python-logo.png\"/></a>\n",
            "</h1>\n",
            "<div class=\"options-bar-container do-not-print\">\n",
            "<a class=\"donate-button\" href=\"https://psfmember.org/civicrm/contribute/transact?reset=1&amp;id=2\">Donate</a>\n",
            "<div class=\"options-bar\">\n",
            "<a class=\"jump-to-menu\" href=\"#site-map\" id=\"site-map-link\"><span class=\"menu-icon\">≡</span> Menu</a><form action=\"/search/\" class=\"search-the-site\" method=\"get\">\n",
            "<fieldset title=\"Search Python.org\">\n",
            "<span aria-hidden=\"true\" class=\"icon-search\"></span>\n",
            "<label class=\"screen-reader-text\" for=\"id-search-field\">Search This Site</label>\n",
            "<input class=\"search-field\" id=\"id-search-field\" name=\"q\" placeholder=\"Search\" role=\"textbox\" tabindex=\"1\" type=\"search\" value=\"\"/>\n",
            "<button class=\"search-button\" id=\"submit\" name=\"submit\" tabindex=\"3\" title=\"Submit this Search\" type=\"submit\">\n",
            "                                    GO\n",
            "                                </button>\n",
            "<!--[if IE]><input type=\"text\" style=\"display: none;\" disabled=\"disabled\" size=\"1\" tabindex=\"4\"><![endif]-->\n",
            "</fieldset>\n",
            "</form><span class=\"breaker\"></span><div aria-hidden=\"true\" class=\"adjust-font-size\">\n",
            "<ul aria-label=\"Adjust Text Size on Page\" class=\"navigation menu\">\n",
            "<li aria-haspopup=\"true\" class=\"tier-1 last\">\n",
            "<a class=\"action-trigger\" href=\"#\"><strong><small>A</small> A</strong></a>\n",
            "<ul class=\"subnav menu\">\n",
            "<li class=\"tier-2 element-1\" role=\"treeitem\"><a class=\"text-shrink\" href=\"javascript:;\" title=\"Make Text Smaller\">Smaller</a></li>\n",
            "<li class=\"tier-2 element-2\" role=\"treeitem\"><a class=\"text-grow\" href=\"javascript:;\" title=\"Make Text Larger\">Larger</a></li>\n",
            "<li class=\"tier-2 element-3\" role=\"treeitem\"><a class=\"text-reset\" href=\"javascript:;\" title=\"Reset any font size changes I have made\">Reset</a></li>\n",
            "</ul>\n",
            "</li>\n",
            "</ul>\n",
            "</div><div class=\"winkwink-nudgenudge\">\n",
            "<ul aria-label=\"Social Media Navigation\" class=\"navigation menu\">\n",
            "<li aria-haspopup=\"true\" class=\"tier-1 last\">\n",
            "<a class=\"action-trigger\" href=\"#\">Socialize</a>\n",
            "<ul class=\"subnav menu\">\n",
            "<li class=\"tier-2 element-1\" role=\"treeitem\"><a href=\"https://www.facebook.com/pythonlang?fref=ts\"><span aria-hidden=\"true\" class=\"icon-facebook\"></span>Facebook</a></li>\n",
            "<li class=\"tier-2 element-2\" role=\"treeitem\"><a href=\"https://twitter.com/ThePSF\"><span aria-hidden=\"true\" class=\"icon-twitter\"></span>Twitter</a></li>\n",
            "<li class=\"tier-2 element-3\" role=\"treeitem\"><a href=\"/community/irc/\"><span aria-hidden=\"true\" class=\"icon-freenode\"></span>Chat on IRC</a></li>\n",
            "</ul>\n",
            "</li>\n",
            "</ul>\n",
            "</div>\n",
            "<span data-html-include=\"/authenticated\"></span>\n",
            "</div><!-- end options-bar -->\n",
            "</div>\n",
            "<nav class=\"python-navigation main-navigation do-not-print\" id=\"mainnav\" role=\"navigation\">\n",
            "<ul aria-label=\"Main Navigation\" class=\"navigation menu\" role=\"menubar\">\n",
            "<li aria-haspopup=\"true\" class=\"tier-1 element-1 \" id=\"about\">\n",
            "<a class=\"\" href=\"/about/\" title=\"\">About</a>\n",
            "<ul aria-hidden=\"true\" class=\"subnav menu\" role=\"menu\">\n",
            "<li class=\"tier-2 element-1\" role=\"treeitem\"><a href=\"/about/apps/\" title=\"\">Applications</a></li>\n",
            "<li class=\"tier-2 element-2\" role=\"treeitem\"><a href=\"/about/quotes/\" title=\"\">Quotes</a></li>\n",
            "<li class=\"tier-2 element-3\" role=\"treeitem\"><a href=\"/about/gettingstarted/\" title=\"\">Getting Started</a></li>\n",
            "<li class=\"tier-2 element-4\" role=\"treeitem\"><a href=\"/about/help/\" title=\"\">Help</a></li>\n",
            "<li class=\"tier-2 element-5\" role=\"treeitem\"><a href=\"http://brochure.getpython.info/\" title=\"\">Python Brochure</a></li>\n",
            "</ul>\n",
            "</li>\n",
            "<li aria-haspopup=\"true\" class=\"tier-1 element-2 \" id=\"downloads\">\n",
            "<a class=\"\" href=\"/downloads/\" title=\"\">Downloads</a>\n",
            "<ul aria-hidden=\"true\" class=\"subnav menu\" role=\"menu\">\n",
            "<li class=\"tier-2 element-1\" role=\"treeitem\"><a href=\"/downloads/\" title=\"\">All releases</a></li>\n",
            "<li class=\"tier-2 element-2\" role=\"treeitem\"><a href=\"/downloads/source/\" title=\"\">Source code</a></li>\n",
            "<li class=\"tier-2 element-3\" role=\"treeitem\"><a href=\"/downloads/windows/\" title=\"\">Windows</a></li>\n",
            "<li class=\"tier-2 element-4\" role=\"treeitem\"><a href=\"/downloads/mac-osx/\" title=\"\">Mac OS X</a></li>\n",
            "<li class=\"tier-2 element-5\" role=\"treeitem\"><a href=\"/download/other/\" title=\"\">Other Platforms</a></li>\n",
            "<li class=\"tier-2 element-6\" role=\"treeitem\"><a href=\"https://docs.python.org/3/license.html\" title=\"\">License</a></li>\n",
            "<li class=\"tier-2 element-7\" role=\"treeitem\"><a href=\"/download/alternatives\" title=\"\">Alternative Implementations</a></li>\n",
            "</ul>\n",
            "</li>\n",
            "<li aria-haspopup=\"true\" class=\"tier-1 element-3 \" id=\"documentation\">\n",
            "<a class=\"\" href=\"/doc/\" title=\"\">Documentation</a>\n",
            "<ul aria-hidden=\"true\" class=\"subnav menu\" role=\"menu\">\n",
            "<li class=\"tier-2 element-1\" role=\"treeitem\"><a href=\"/doc/\" title=\"\">Docs</a></li>\n",
            "<li class=\"tier-2 element-2\" role=\"treeitem\"><a href=\"/doc/av\" title=\"\">Audio/Visual Talks</a></li>\n",
            "<li class=\"tier-2 element-3\" role=\"treeitem\"><a href=\"https://wiki.python.org/moin/BeginnersGuide\" title=\"\">Beginner's Guide</a></li>\n",
            "<li class=\"tier-2 element-4\" role=\"treeitem\"><a href=\"https://devguide.python.org/\" title=\"\">Developer's Guide</a></li>\n",
            "<li class=\"tier-2 element-5\" role=\"treeitem\"><a href=\"https://docs.python.org/faq/\" title=\"\">FAQ</a></li>\n",
            "<li class=\"tier-2 element-6\" role=\"treeitem\"><a href=\"http://wiki.python.org/moin/Languages\" title=\"\">Non-English Docs</a></li>\n",
            "<li class=\"tier-2 element-7\" role=\"treeitem\"><a href=\"http://python.org/dev/peps/\" title=\"\">PEP Index</a></li>\n",
            "<li class=\"tier-2 element-8\" role=\"treeitem\"><a href=\"https://wiki.python.org/moin/PythonBooks\" title=\"\">Python Books</a></li>\n",
            "<li class=\"tier-2 element-9\" role=\"treeitem\"><a href=\"/doc/essays/\" title=\"\">Python Essays</a></li>\n",
            "</ul>\n",
            "</li>\n",
            "<li aria-haspopup=\"true\" class=\"tier-1 element-4 \" id=\"community\">\n",
            "<a class=\"\" href=\"/community/\" title=\"\">Community</a>\n",
            "<ul aria-hidden=\"true\" class=\"subnav menu\" role=\"menu\">\n",
            "<li class=\"tier-2 element-1\" role=\"treeitem\"><a href=\"/community/survey\" title=\"\">Community Survey</a></li>\n",
            "<li class=\"tier-2 element-2\" role=\"treeitem\"><a href=\"/community/diversity/\" title=\"\">Diversity</a></li>\n",
            "<li class=\"tier-2 element-3\" role=\"treeitem\"><a href=\"/community/lists/\" title=\"\">Mailing Lists</a></li>\n",
            "<li class=\"tier-2 element-4\" role=\"treeitem\"><a href=\"/community/irc/\" title=\"\">IRC</a></li>\n",
            "<li class=\"tier-2 element-5\" role=\"treeitem\"><a href=\"/community/forums/\" title=\"\">Forums</a></li>\n",
            "<li class=\"tier-2 element-6\" role=\"treeitem\"><a href=\"/psf/annual-report/2020/\" title=\"\">PSF Annual Impact Report</a></li>\n",
            "<li class=\"tier-2 element-7\" role=\"treeitem\"><a href=\"/community/workshops/\" title=\"\">Python Conferences</a></li>\n",
            "<li class=\"tier-2 element-8\" role=\"treeitem\"><a href=\"/community/sigs/\" title=\"\">Special Interest Groups</a></li>\n",
            "<li class=\"tier-2 element-9\" role=\"treeitem\"><a href=\"/community/logos/\" title=\"\">Python Logo</a></li>\n",
            "<li class=\"tier-2 element-10\" role=\"treeitem\"><a href=\"https://wiki.python.org/moin/\" title=\"\">Python Wiki</a></li>\n",
            "<li class=\"tier-2 element-11\" role=\"treeitem\"><a href=\"/community/merchandise/\" title=\"\">Merchandise</a></li>\n",
            "<li class=\"tier-2 element-12\" role=\"treeitem\"><a href=\"/community/awards\" title=\"\">Community Awards</a></li>\n",
            "<li class=\"tier-2 element-13\" role=\"treeitem\"><a href=\"/psf/conduct/\" title=\"\">Code of Conduct</a></li>\n",
            "<li class=\"tier-2 element-14\" role=\"treeitem\"><a href=\"/psf/get-involved/\" title=\"\">Get Involved</a></li>\n",
            "<li class=\"tier-2 element-15\" role=\"treeitem\"><a href=\"/psf/community-stories/\" title=\"\">Shared Stories</a></li>\n",
            "</ul>\n",
            "</li>\n",
            "<li aria-haspopup=\"true\" class=\"tier-1 element-5 \" id=\"success-stories\">\n",
            "<a class=\"\" href=\"/success-stories/\" title=\"success-stories\">Success Stories</a>\n",
            "<ul aria-hidden=\"true\" class=\"subnav menu\" role=\"menu\">\n",
            "<li class=\"tier-2 element-1\" role=\"treeitem\"><a href=\"/success-stories/category/arts/\" title=\"\">Arts</a></li>\n",
            "<li class=\"tier-2 element-2\" role=\"treeitem\"><a href=\"/success-stories/category/business/\" title=\"\">Business</a></li>\n",
            "<li class=\"tier-2 element-3\" role=\"treeitem\"><a href=\"/success-stories/category/education/\" title=\"\">Education</a></li>\n",
            "<li class=\"tier-2 element-4\" role=\"treeitem\"><a href=\"/success-stories/category/engineering/\" title=\"\">Engineering</a></li>\n",
            "<li class=\"tier-2 element-5\" role=\"treeitem\"><a href=\"/success-stories/category/government/\" title=\"\">Government</a></li>\n",
            "<li class=\"tier-2 element-6\" role=\"treeitem\"><a href=\"/success-stories/category/scientific/\" title=\"\">Scientific</a></li>\n",
            "<li class=\"tier-2 element-7\" role=\"treeitem\"><a href=\"/success-stories/category/software-development/\" title=\"\">Software Development</a></li>\n",
            "</ul>\n",
            "</li>\n",
            "<li aria-haspopup=\"true\" class=\"tier-1 element-6 \" id=\"news\">\n",
            "<a class=\"\" href=\"/blogs/\" title=\"News from around the Python world\">News</a>\n",
            "<ul aria-hidden=\"true\" class=\"subnav menu\" role=\"menu\">\n",
            "<li class=\"tier-2 element-1\" role=\"treeitem\"><a href=\"/blogs/\" title=\"Python Insider Blog Posts\">Python News</a></li>\n",
            "<li class=\"tier-2 element-2\" role=\"treeitem\"><a href=\"/psf/newsletter/\" title=\"Python Software Foundation Newsletter\">PSF Newsletter</a></li>\n",
            "<li class=\"tier-2 element-3\" role=\"treeitem\"><a href=\"http://planetpython.org/\" title=\"Planet Python\">Community News</a></li>\n",
            "<li class=\"tier-2 element-4\" role=\"treeitem\"><a href=\"http://pyfound.blogspot.com/\" title=\"PSF Blog\">PSF News</a></li>\n",
            "<li class=\"tier-2 element-5\" role=\"treeitem\"><a href=\"http://pycon.blogspot.com/\" title=\"PyCon Blog\">PyCon News</a></li>\n",
            "</ul>\n",
            "</li>\n",
            "<li aria-haspopup=\"true\" class=\"tier-1 element-7 \" id=\"events\">\n",
            "<a class=\"\" href=\"/events/\" title=\"\">Events</a>\n",
            "<ul aria-hidden=\"true\" class=\"subnav menu\" role=\"menu\">\n",
            "<li class=\"tier-2 element-1\" role=\"treeitem\"><a href=\"/events/python-events\" title=\"\">Python Events</a></li>\n",
            "<li class=\"tier-2 element-2\" role=\"treeitem\"><a href=\"/events/python-user-group/\" title=\"\">User Group Events</a></li>\n",
            "<li class=\"tier-2 element-3\" role=\"treeitem\"><a href=\"/events/python-events/past/\" title=\"\">Python Events Archive</a></li>\n",
            "<li class=\"tier-2 element-4\" role=\"treeitem\"><a href=\"/events/python-user-group/past/\" title=\"\">User Group Events Archive</a></li>\n",
            "<li class=\"tier-2 element-5\" role=\"treeitem\"><a href=\"https://wiki.python.org/moin/PythonEventsCalendar#Submitting_an_Event\" title=\"\">Submit an Event</a></li>\n",
            "</ul>\n",
            "</li>\n",
            "</ul>\n",
            "</nav>\n",
            "<div class=\"header-banner \"> <!-- for optional \"do-not-print\" class -->\n",
            "</div>\n",
            "</div><!-- end .container -->\n",
            "</header>\n",
            "<div class=\"content-wrapper\" id=\"content\">\n",
            "<!-- Main Content Column -->\n",
            "<div class=\"container\">\n",
            "<section class=\"main-content \" role=\"main\">\n",
            "<article class=\"text\">\n",
            "<h1 class=\"giga\">Error 404: File not Found</h1>\n",
            "<p>We couldn’t find what you were looking for. This error has been reported and we will look into it shortly. For now,</p>\n",
            "<ul>\n",
            "<li>Try using the search box.</li>\n",
            "<li>\n",
            "                Python.org went through a redesign and you may have followed a broken link.\n",
            "                Sorry for that, see if <a href=\"http://legacy.python.org/'\">the same page on the legacy website</a> works.\n",
            "            </li>\n",
            "<li>\n",
            "                Lost in an exotic function call? Read <a href=\"/doc/\">the docs</a>.\n",
            "                Looking for a package? Check the <a href=\"https://pypi.org/\">Package Index</a>.\n",
            "                Need a specific version of Python? The <a href=\"/downloads/\">downloads section</a> has it.\n",
            "            </li>\n",
            "</ul>\n",
            "</article>\n",
            "</section>\n",
            "</div><!-- end .container -->\n",
            "</div><!-- end #content .content-wrapper -->\n",
            "<!-- Footer and social media list -->\n",
            "<footer class=\"main-footer\" id=\"site-map\" role=\"contentinfo\">\n",
            "<div class=\"main-footer-links\">\n",
            "<div class=\"container\">\n",
            "<a class=\"jump-link\" href=\"#python-network\" id=\"back-to-top-1\"><span aria-hidden=\"true\" class=\"icon-arrow-up\"><span>▲</span></span> Back to Top</a>\n",
            "<ul class=\"sitemap navigation menu do-not-print\" id=\"container\" role=\"tree\">\n",
            "<li class=\"tier-1 element-1\">\n",
            "<a href=\"/about/\">About</a>\n",
            "<ul class=\"subnav menu\">\n",
            "<li class=\"tier-2 element-1\" role=\"treeitem\"><a href=\"/about/apps/\" title=\"\">Applications</a></li>\n",
            "<li class=\"tier-2 element-2\" role=\"treeitem\"><a href=\"/about/quotes/\" title=\"\">Quotes</a></li>\n",
            "<li class=\"tier-2 element-3\" role=\"treeitem\"><a href=\"/about/gettingstarted/\" title=\"\">Getting Started</a></li>\n",
            "<li class=\"tier-2 element-4\" role=\"treeitem\"><a href=\"/about/help/\" title=\"\">Help</a></li>\n",
            "<li class=\"tier-2 element-5\" role=\"treeitem\"><a href=\"http://brochure.getpython.info/\" title=\"\">Python Brochure</a></li>\n",
            "</ul>\n",
            "</li>\n",
            "<li class=\"tier-1 element-2\">\n",
            "<a href=\"/downloads/\">Downloads</a>\n",
            "<ul class=\"subnav menu\">\n",
            "<li class=\"tier-2 element-1\" role=\"treeitem\"><a href=\"/downloads/\" title=\"\">All releases</a></li>\n",
            "<li class=\"tier-2 element-2\" role=\"treeitem\"><a href=\"/downloads/source/\" title=\"\">Source code</a></li>\n",
            "<li class=\"tier-2 element-3\" role=\"treeitem\"><a href=\"/downloads/windows/\" title=\"\">Windows</a></li>\n",
            "<li class=\"tier-2 element-4\" role=\"treeitem\"><a href=\"/downloads/mac-osx/\" title=\"\">Mac OS X</a></li>\n",
            "<li class=\"tier-2 element-5\" role=\"treeitem\"><a href=\"/download/other/\" title=\"\">Other Platforms</a></li>\n",
            "<li class=\"tier-2 element-6\" role=\"treeitem\"><a href=\"https://docs.python.org/3/license.html\" title=\"\">License</a></li>\n",
            "<li class=\"tier-2 element-7\" role=\"treeitem\"><a href=\"/download/alternatives\" title=\"\">Alternative Implementations</a></li>\n",
            "</ul>\n",
            "</li>\n",
            "<li class=\"tier-1 element-3\">\n",
            "<a href=\"/doc/\">Documentation</a>\n",
            "<ul class=\"subnav menu\">\n",
            "<li class=\"tier-2 element-1\" role=\"treeitem\"><a href=\"/doc/\" title=\"\">Docs</a></li>\n",
            "<li class=\"tier-2 element-2\" role=\"treeitem\"><a href=\"/doc/av\" title=\"\">Audio/Visual Talks</a></li>\n",
            "<li class=\"tier-2 element-3\" role=\"treeitem\"><a href=\"https://wiki.python.org/moin/BeginnersGuide\" title=\"\">Beginner's Guide</a></li>\n",
            "<li class=\"tier-2 element-4\" role=\"treeitem\"><a href=\"https://devguide.python.org/\" title=\"\">Developer's Guide</a></li>\n",
            "<li class=\"tier-2 element-5\" role=\"treeitem\"><a href=\"https://docs.python.org/faq/\" title=\"\">FAQ</a></li>\n",
            "<li class=\"tier-2 element-6\" role=\"treeitem\"><a href=\"http://wiki.python.org/moin/Languages\" title=\"\">Non-English Docs</a></li>\n",
            "<li class=\"tier-2 element-7\" role=\"treeitem\"><a href=\"http://python.org/dev/peps/\" title=\"\">PEP Index</a></li>\n",
            "<li class=\"tier-2 element-8\" role=\"treeitem\"><a href=\"https://wiki.python.org/moin/PythonBooks\" title=\"\">Python Books</a></li>\n",
            "<li class=\"tier-2 element-9\" role=\"treeitem\"><a href=\"/doc/essays/\" title=\"\">Python Essays</a></li>\n",
            "</ul>\n",
            "</li>\n",
            "<li class=\"tier-1 element-4\">\n",
            "<a href=\"/community/\">Community</a>\n",
            "<ul class=\"subnav menu\">\n",
            "<li class=\"tier-2 element-1\" role=\"treeitem\"><a href=\"/community/survey\" title=\"\">Community Survey</a></li>\n",
            "<li class=\"tier-2 element-2\" role=\"treeitem\"><a href=\"/community/diversity/\" title=\"\">Diversity</a></li>\n",
            "<li class=\"tier-2 element-3\" role=\"treeitem\"><a href=\"/community/lists/\" title=\"\">Mailing Lists</a></li>\n",
            "<li class=\"tier-2 element-4\" role=\"treeitem\"><a href=\"/community/irc/\" title=\"\">IRC</a></li>\n",
            "<li class=\"tier-2 element-5\" role=\"treeitem\"><a href=\"/community/forums/\" title=\"\">Forums</a></li>\n",
            "<li class=\"tier-2 element-6\" role=\"treeitem\"><a href=\"/psf/annual-report/2020/\" title=\"\">PSF Annual Impact Report</a></li>\n",
            "<li class=\"tier-2 element-7\" role=\"treeitem\"><a href=\"/community/workshops/\" title=\"\">Python Conferences</a></li>\n",
            "<li class=\"tier-2 element-8\" role=\"treeitem\"><a href=\"/community/sigs/\" title=\"\">Special Interest Groups</a></li>\n",
            "<li class=\"tier-2 element-9\" role=\"treeitem\"><a href=\"/community/logos/\" title=\"\">Python Logo</a></li>\n",
            "<li class=\"tier-2 element-10\" role=\"treeitem\"><a href=\"https://wiki.python.org/moin/\" title=\"\">Python Wiki</a></li>\n",
            "<li class=\"tier-2 element-11\" role=\"treeitem\"><a href=\"/community/merchandise/\" title=\"\">Merchandise</a></li>\n",
            "<li class=\"tier-2 element-12\" role=\"treeitem\"><a href=\"/community/awards\" title=\"\">Community Awards</a></li>\n",
            "<li class=\"tier-2 element-13\" role=\"treeitem\"><a href=\"/psf/conduct/\" title=\"\">Code of Conduct</a></li>\n",
            "<li class=\"tier-2 element-14\" role=\"treeitem\"><a href=\"/psf/get-involved/\" title=\"\">Get Involved</a></li>\n",
            "<li class=\"tier-2 element-15\" role=\"treeitem\"><a href=\"/psf/community-stories/\" title=\"\">Shared Stories</a></li>\n",
            "</ul>\n",
            "</li>\n",
            "<li class=\"tier-1 element-5\">\n",
            "<a href=\"/success-stories/\" title=\"success-stories\">Success Stories</a>\n",
            "<ul class=\"subnav menu\">\n",
            "<li class=\"tier-2 element-1\" role=\"treeitem\"><a href=\"/success-stories/category/arts/\" title=\"\">Arts</a></li>\n",
            "<li class=\"tier-2 element-2\" role=\"treeitem\"><a href=\"/success-stories/category/business/\" title=\"\">Business</a></li>\n",
            "<li class=\"tier-2 element-3\" role=\"treeitem\"><a href=\"/success-stories/category/education/\" title=\"\">Education</a></li>\n",
            "<li class=\"tier-2 element-4\" role=\"treeitem\"><a href=\"/success-stories/category/engineering/\" title=\"\">Engineering</a></li>\n",
            "<li class=\"tier-2 element-5\" role=\"treeitem\"><a href=\"/success-stories/category/government/\" title=\"\">Government</a></li>\n",
            "<li class=\"tier-2 element-6\" role=\"treeitem\"><a href=\"/success-stories/category/scientific/\" title=\"\">Scientific</a></li>\n",
            "<li class=\"tier-2 element-7\" role=\"treeitem\"><a href=\"/success-stories/category/software-development/\" title=\"\">Software Development</a></li>\n",
            "</ul>\n",
            "</li>\n",
            "<li class=\"tier-1 element-6\">\n",
            "<a href=\"/blogs/\" title=\"News from around the Python world\">News</a>\n",
            "<ul class=\"subnav menu\">\n",
            "<li class=\"tier-2 element-1\" role=\"treeitem\"><a href=\"/blogs/\" title=\"Python Insider Blog Posts\">Python News</a></li>\n",
            "<li class=\"tier-2 element-2\" role=\"treeitem\"><a href=\"/psf/newsletter/\" title=\"Python Software Foundation Newsletter\">PSF Newsletter</a></li>\n",
            "<li class=\"tier-2 element-3\" role=\"treeitem\"><a href=\"http://planetpython.org/\" title=\"Planet Python\">Community News</a></li>\n",
            "<li class=\"tier-2 element-4\" role=\"treeitem\"><a href=\"http://pyfound.blogspot.com/\" title=\"PSF Blog\">PSF News</a></li>\n",
            "<li class=\"tier-2 element-5\" role=\"treeitem\"><a href=\"http://pycon.blogspot.com/\" title=\"PyCon Blog\">PyCon News</a></li>\n",
            "</ul>\n",
            "</li>\n",
            "<li class=\"tier-1 element-7\">\n",
            "<a href=\"/events/\">Events</a>\n",
            "<ul class=\"subnav menu\">\n",
            "<li class=\"tier-2 element-1\" role=\"treeitem\"><a href=\"/events/python-events\" title=\"\">Python Events</a></li>\n",
            "<li class=\"tier-2 element-2\" role=\"treeitem\"><a href=\"/events/python-user-group/\" title=\"\">User Group Events</a></li>\n",
            "<li class=\"tier-2 element-3\" role=\"treeitem\"><a href=\"/events/python-events/past/\" title=\"\">Python Events Archive</a></li>\n",
            "<li class=\"tier-2 element-4\" role=\"treeitem\"><a href=\"/events/python-user-group/past/\" title=\"\">User Group Events Archive</a></li>\n",
            "<li class=\"tier-2 element-5\" role=\"treeitem\"><a href=\"https://wiki.python.org/moin/PythonEventsCalendar#Submitting_an_Event\" title=\"\">Submit an Event</a></li>\n",
            "</ul>\n",
            "</li>\n",
            "<li class=\"tier-1 element-8\">\n",
            "<a href=\"/dev/\">Contributing</a>\n",
            "<ul class=\"subnav menu\">\n",
            "<li class=\"tier-2 element-1\" role=\"treeitem\"><a href=\"https://devguide.python.org/\" title=\"\">Developer's Guide</a></li>\n",
            "<li class=\"tier-2 element-2\" role=\"treeitem\"><a href=\"https://bugs.python.org/\" title=\"\">Issue Tracker</a></li>\n",
            "<li class=\"tier-2 element-3\" role=\"treeitem\"><a href=\"https://mail.python.org/mailman/listinfo/python-dev\" title=\"\">python-dev list</a></li>\n",
            "<li class=\"tier-2 element-4\" role=\"treeitem\"><a href=\"/dev/core-mentorship/\" title=\"\">Core Mentorship</a></li>\n",
            "<li class=\"tier-2 element-5\" role=\"treeitem\"><a href=\"/dev/security/\" title=\"\">Report a Security Issue</a></li>\n",
            "</ul>\n",
            "</li>\n",
            "</ul>\n",
            "<a class=\"jump-link\" href=\"#python-network\" id=\"back-to-top-2\"><span aria-hidden=\"true\" class=\"icon-arrow-up\"><span>▲</span></span> Back to Top</a>\n",
            "</div><!-- end .container -->\n",
            "</div> <!-- end .main-footer-links -->\n",
            "<div class=\"site-base\">\n",
            "<div class=\"container\">\n",
            "<ul class=\"footer-links navigation menu do-not-print\" role=\"tree\">\n",
            "<li class=\"tier-1 element-1\"><a href=\"/about/help/\">Help &amp; <span class=\"say-no-more\">General</span> Contact</a></li>\n",
            "<li class=\"tier-1 element-2\"><a href=\"/community/diversity/\">Diversity <span class=\"say-no-more\">Initiatives</span></a></li>\n",
            "<li class=\"tier-1 element-3\"><a href=\"https://github.com/python/pythondotorg/issues\">Submit Website Bug</a></li>\n",
            "<li class=\"tier-1 element-4\">\n",
            "<a href=\"https://status.python.org/\">Status <span class=\"python-status-indicator-default\" id=\"python-status-indicator\"></span></a>\n",
            "</li>\n",
            "</ul>\n",
            "<div class=\"copyright\">\n",
            "<p><small>\n",
            "<span class=\"pre\">Copyright ©2001-2021.</span>\n",
            "                             <span class=\"pre\"><a href=\"/psf-landing/\">Python Software Foundation</a></span>\n",
            "                             <span class=\"pre\"><a href=\"/about/legal/\">Legal Statements</a></span>\n",
            "                             <span class=\"pre\"><a href=\"/privacy/\">Privacy Policy</a></span>\n",
            "                             <span class=\"pre\"><a href=\"/psf/sponsorship/sponsors/#heroku\">Powered by Heroku</a></span>\n",
            "</small></p>\n",
            "</div>\n",
            "</div><!-- end .container -->\n",
            "</div><!-- end .site-base -->\n",
            "</footer>\n",
            "</div><!-- end #touchnav-wrapper -->\n",
            "<script src=\"//ajax.googleapis.com/ajax/libs/jquery/1.8.2/jquery.min.js\"></script>\n",
            "<script>window.jQuery || document.write('<script src=\"/static/js/libs/jquery-1.8.2.min.js\"><\\/script>')</script>\n",
            "<script src=\"//ajax.googleapis.com/ajax/libs/jqueryui/1.12.1/jquery-ui.min.js\"></script>\n",
            "<script>window.jQuery || document.write('<script src=\"/static/js/libs/jquery-ui-1.12.1.min.js\"><\\/script>')</script>\n",
            "<script src=\"/static/js/libs/masonry.pkgd.min.js\"></script>\n",
            "<script src=\"/static/js/libs/html-includes.js\"></script>\n",
            "<script charset=\"utf-8\" src=\"/static/js/main-min.35a45664b78f.js\" type=\"text/javascript\"></script>\n",
            "<!--[if lte IE 7]>\n",
            "    <script type=\"text/javascript\" src=\"/static/js/plugins/IE8-min.1fa19688541a.js\" charset=\"utf-8\"></script>\n",
            "    \n",
            "    \n",
            "    <![endif]-->\n",
            "<!--[if lte IE 8]>\n",
            "    <script type=\"text/javascript\" src=\"/static/js/plugins/getComputedStyle-min.722f490e4652.js\" charset=\"utf-8\"></script>\n",
            "    \n",
            "    \n",
            "    <![endif]-->\n",
            "</body> <head>\n",
            "<meta charset=\"utf-8\"/>\n",
            "<meta content=\"IE=edge\" http-equiv=\"X-UA-Compatible\"/>\n",
            "<link href=\"//ajax.googleapis.com/ajax/libs/jquery/1.8.2/jquery.min.js\" rel=\"prefetch\"/>\n",
            "<link href=\"//ajax.googleapis.com/ajax/libs/jqueryui/1.12.1/jquery-ui.min.js\" rel=\"prefetch\"/>\n",
            "<meta content=\"Python.org\" name=\"application-name\"/>\n",
            "<meta content=\"The official home of the Python Programming Language\" name=\"msapplication-tooltip\"/>\n",
            "<meta content=\"Python.org\" name=\"apple-mobile-web-app-title\"/>\n",
            "<meta content=\"yes\" name=\"apple-mobile-web-app-capable\"/>\n",
            "<meta content=\"black\" name=\"apple-mobile-web-app-status-bar-style\"/>\n",
            "<meta content=\"width=device-width, initial-scale=1.0\" name=\"viewport\"/>\n",
            "<meta content=\"True\" name=\"HandheldFriendly\"/>\n",
            "<meta content=\"telephone=no\" name=\"format-detection\"/>\n",
            "<meta content=\"on\" http-equiv=\"cleartype\"/>\n",
            "<meta content=\"false\" http-equiv=\"imagetoolbar\"/>\n",
            "<script src=\"/static/js/libs/modernizr.js\"></script>\n",
            "<link href=\"/static/stylesheets/style.b50cef9e3bb9.css\" rel=\"stylesheet\" title=\"default\" type=\"text/css\">\n",
            "<link href=\"/static/stylesheets/mq.eef77a5d2257.css\" media=\"not print, braille, embossed, speech, tty\" rel=\"stylesheet\" type=\"text/css\">\n",
            "<!--[if (lte IE 8)&(!IEMobile)]>\n",
            "    <link href=\"/static/stylesheets/no-mq.e636abc55522.css\" rel=\"stylesheet\" type=\"text/css\" media=\"screen\" />\n",
            "    \n",
            "    \n",
            "    <![endif]-->\n",
            "<link href=\"//ajax.googleapis.com/ajax/libs/jqueryui/1.12.1/themes/smoothness/jquery-ui.css\" rel=\"stylesheet\"/>\n",
            "<link href=\"/static/favicon.ico\" rel=\"icon\" type=\"image/x-icon\"/>\n",
            "<link href=\"/static/apple-touch-icon-144x144-precomposed.png\" rel=\"apple-touch-icon-precomposed\" sizes=\"144x144\"/>\n",
            "<link href=\"/static/apple-touch-icon-114x114-precomposed.png\" rel=\"apple-touch-icon-precomposed\" sizes=\"114x114\"/>\n",
            "<link href=\"/static/apple-touch-icon-72x72-precomposed.png\" rel=\"apple-touch-icon-precomposed\" sizes=\"72x72\"/>\n",
            "<link href=\"/static/apple-touch-icon-precomposed.png\" rel=\"apple-touch-icon-precomposed\"/>\n",
            "<link href=\"/static/apple-touch-icon-precomposed.png\" rel=\"apple-touch-icon\"/>\n",
            "<meta content=\"/static/metro-icon-144x144-precomposed.png\" name=\"msapplication-TileImage\"/><!-- white shape -->\n",
            "<meta content=\"#3673a5\" name=\"msapplication-TileColor\"/><!-- python blue -->\n",
            "<meta content=\"#3673a5\" name=\"msapplication-navbutton-color\"/>\n",
            "<title>Welcome to Python.org</title>\n",
            "<meta content=\"The official home of the Python Programming Language\" name=\"description\"/>\n",
            "<meta content=\"Python programming language object oriented web free open source software license documentation download community\" name=\"keywords\"/>\n",
            "<meta content=\"website\" property=\"og:type\"/>\n",
            "<meta content=\"Python.org\" property=\"og:site_name\"/>\n",
            "<meta content=\"Welcome to Python.org\" property=\"og:title\"/>\n",
            "<meta content=\"The official home of the Python Programming Language\" property=\"og:description\"/>\n",
            "<meta content=\"https://www.python.org/static/opengraph-icon-200x200.png\" property=\"og:image\"/>\n",
            "<meta content=\"https://www.python.org/static/opengraph-icon-200x200.png\" property=\"og:image:secure_url\"/>\n",
            "<meta content=\"https://www.python.org/'\" property=\"og:url\"/>\n",
            "<link href=\"/static/humans.txt\" rel=\"author\"/>\n",
            "<link href=\"https://www.python.org/dev/peps/peps.rss/\" rel=\"alternate\" title=\"Python Enhancement Proposals\" type=\"application/rss+xml\"/>\n",
            "<link href=\"https://www.python.org/jobs/feed/rss/\" rel=\"alternate\" title=\"Python Job Opportunities\" type=\"application/rss+xml\"/>\n",
            "<link href=\"https://feeds.feedburner.com/PythonSoftwareFoundationNews\" rel=\"alternate\" title=\"Python Software Foundation News\" type=\"application/rss+xml\"/>\n",
            "<link href=\"https://feeds.feedburner.com/PythonInsider\" rel=\"alternate\" title=\"Python Insider\" type=\"application/rss+xml\"/>\n",
            "<script type=\"application/ld+json\">\n",
            "     {\n",
            "       \"@context\": \"https://schema.org\",\n",
            "       \"@type\": \"WebSite\",\n",
            "       \"url\": \"https://www.python.org/\",\n",
            "       \"potentialAction\": {\n",
            "         \"@type\": \"SearchAction\",\n",
            "         \"target\": \"https://www.python.org/search/?q={search_term_string}\",\n",
            "         \"query-input\": \"required name=search_term_string\"\n",
            "       }\n",
            "     }\n",
            "    </script>\n",
            "<script type=\"text/javascript\">\n",
            "    var _gaq = _gaq || [];\n",
            "    _gaq.push(['_setAccount', 'UA-39055973-1']);\n",
            "    _gaq.push(['_trackPageview']);\n",
            "\n",
            "    (function() {\n",
            "        var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;\n",
            "        ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';\n",
            "        var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);\n",
            "    })();\n",
            "    </script>\n",
            "</link></link></head>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jekAEtPyDrtb",
        "outputId": "b2cb4540-b92d-4039-e7a4-821906e31b6a"
      },
      "source": [
        "page = requests.get( \"https://arxiv.org/pdf/1301.3781\")\n",
        "soup = BeautifulSoup(page.content, 'html.parser')\n",
        "\n",
        "#page_title = soup.title.text\n",
        "page_body = soup.body\n",
        "page_head = soup.head\n",
        "print(page_body, page_head)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "None None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qXmMncrLH3bh",
        "outputId": "b44dfbfc-6e36-4a48-ee30-c831a43a06a1"
      },
      "source": [
        "pip install newspaper3k"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting newspaper3k\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d7/b9/51afecb35bb61b188a4b44868001de348a0e8134b4dfa00ffc191567c4b9/newspaper3k-0.2.8-py3-none-any.whl (211kB)\n",
            "\u001b[K     |████████████████████████████████| 215kB 11.3MB/s \n",
            "\u001b[?25hCollecting jieba3k>=0.35.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a9/cb/2c8332bcdc14d33b0bedd18ae0a4981a069c3513e445120da3c3f23a8aaa/jieba3k-0.35.1.zip (7.4MB)\n",
            "\u001b[K     |████████████████████████████████| 7.4MB 8.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: lxml>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from newspaper3k) (4.2.6)\n",
            "Collecting feedfinder2>=0.0.4\n",
            "  Downloading https://files.pythonhosted.org/packages/35/82/1251fefec3bb4b03fd966c7e7f7a41c9fc2bb00d823a34c13f847fd61406/feedfinder2-0.0.4.tar.gz\n",
            "Requirement already satisfied: PyYAML>=3.11 in /usr/local/lib/python3.7/dist-packages (from newspaper3k) (3.13)\n",
            "Collecting feedparser>=5.2.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1c/21/faf1bac028662cc8adb2b5ef7a6f3999a765baa2835331df365289b0ca56/feedparser-6.0.2-py3-none-any.whl (80kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 7.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests>=2.10.0 in /usr/local/lib/python3.7/dist-packages (from newspaper3k) (2.23.0)\n",
            "Requirement already satisfied: Pillow>=3.3.0 in /usr/local/lib/python3.7/dist-packages (from newspaper3k) (7.0.0)\n",
            "Collecting tinysegmenter==0.3\n",
            "  Downloading https://files.pythonhosted.org/packages/17/82/86982e4b6d16e4febc79c2a1d68ee3b707e8a020c5d2bc4af8052d0f136a/tinysegmenter-0.3.tar.gz\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.7/dist-packages (from newspaper3k) (2.8.1)\n",
            "Collecting tldextract>=2.0.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7e/62/b6acd3129c5615b9860e670df07fd55b76175b63e6b7f68282c7cad38e9e/tldextract-3.1.0-py2.py3-none-any.whl (87kB)\n",
            "\u001b[K     |████████████████████████████████| 92kB 8.3MB/s \n",
            "\u001b[?25hCollecting cssselect>=0.9.2\n",
            "  Downloading https://files.pythonhosted.org/packages/3b/d4/3b5c17f00cce85b9a1e6f91096e1cc8e8ede2e1be8e96b87ce1ed09e92c5/cssselect-1.1.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: beautifulsoup4>=4.4.1 in /usr/local/lib/python3.7/dist-packages (from newspaper3k) (4.6.3)\n",
            "Requirement already satisfied: nltk>=3.2.1 in /usr/local/lib/python3.7/dist-packages (from newspaper3k) (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from feedfinder2>=0.0.4->newspaper3k) (1.15.0)\n",
            "Collecting sgmllib3k\n",
            "  Downloading https://files.pythonhosted.org/packages/9e/bd/3704a8c3e0942d711c1299ebf7b9091930adae6675d7c8f476a7ce48653c/sgmllib3k-1.0.0.tar.gz\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.10.0->newspaper3k) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.10.0->newspaper3k) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.10.0->newspaper3k) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.10.0->newspaper3k) (3.0.4)\n",
            "Collecting requests-file>=1.4\n",
            "  Downloading https://files.pythonhosted.org/packages/77/86/cdb5e8eaed90796aa83a6d9f75cfbd37af553c47a291cd47bc410ef9bdb2/requests_file-1.5.1-py2.py3-none-any.whl\n",
            "Requirement already satisfied: filelock>=3.0.8 in /usr/local/lib/python3.7/dist-packages (from tldextract>=2.0.1->newspaper3k) (3.0.12)\n",
            "Building wheels for collected packages: jieba3k, feedfinder2, tinysegmenter, sgmllib3k\n",
            "  Building wheel for jieba3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for jieba3k: filename=jieba3k-0.35.1-cp37-none-any.whl size=7398406 sha256=543226d1a6801563e4e9bda2a36ca288218f7475339e65624382a6f10e565943\n",
            "  Stored in directory: /root/.cache/pip/wheels/83/15/9c/a3f1f67e7f7181170ad37d32e503c35da20627c013f438ed34\n",
            "  Building wheel for feedfinder2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for feedfinder2: filename=feedfinder2-0.0.4-cp37-none-any.whl size=3358 sha256=e3ec016f762a89dd1b191eb9c8b60661f62185b0da59e538063618b786cf76cd\n",
            "  Stored in directory: /root/.cache/pip/wheels/de/03/ca/778e3a7a627e3d98836cc890e7cb40c7575424cfd3340f40ed\n",
            "  Building wheel for tinysegmenter (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for tinysegmenter: filename=tinysegmenter-0.3-cp37-none-any.whl size=13538 sha256=34513c2ce5431ba8dae61b421c39898daaed513f6582969b839835c6654b2f75\n",
            "  Stored in directory: /root/.cache/pip/wheels/81/2b/43/a02ede72324dd40cdd7ca53aad718c7710628e91b8b0dc0f02\n",
            "  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sgmllib3k: filename=sgmllib3k-1.0.0-cp37-none-any.whl size=6067 sha256=3b5ba97ea89795b7973762971a47d105b91e6dfa2c9a2ac2db001ea79e677fa1\n",
            "  Stored in directory: /root/.cache/pip/wheels/f1/80/5a/444ba08a550cdd241bd9baf8bae44be750efe370adb944506a\n",
            "Successfully built jieba3k feedfinder2 tinysegmenter sgmllib3k\n",
            "Installing collected packages: jieba3k, feedfinder2, sgmllib3k, feedparser, tinysegmenter, requests-file, tldextract, cssselect, newspaper3k\n",
            "Successfully installed cssselect-1.1.0 feedfinder2-0.0.4 feedparser-6.0.2 jieba3k-0.35.1 newspaper3k-0.2.8 requests-file-1.5.1 sgmllib3k-1.0.0 tinysegmenter-0.3 tldextract-3.1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sz7ZKAvyW1Zq",
        "outputId": "422a6481-58ff-4c60-e849-1f1cae297cc0"
      },
      "source": [
        "url = URL_link[4]\n",
        "  \n",
        "# Extract web data \n",
        "url_i = newspaper.Article(url=\"%s\" % (url), language='en') \n",
        "url_i.download() \n",
        "url_i.parse() \n",
        "  \n",
        "# Display scrapped data \n",
        "print(url_i.text) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 370
        },
        "id": "iuIrlE6YKn0v",
        "outputId": "2d3b857e-f2f0-4ae8-98d5-c289d97c979f"
      },
      "source": [
        "import newspaper \n",
        "all_scraped_documents =[]  \n",
        "i= 0\n",
        "# Parse through each url and display its content \n",
        "for url in URL_link: \n",
        "    url_i = newspaper.Article(url=\"%s\" % (url), language='en') \n",
        "    url_i.download() \n",
        "    url_i.parse() \n",
        "    all_scraped_documents.append(url_i.text)\n",
        "    i = i+1\n",
        "    #print(url_i.text) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-80-443d4dae5588>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0murl\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mURL_link\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0murl_i\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnewspaper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mArticle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"%s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'en'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0murl_i\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0murl_i\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mall_scraped_documents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl_i\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/newspaper/article.py\u001b[0m in \u001b[0;36mdownload\u001b[0;34m(self, input_html, title, recursion_counter)\u001b[0m\n\u001b[1;32m    185\u001b[0m                     recursion_counter=recursion_counter + 1)\n\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_html\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhtml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_title\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/newspaper/article.py\u001b[0m in \u001b[0;36mset_html\u001b[0;34m(self, html)\u001b[0m\n\u001b[1;32m    433\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhtml\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhtml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbytes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 435\u001b[0;31m                 \u001b[0mhtml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_parser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_unicode_html\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhtml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    436\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhtml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhtml\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mArticleDownloadState\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSUCCESS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/newspaper/parsers.py\u001b[0m in \u001b[0;36mget_unicode_html\u001b[0;34m(cls, html)\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhtml\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mhtml\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mUnicodeDammit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhtml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_html\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mconverted\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0municode_markup\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             raise Exception(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/bs4/dammit.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, markup, override_encodings, smart_quotes_to, is_html, exclude_encodings)\u001b[0m\n\u001b[1;32m    374\u001b[0m             \u001b[0;31m# try them again with character replacement.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    375\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 376\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mencoding\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencodings\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    377\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mencoding\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"ascii\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    378\u001b[0m                     \u001b[0mu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_convert_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"replace\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/bs4/dammit.py\u001b[0m in \u001b[0;36mencodings\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    262\u001b[0m         \u001b[0;31m# encoding.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchardet_encoding\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 264\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchardet_encoding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchardet_dammit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmarkup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    265\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_usable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchardet_encoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtried\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchardet_encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/bs4/dammit.py\u001b[0m in \u001b[0;36mchardet_dammit\u001b[0;34m(s)\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0;32mimport\u001b[0m \u001b[0mchardet\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mchardet_dammit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mchardet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'encoding'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m         \u001b[0;31m#import chardet.constants\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0;31m#chardet.constants._debug = 1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/chardet/__init__.py\u001b[0m in \u001b[0;36mdetect\u001b[0;34m(byte_str)\u001b[0m\n\u001b[1;32m     36\u001b[0m             \u001b[0mbyte_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbytearray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbyte_str\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0mdetector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mUniversalDetector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m     \u001b[0mdetector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbyte_str\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdetector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/chardet/universaldetector.py\u001b[0m in \u001b[0;36mfeed\u001b[0;34m(self, byte_str)\u001b[0m\n\u001b[1;32m    209\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_charset_probers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLatin1Prober\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mprober\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_charset_probers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m                 \u001b[0;32mif\u001b[0m \u001b[0mprober\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbyte_str\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mProbingState\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFOUND_IT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m                     self.result = {'encoding': prober.charset_name,\n\u001b[1;32m    213\u001b[0m                                    \u001b[0;34m'confidence'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mprober\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_confidence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/chardet/charsetgroupprober.py\u001b[0m in \u001b[0;36mfeed\u001b[0;34m(self, byte_str)\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mprober\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactive\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m                 \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m             \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprober\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbyte_str\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m                 \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/chardet/hebrewprober.py\u001b[0m in \u001b[0;36mfeed\u001b[0;34m(self, byte_str)\u001b[0m\n\u001b[1;32m    225\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mProbingState\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNOT_ME\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 227\u001b[0;31m         \u001b[0mbyte_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter_high_byte_only\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbyte_str\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    228\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mcur\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbyte_str\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/chardet/charsetprober.py\u001b[0m in \u001b[0;36mfilter_high_byte_only\u001b[0;34m(buf)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfilter_high_byte_only\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m         \u001b[0mbuf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mb'([\\x00-\\x7F])+'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mb' '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mbuf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/re.py\u001b[0m in \u001b[0;36msub\u001b[0;34m(pattern, repl, string, count, flags)\u001b[0m\n\u001b[1;32m    192\u001b[0m     \u001b[0ma\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mit\u001b[0m\u001b[0;31m'\u001b[0m\u001b[0ms\u001b[0m \u001b[0mpassed\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mMatch\u001b[0m \u001b[0mobject\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mmust\u001b[0m \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m     a replacement string to be used.\"\"\"\n\u001b[0;32m--> 194\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_compile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrepl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcount\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msubn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrepl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9flncTxFsQGe"
      },
      "source": [
        "# Document Similarity"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "coLnY9eyzskU"
      },
      "source": [
        "documents_df=pd.DataFrame(sentences,columns=['documents'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r7ygwTniwYPl"
      },
      "source": [
        "def most_similar(doc_id,similarity_matrix,matrix):\n",
        "   # print (f'Document: {documents_df.iloc[doc_id][\"documents\"]}')\n",
        "   # print ('\\n')\n",
        "    #print (f'Similar Documents using {matrix}:')\n",
        "    if matrix=='Cosine Similarity':\n",
        "        similar_ix=np.argsort(similarity_matrix[doc_id])[::-1]\n",
        "    elif matrix=='Euclidean Distance':\n",
        "        similar_ix=np.argsort(similarity_matrix[doc_id])\n",
        "    for ix in similar_ix:\n",
        "        if ix==doc_id:\n",
        "            continue\n",
        "        print('\\n')\n",
        "        print (f'Document: {documents_df.iloc[ix][\"documents\"]}')\n",
        "        print (f'{matrix} : {similarity_matrix[doc_id][ix]}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YOLcSobwsYPb",
        "outputId": "ca663630-a101-4f32-9a55-84b1be6e9a2b"
      },
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "sbert_model = SentenceTransformer('bert-base-nli-mean-tokens')\n",
        "\n",
        "document_embeddings = sbert_model.encode(sentences)\n",
        "\n",
        "pairwise_similarities=cosine_similarity(document_embeddings)\n",
        "pairwise_differences=euclidean_distances(document_embeddings)\n",
        "\n",
        "most_similar(0,pairwise_similarities,'Cosine Similarity')\n",
        "most_similar(0,pairwise_differences,'Euclidean Distance')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "Document: 4.2\tMaximization of Accuracy\n",
            " We have used a Google News corpus for training the word vectors.\n",
            "Cosine Similarity : 0.7693995237350464\n",
            "\n",
            "\n",
            "Document: Finally, we found that when we train high dimensional word vectors on a large amount of data, the resulting vectors can be used to answer very subtle semantic relationships between words, such as a city and the country it belongs to, e.g.\n",
            "Cosine Similarity : 0.7453286647796631\n",
            "\n",
            "\n",
            "Document: 1.1\tGoals of the Paper\n",
            " The main goal of this paper is to introduce techniques that can be used for learning high-quality word vectors from huge data sets with billions of words, and with millions of words in the vocabulary.\n",
            "Cosine Similarity : 0.7398302555084229\n",
            "\n",
            "\n",
            "Document: We design a new comprehensive test set for measuring both syntactic and semantic regularities , and show that many such regularities can be learned with high accuracy.\n",
            "Cosine Similarity : 0.7162097692489624\n",
            "\n",
            "\n",
            "Document: Model\tVector\tTraining\tAccuracy [%]\t\tTraining time\n",
            " \tDimensionality\twords\t\t\t[days x CPU cores]\n",
            " \t\t\tSemantic\tSyntactic\tTotal\t\n",
            " NNLM\t100\t6B\t34.2\t64.5\t50.8\t14 x 180\n",
            " CBOW\t1000\t6B\t57.3\t68.9\t63.7\t2 x 140\n",
            " Skip-gram\t1000\t6B\t66.1\t65.1\t65.6\t2.5 x 125\n",
            " Table 7: Comparison and combination of models on the Microsoft Sentence Completion Challenge.\n",
            "Cosine Similarity : 0.7159256935119629\n",
            "\n",
            "\n",
            "Document: Below we report the results of several models trained on the Google News 6B data set, with mini-batch asynchronous gradient descent and the adaptive learning rate procedure called Adagrad [7].\n",
            "Cosine Similarity : 0.689603328704834\n",
            "\n",
            "\n",
            "Document: An example is the popular N-gram model used for statistical language modeling - today, it is possible to train N-grams on virtually all available data (trillions of words [3]).\n",
            "Cosine Similarity : 0.6839544773101807\n",
            "\n",
            "\n",
            "Document: 4.5\tMicrosoft Research Sentence Completion Challenge\n",
            " The Microsoft Sentence Completion Challenge has been recently introduced as a task for advancing language modeling and other NLP techniques [32].\n",
            "Cosine Similarity : 0.677141010761261\n",
            "\n",
            "\n",
            "Document: Using the DistBelief distributed framework, it should be possible to train the CBOW and Skip-gram models even on corpora with one trillion words, for basically unlimited size of the vocabulary.\n",
            "Cosine Similarity : 0.6755150556564331\n",
            "\n",
            "\n",
            "Document: Dimensionality / Training words\t24M\t49M\t98M\t196M\t391M\t783M\n",
            " 50\t13.4\t15.7\t18.6\t19.1\t22.5\t23.2\n",
            " 100\t19.4\t23.1\t27.8\t28.7\t33.4\t32.2\n",
            " 300\t23.2\t29.2\t35.3\t38.6\t43.7\t45.9\n",
            " 600\t24.0\t30.1\t36.5\t40.8\t46.6\t50.4\n",
            " Table 3: Comparison of architectures using models trained on the same data, with 640-dimensional word vectors.\n",
            "Cosine Similarity : 0.6750458478927612\n",
            "\n",
            "\n",
            "Document: A very popular model architecture for estimating neural network language model (NNLM) was proposed in [1], where a feedforward neural network with a linear projection layer and a non-linear hidden layer was used to learn jointly the word vector representation and a statistical language model.\n",
            "Cosine Similarity : 0.6744118928909302\n",
            "\n",
            "\n",
            "Document: As it can be seen, accuracy is quite good, although there is clearly a lot of room for further improvements (note that using our accuracy metric that Table 8: Examples of the word pair relationships, using the best word vectors from Table 4 (Skipgram model trained on 783M words with 300 dimensionality).\n",
            "Cosine Similarity : 0.673480749130249\n",
            "\n",
            "\n",
            "Document: Model\tVector\tTraining\tAccuracy [%]\t\n",
            " \tDimensionality\twords\t\t\n",
            " \t\t\tSemantic\tSyntactic\tTotal\n",
            " Collobert-Weston NNLM\t50\t660M\t9.3\t12.3\t11.0\n",
            " Turian NNLM\t50\t37M\t1.4\t2.6\t2.1\n",
            " Turian NNLM\t200\t37M\t1.4\t2.2\t1.8\n",
            " Mnih NNLM\t50\t37M\t1.8\t9.1\t5.8\n",
            " Mnih NNLM\t100\t37M\t3.3\t13.2\t8.8\n",
            " Mikolov RNNLM\t80\t320M\t4.9\t18.4\t12.7\n",
            " Mikolov RNNLM\t640\t320M\t8.6\t36.5\t24.6\n",
            " Huang NNLM\t50\t990M\t13.3\t11.6\t12.3\n",
            " Our NNLM\t20\t6B\t12.9\t26.4\t20.3\n",
            " Our NNLM\t50\t6B\t27.9\t55.8\t43.2\n",
            " Our NNLM\t100\t6B\t34.2\t64.5\t50.8\n",
            " CBOW\t300\t783M\t15.5\t53.1\t36.1\n",
            " Skip-gram\t300\t783M\t50.0\t55.9\t53.3\n",
            " Table 5: Comparison of models trained for three epochs on the same data and models trained for one epoch.\n",
            "Cosine Similarity : 0.6639412641525269\n",
            "\n",
            "\n",
            "Document: Most of the complexity then comes from H × H.\n",
            " 2.3\tParallel Training of Neural Networks\n",
            " To train models on huge data sets, we have implemented several models on top of a large-scale distributed framework called DistBelief [6], including the feedforward NNLM and the new models proposed in this paper.\n",
            "Cosine Similarity : 0.660672128200531\n",
            "\n",
            "\n",
            "Document: [14]\tT. Mikolov, J. Kopecky, L. Burget, O. Glembek and J.´ Cernockˇ y. Neural network based lan-´ guage models for higly inflective languages, In: Proc.\n",
            "Cosine Similarity : 0.652576744556427\n",
            "\n",
            "\n",
            "Document: 2.2\tRecurrent Neural Net Language Model (RNNLM)\n",
            " Recurrent neural network based language model has been proposed to overcome certain limitations of the feedforward NNLM, such as the need to specify the context length (the order of the model N), and because theoretically RNNs can efficiently represent more complex patterns than the shallow neural networks [15, 2].\n",
            "Cosine Similarity : 0.6522058844566345\n",
            "\n",
            "\n",
            "Document: This follows previous observations that the frequency of words works well for obtaining classes in neural net language models [16].\n",
            "Cosine Similarity : 0.6482982635498047\n",
            "\n",
            "\n",
            "Document: Large language models in machine translation.\n",
            "Cosine Similarity : 0.6475296020507812\n",
            "\n",
            "\n",
            "Document: In this paper, we try to maximize accuracy of these vector operations by developing new model architectures that preserve the linear regularities among words.\n",
            "Cosine Similarity : 0.6430699825286865\n",
            "\n",
            "\n",
            "Document: We believe that word vectors trained on even larger data sets with larger dimensionality will perform significantly better, and will enable the development of new innovative applications.\n",
            "Cosine Similarity : 0.6382659673690796\n",
            "\n",
            "\n",
            "Document: 4\tResults\n",
            " To compare the quality of different versions of word vectors, previous papers typically use a table showing example words and their most similar words, and understand them intuitively.\n",
            "Cosine Similarity : 0.6348801851272583\n",
            "\n",
            "\n",
            "Document: The new architectures directly follow those proposed in our earlier work [13, 14], where it was found that neural network language model can be successfully trained in two steps: first, continuous word vectors are learned using simple model, and then the N-gram NNLM is trained on top of these distributed representations of words.\n",
            "Cosine Similarity : 0.6347163915634155\n",
            "\n",
            "\n",
            "Document: The complexity per training example of the RNN model is\n",
            " \tQ = H × H + H × V,\t(3)\n",
            " where the word representations D have the same dimensionality as the hidden layer H. Again, the term H × V can be efficiently reduced to H × log2(V ) by using hierarchical softmax.\n",
            "Cosine Similarity : 0.6345633268356323\n",
            "\n",
            "\n",
            "Document: Furthermore, we show that these vectors provide state-of-the-art performance on our test set for measuring syntactic and semantic word similarities.\n",
            "Cosine Similarity : 0.634476900100708\n",
            "\n",
            "\n",
            "Document: We believe that our comprehensive test set will help the research community to improve the existing techniques for estimating the word vectors.\n",
            "Cosine Similarity : 0.6336861848831177\n",
            "\n",
            "\n",
            "Document: In the further experiments, we use full set of questions in the new Semantic-Syntactic Word Relationship test set, i.e.\n",
            "Cosine Similarity : 0.6325159072875977\n",
            "\n",
            "\n",
            "Document: 3\tNew Log-linear Models\n",
            " In this section, we propose two new model architectures for learning distributed representations of words that try to minimize computational complexity.\n",
            "Cosine Similarity : 0.6316813230514526\n",
            "\n",
            "\n",
            "Document: A Unified Architecture for Natural Language Processing: Deep Neural Networks with Multitask Learning.\n",
            "Cosine Similarity : 0.6314859986305237\n",
            "\n",
            "\n",
            "Document: To find a word that is similar to small in the same sense as biggest is similar to big, we can simply compute vector X = vector(”biggest”)−vector(”big”)+ vector(”small”).\n",
            "Cosine Similarity : 0.6290920972824097\n",
            "\n",
            "\n",
            "Document: Architecture\tAccuracy [%]\n",
            " 4-gram [32]\t39\n",
            " Average LSA similarity [32]\t49\n",
            " Log-bilinear model [24]\t54.8\n",
            " RNNLMs [19]\t55.4\n",
            " Skip-gram\t48.0\n",
            " Skip-gram + RNNLMs\t58.9\n",
            " estimate since the data center machines are shared with other production tasks, and the usage can fluctuate quite a bit.\n",
            "Cosine Similarity : 0.6250507235527039\n",
            "\n",
            "\n",
            "Document: We further denote two pairs of words with the same relationship as a question, as we can ask: ”What is the word that is similar to small in the same sense as biggest is similar to big?”\n",
            " Somewhat surprisingly, these questions can be answered by performing simple algebraic operations with the vector representation of words.\n",
            "Cosine Similarity : 0.622162938117981\n",
            "\n",
            "\n",
            "Document: Strategies for Training Large Scale´ Neural Network Language Models, In: Proc.\n",
            "Cosine Similarity : 0.6201924681663513\n",
            "\n",
            "\n",
            "Document: We found that increasing the range improves quality of the resulting word vectors, but it also increases the computational complexity.\n",
            "Cosine Similarity : 0.616401731967926\n",
            "\n",
            "\n",
            "Document: Our ongoing work shows that the word vectors can be successfully applied to automatic extension of facts in Knowledge Bases, and also for verification of correctness of existing facts.\n",
            "Cosine Similarity : 0.6135175228118896\n",
            "\n",
            "\n",
            "Document: Clearly, we are facing time constrained optimization problem, as it can be expected that both using more data and higher dimensional word vectors will improve the accuracy.\n",
            "Cosine Similarity : 0.6120108366012573\n",
            "\n",
            "\n",
            "Document: In this paper, we focus on distributed representations of words learned by neural networks, as it was previously shown that they perform significantly better than LSA for preserving linear regularities among words [20, 31]; LDA moreover becomes computationally very expensive on large data sets.\n",
            "Cosine Similarity : 0.6078912615776062\n",
            "\n",
            "\n",
            "Document: Model\tVector\tTraining\tAccuracy [%]\t\tTraining time\n",
            " \tDimensionality\twords\t\t\t[days]\n",
            " \t\t\tSemantic\tSyntactic\tTotal\t\n",
            " 3 epoch CBOW\t300\t783M\t15.5\t53.1\t36.1\t1\n",
            " 3 epoch Skip-gram\t300\t783M\t50.0\t55.9\t53.3\t3\n",
            " 1 epoch CBOW\t300\t783M\t13.8\t49.9\t33.6\t0.3\n",
            " 1 epoch CBOW\t300\t1.6B\t16.1\t52.6\t36.1\t0.6\n",
            " 1 epoch CBOW\t600\t783M\t15.4\t53.3\t36.2\t0.7\n",
            " 1 epoch Skip-gram\t300\t783M\t45.6\t52.2\t49.2\t1\n",
            " 1 epoch Skip-gram\t300\t1.6B\t52.2\t55.1\t53.8\t2\n",
            " 1 epoch Skip-gram\t600\t783M\t56.7\t54.5\t55.5\t2.5\n",
            " of the Google News data in about a day, while training time for the Skip-gram model was about three days.\n",
            "Cosine Similarity : 0.6045629978179932\n",
            "\n",
            "\n",
            "Document: The Microsoft Research Sentence Completion Challenge, Microsoft Research Technical Report MSR-TR-2011-129, 2011.\n",
            "Cosine Similarity : 0.6031613349914551\n",
            "\n",
            "\n",
            "Document: While this observation might seem trivial, it must be noted that it is currently popular to train word vectors on relatively large amounts of data, but with insufficient size Table 2: Accuracy on subset of the Semantic-Syntactic Word Relationship test set, using word vectors from the CBOW architecture with limited vocabulary.\n",
            "Cosine Similarity : 0.6023099422454834\n",
            "\n",
            "\n",
            "Document: Furthermore, we also use words from the future; we have obtained the best performance on the task introduced in the next section by building a log-linear classifier with four future and four history words at the input, where the training criterion is to correctly classify the current (middle) word.\n",
            "Cosine Similarity : 0.6010338068008423\n",
            "\n",
            "\n",
            "Document: It was later shown that the word vectors can be used to significantly improve and simplify many NLP applications [4, 5, 29].\n",
            "Cosine Similarity : 0.5989081263542175\n",
            "\n",
            "\n",
            "Document: For example, we have observed good accuracy for selecting out-of-the-list words, by computing average vector for a list of words, and finding the most distant word vector.\n",
            "Cosine Similarity : 0.5988873243331909\n",
            "\n",
            "\n",
            "Document: Word vectors with such semantic relationships could be used to improve many existing NLP applications, such as machine translation, information retrieval and question answering systems, and may enable other future applications yet to be invented.\n",
            "Cosine Similarity : 0.5985941290855408\n",
            "\n",
            "\n",
            "Document: The accuracies are reported on our Semantic-Syntactic Word Relationship test set, and on the syntactic relationship test set of [20]\n",
            " Model\n",
            " Architecture\tSemantic-Syntactic Word Relationship test set\tMSR Word Relatedness\n",
            " Test Set [20]\n",
            " \tSemantic Accuracy [%]\tSyntactic Accuracy [%]\t\n",
            " RNNLM\t9\t36\t35\n",
            " NNLM\t23\t53\t47\n",
            " CBOW\t24\t64\t61\n",
            " Skip-gram\t55\t59\t56\n",
            " (such as 50 - 100).\n",
            "Cosine Similarity : 0.5958734154701233\n",
            "\n",
            "\n",
            "Document: We observed that it is possible to train high quality word vectors using very simple model architectures, compared to the popular neural network models (both feedforward and recurrent).\n",
            "Cosine Similarity : 0.594068169593811\n",
            "\n",
            "\n",
            "Document: Relationship\tExample 1\tExample 2\tExample 3\n",
            " France - Paris\tItaly: Rome\tJapan: Tokyo\tFlorida: Tallahassee\n",
            " big - bigger\tsmall: larger\tcold: colder\tquick: quicker\n",
            " Miami - Florida\tBaltimore: Maryland\tDallas: Texas\tKona: Hawaii\n",
            " Einstein - scientist\tMessi: midfielder\tMozart: violinist\tPicasso: painter\n",
            " Sarkozy - France\tBerlusconi: Italy\tMerkel: Germany\tKoizumi: Japan\n",
            " copper - Cu\tzinc: Zn\tgold: Au\turanium: plutonium\n",
            " Berlusconi - Silvio\tSarkozy: Nicolas\tPutin: Medvedev\tObama: Barack\n",
            " Microsoft - Windows\tGoogle: Android\tIBM: Linux\tApple: iPhone\n",
            " Microsoft - Ballmer\tGoogle: Yahoo\tIBM: McNealy\tApple: Jobs\n",
            " Japan - sushi\tGermany: bratwurst\tFrance: tapas\tUSA: pizza\n",
            " assumes exact match, the results in Table 8 would score only about 60%).\n",
            "Cosine Similarity : 0.5935059189796448\n",
            "\n",
            "\n",
            "Document: We also expect that high quality word vectors will become an important building block for future NLP applications.\n",
            "Cosine Similarity : 0.5907735228538513\n",
            "\n",
            "\n",
            "Document: 6\tConclusion\n",
            " In this paper we studied the quality of vector representations of words derived by various models on a collection of syntactic and semantic language tasks.\n",
            "Cosine Similarity : 0.5897421836853027\n",
            "\n",
            "\n",
            "Document: Using a word offset technique where simple algebraic operations are performed on the word vectors, it was shown for example that vector(”King”) - vector(”Man”) + vector(”Woman”) results in a vector that is closest to the vector representation of the word Queen [20].\n",
            "Cosine Similarity : 0.5892516374588013\n",
            "\n",
            "\n",
            "Document: Language Modeling for Speech Recognition in Czech, Masters thesis, Brno University of Technology, 2007.\n",
            "Cosine Similarity : 0.5889636278152466\n",
            "\n",
            "\n",
            "Document: 2\tModel Architectures\n",
            " Many different types of models were proposed for estimating continuous representations of words, including the well-known Latent Semantic Analysis (LSA) and Latent Dirichlet Allocation (LDA).\n",
            "Cosine Similarity : 0.5864479541778564\n",
            "\n",
            "\n",
            "Document: Then, we search in the vector space for the word closest to X measured by cosine distance, and use it as the answer to the question (we discard the input question words during this search).\n",
            "Cosine Similarity : 0.5862692594528198\n",
            "\n",
            "\n",
            "Document: In Table 3, it can be seen that the word vectors from the RNN (as used in [20]) perform well mostly on the syntactic questions.\n",
            "Cosine Similarity : 0.5855271816253662\n",
            "\n",
            "\n",
            "Document: Improving Word Representations via Global Context and Multiple Word Prototypes.\n",
            "Cosine Similarity : 0.5851956605911255\n",
            "\n",
            "\n",
            "Document: This has been observed earlier in the context of inflectional languages - for example, nouns can have multiple word endings, and if we search for similar words in a subspace of the original vector space, it is possible to find words that have similar endings [13, 14].\n",
            "Cosine Similarity : 0.5844974517822266\n",
            "\n",
            "\n",
            "Document: 4.3\tComparison of Model Architectures\n",
            " First we compare different model architectures for deriving the word vectors using the same training data and using the same dimensionality of 640 of the word vectors.\n",
            "Cosine Similarity : 0.5823829174041748\n",
            "\n",
            "\n",
            "Document: Thus, most of the complexity is caused by the term N × D × H.\n",
            " In our models, we use hierarchical softmax where the vocabulary is represented as a Huffman binary tree.\n",
            "Cosine Similarity : 0.5819079875946045\n",
            "\n",
            "\n",
            "Document: The CBOW model was trained on subset Table 4: Comparison of publicly available word vectors on the Semantic-Syntactic Word Relationship test set, and word vectors from our models.\n",
            "Cosine Similarity : 0.5798032283782959\n",
            "\n",
            "\n",
            "Document: In: Large-Scale Kernel Machines, MIT Press, 2007.\n",
            "Cosine Similarity : 0.5780225396156311\n",
            "\n",
            "\n",
            "Document: Given Equation 4, increasing amount of training data twice results in about the same increase of computational complexity as increasing vector size twice.\n",
            "Cosine Similarity : 0.5733173489570618\n",
            "\n",
            "\n",
            "Document: [17]\tT. Mikolov, A. Deoras, S. Kombrink, L. Burget, J. Cernockˇ\ty. Empirical Evaluation and Com-´ bination of Advanced Language Modeling Techniques, In: Proceedings of Interspeech, 2011.\n",
            "Cosine Similarity : 0.5718151926994324\n",
            "\n",
            "\n",
            "Document: For all the following models, the training complexity is proportional to\n",
            " \tO = E × T × Q,\t(1)\n",
            " where E is number of the training epochs, T is the number of the words in the training set and Q is defined further for each model architecture.\n",
            "Cosine Similarity : 0.5716006755828857\n",
            "\n",
            "\n",
            "Document: Dynamic Pooling and Unfolding Recursive Autoencoders for Paraphrase Detection.\n",
            "Cosine Similarity : 0.5698275566101074\n",
            "\n",
            "\n",
            "Document: Adaptive subgradient methods for online learning and stochastic optimization.\n",
            "Cosine Similarity : 0.567306637763977\n",
            "\n",
            "\n",
            "Document: 4.4\tLarge Scale Parallel Training of Models\n",
            " As mentioned earlier, we have implemented various models in a distributed framework called DistBelief.\n",
            "Cosine Similarity : 0.5672658681869507\n",
            "\n",
            "\n",
            "Document: The quality of these representations is measured in a word similarity task, and the results are compared to the previously best performing techniques based on different types of neural networks.\n",
            "Cosine Similarity : 0.5643119812011719\n",
            "\n",
            "\n",
            "Document: Performance of several techniques has been already reported on this set, including N-gram models, LSA-based model [32], log-bilinear model [24] and a combination of recurrent neural networks that currently holds the state of the art performance of 55.4% accuracy on this benchmark [19].\n",
            "Cosine Similarity : 0.5627422332763672\n",
            "\n",
            "\n",
            "Document: To estimate the best choice of model architecture for obtaining as good as possible results quickly, we have first evaluated models trained on subsets of the training data, with vocabulary restricted to the most frequent 30k words.\n",
            "Cosine Similarity : 0.5613564848899841\n",
            "\n",
            "\n",
            "Document: However, we believe that usefulness of the word vectors for certain applications should be positively correlated with this accuracy metric.\n",
            "Cosine Similarity : 0.560248613357544\n",
            "\n",
            "\n",
            "Document: So, we have to increase both vector dimensionality and the amount of the training data together.\n",
            "Cosine Similarity : 0.5565042495727539\n",
            "\n",
            "\n",
            "Document: More precisely, we use each current word as an input to a log-linear classifier with continuous projection layer, and predict words within a certain range before and after the current word.\n",
            "Cosine Similarity : 0.5515553951263428\n",
            "\n",
            "\n",
            "Document: Thus, if we choose C = 5, for each training word we will select randomly a number R in range < 1;C >, and then use R words from history and\n",
            " \t       INPUT         PROJECTION         OUTPUT\t          INPUT         PROJECTION      OUTPUT\n",
            " w(t-2)w(t-2)\n",
            " w(t-1)w(t-1)\n",
            " \tw(t)\tw(t)\n",
            " w(t+1)w(t+1)\n",
            " w(t+2)w(t+2)\n",
            "                    CBOW                                                   Skip-gram\n",
            " Figure 1: New model architectures.\n",
            "Cosine Similarity : 0.5511009693145752\n",
            "\n",
            "\n",
            "Document: A fast and simple algorithm for training neural probabilistic language models.\n",
            "Cosine Similarity : 0.550777792930603\n",
            "\n",
            "\n",
            "Document: In Proceedings of the Joint Conference on Empirical Methods in Natural Language Processing and Computational Language Learning, 2007.\n",
            "Cosine Similarity : 0.5506722331047058\n",
            "\n",
            "\n",
            "Document: With progress of machine learning techniques in recent years, it has become possible to train more complex models on much larger data set, and they typically outperform the simple models.\n",
            "Cosine Similarity : 0.5486314296722412\n",
            "\n",
            "\n",
            "Document: We follow previous observation that there can be many different types of similarities between words, for example, word big is similar to bigger in the same sense that small is similar to smaller.\n",
            "Cosine Similarity : 0.5477935075759888\n",
            "\n",
            "\n",
            "Document: Type of relationship\tWord Pair 1\tWord Pair 2\n",
            " Common capital city\tAthens\tGreece\tOslo\tNorway\n",
            " All capital cities\tAstana\tKazakhstan\tHarare\tZimbabwe\n",
            " Currency\tAngola\tkwanza\tIran\trial\n",
            " City-in-state\tChicago\tIllinois\tStockton\tCalifornia\n",
            " Man-Woman\tbrother\tsister\tgrandson\tgranddaughter\n",
            " Adjective to adverb\tapparent\tapparently\trapid\trapidly\n",
            " Opposite\tpossibly\timpossibly\tethical\tunethical\n",
            " Comparative\tgreat\tgreater\ttough\ttougher\n",
            " Superlative\teasy\teasiest\tlucky\tluckiest\n",
            " Present Participle\tthink\tthinking\tread\treading\n",
            " Nationality adjective\tSwitzerland\tSwiss\tCambodia\tCambodian\n",
            " Past tense\twalking\twalked\tswimming\tswam\n",
            " Plural nouns\tmouse\tmice\tdollar\tdollars\n",
            " Plural verbs\twork\tworks\tspeak\tspeaks\n",
            " 4.1\tTask Description\n",
            " To measure quality of the word vectors, we define a comprehensive test set that contains five types of semantic questions, and nine types of syntactic questions.\n",
            "Cosine Similarity : 0.5434209108352661\n",
            "\n",
            "\n",
            "Document: Similar to [18], to compare different model architectures we define first the computational complexity of a model as the number of parameters that need to be accessed to fully train the model.\n",
            "Cosine Similarity : 0.5433699488639832\n",
            "\n",
            "\n",
            "Document: This choice has several good reasons - simplicity, robustness and the observation that simple models trained on huge amounts of data outperform complex systems trained on less data.\n",
            "Cosine Similarity : 0.5428494215011597\n",
            "\n",
            "\n",
            "Document: The neural network based word vectors were previously applied to many other NLP tasks, for example sentiment analysis [12] and paraphrase detection [28].\n",
            "Cosine Similarity : 0.5409491658210754\n",
            "\n",
            "\n",
            "Document: Thus, the computational complexity per each training example is\n",
            " \tQ = N × D + N × D × H + H × V,\t(2)\n",
            " where the dominating term is H × V .\n",
            "Cosine Similarity : 0.5390467643737793\n",
            "\n",
            "\n",
            "Document: Journal of Machine Learning Research, 3:1137-1155, 2003.\n",
            "Cosine Similarity : 0.5365540981292725\n",
            "\n",
            "\n",
            "Document: This will require us to do R × 2 word classifications, with the current word as input, and each of the R + R words as output.\n",
            "Cosine Similarity : 0.5362706184387207\n",
            "\n",
            "\n",
            "Document: Moreover, the hidden layer is used to compute probability distribution over all the words in the vocabulary, resulting in an output layer with dimensionality V .\n",
            "Cosine Similarity : 0.5361760854721069\n",
            "\n",
            "\n",
            "Document: For this parallel training, we use mini-batch asynchronous gradient descent with an adaptive learning rate procedure called Adagrad [7].\n",
            "Cosine Similarity : 0.5360239744186401\n",
            "\n",
            "\n",
            "Document: 7\tFollow-Up Work\n",
            " After the initial version of this paper was written, we published single-machine multi-threaded C++ code for computing the word vectors, using both the continuous bag-of-words and skip-gram architectures .\n",
            "Cosine Similarity : 0.5345257520675659\n",
            "\n",
            "\n",
            "Document: We use recently proposed techniques for measuring the quality of the resulting vector representations, with the expectation that not only will similar words tend to be close to each other, but that words can have multiple degrees of similarity [20].\n",
            "Cosine Similarity : 0.5308755040168762\n",
            "\n",
            "\n",
            "Document: Then, a large list of questions is formed by connecting two word pairs.\n",
            "Cosine Similarity : 0.5302681922912598\n",
            "\n",
            "\n",
            "Document: Three new graphical models for statistical language modelling.\n",
            "Cosine Similarity : 0.5297802686691284\n",
            "\n",
            "\n",
            "Document: We also published more than 1.4 million vectors that represent named entities, trained on more than 100 billion words.\n",
            "Cosine Similarity : 0.5272673964500427\n",
            "\n",
            "\n",
            "Document: Further progress can be achieved by incorporating information about structure of words, especially for the syntactic questions.\n",
            "Cosine Similarity : 0.5267382860183716\n",
            "\n",
            "\n",
            "Document: Advances in Neural Information Processing Systems 21, MIT Press, 2009.\n",
            "Cosine Similarity : 0.5257976651191711\n",
            "\n",
            "\n",
            "Document: Recurrent neural network´ based language model, In: Proceedings of Interspeech, 2010.\n",
            "Cosine Similarity : 0.5252130031585693\n",
            "\n",
            "\n",
            "Document: 3.2\tContinuous Skip-gram Model\n",
            " The second architecture is similar to CBOW, but instead of predicting the current word based on the context, it tries to maximize classification of a word based on another word in the same sentence.\n",
            "Cosine Similarity : 0.5231382250785828\n",
            "\n",
            "\n",
            "Document: We trained a feedforward NNLM with the same number of 640 hidden units using the DistBelief parallel training [6], using a history of 8 previous words (thus, the NNLM has more parameters than the RNNLM, as the projection layer has size 640 × 8).\n",
            "Cosine Similarity : 0.5228265523910522\n",
            "\n",
            "\n",
            "Document: Scaling learning algorithms towards AI.\n",
            "Cosine Similarity : 0.5201882719993591\n",
            "\n",
            "\n",
            "Document: 2.1\tFeedforward Neural Net Language Model (NNLM)\n",
            " The probabilistic feedforward neural network language model has been proposed in [1].\n",
            "Cosine Similarity : 0.5200732350349426\n",
            "\n",
            "\n",
            "Document: Although it is easy to show that word France is similar to Italy and perhaps some other countries, it is much more challenging when subjecting those vectors in a more complex similarity task, as follows.\n",
            "Cosine Similarity : 0.5195497870445251\n",
            "\n",
            "\n",
            "Document: While this is what makes neural networks so attractive, we decided to explore simpler models that might not be able to represent the data as precisely as neural networks, but can possibly be trained on much more data efficiently.\n",
            "Cosine Similarity : 0.5190775394439697\n",
            "\n",
            "\n",
            "Document: While the Skip-gram model itself does not perform on this task better than LSA similarity, the scores from this model are complementary to scores obtained with RNNLMs, and a weighted combination leads to a new state of the art result 58.9% accuracy (59.2% on the development part of the set and 58.7% on the test part of the set).\n",
            "Cosine Similarity : 0.518877387046814\n",
            "\n",
            "\n",
            "Document: All models are trained using stochastic gradient descent and backpropagation [26].\n",
            "Cosine Similarity : 0.5175935626029968\n",
            "\n",
            "\n",
            "Document: The questions in each category were created in two steps: first, a list of similar word pairs was created manually.\n",
            "Cosine Similarity : 0.5171796083450317\n",
            "\n",
            "\n",
            "Document: In this work, we directly extend this architecture, and focus just on the first step where the word vectors are learned using a simple model.\n",
            "Cosine Similarity : 0.516791820526123\n",
            "\n",
            "\n",
            "Document: For example, the amount of relevant in-domain data for automatic speech recognition is limited - the performance is usually dominated by the size of high quality transcribed speech data (often just millions of words).\n",
            "Cosine Similarity : 0.5166217088699341\n",
            "\n",
            "\n",
            "Document: Then, we compute score of each sentence in the test set by using the unknown word at the input, and predict all surrounding words in a sentence.\n",
            "Cosine Similarity : 0.5164264440536499\n",
            "\n",
            "\n",
            "Document: This is a popular type of problems in certain human intelligence tests.\n",
            "Cosine Similarity : 0.5161146521568298\n",
            "\n",
            "\n",
            "Document: Results from machine translation experiments also look very promising.\n",
            "Cosine Similarity : 0.5153805613517761\n",
            "\n",
            "\n",
            "Document: 3.1\tContinuous Bag-of-Words Model\n",
            " The first proposed architecture is similar to the feedforward NNLM, where the non-linear hidden layer is removed and the projection layer is shared for all words (not just the projection matrix); thus, all words get projected into the same position (their vectors are averaged).\n",
            "Cosine Similarity : 0.5151841640472412\n",
            "\n",
            "\n",
            "Document: Estimation of the word vectors itself was performed using different model architectures and trained on various corpora [4, 29, 23, 19, 9], and some of the resulting word vectors were made available for future research and comparison .\n",
            "Cosine Similarity : 0.5151371955871582\n",
            "\n",
            "\n",
            "Document: Accuracy is reported on the full Semantic-Syntactic data set.\n",
            "Cosine Similarity : 0.514263927936554\n",
            "\n",
            "\n",
            "Document: The NNLM architecture becomes complex for computation between the projection and the hidden layer, as values in the projection layer are dense.\n",
            "Cosine Similarity : 0.5141286849975586\n",
            "\n",
            "\n",
            "Document: Probably the most successful concept is to use distributed representations of words [10].\n",
            "Cosine Similarity : 0.513513445854187\n",
            "\n",
            "\n",
            "Document: Under this framework, it is common to use one hundred or more model replicas, each using many CPU cores at different machines in a data center.\n",
            "Cosine Similarity : 0.5117645263671875\n",
            "\n",
            "\n",
            "Document: (4)\n",
            " We denote this model further as CBOW, as unlike standard bag-of-words model, it uses continuous distributed representation of the context.\n",
            "Cosine Similarity : 0.5104221701622009\n",
            "\n",
            "\n",
            "Document: However, as far as we know, these architectures were significantly more computationally expensive for training than the one proposed in [13], with the exception of certain version of log-bilinear model where diagonal weight matrices are used [23].\n",
            "Cosine Similarity : 0.5094785690307617\n",
            "\n",
            "\n",
            "Document: Ng., Large Scale Distributed Deep Networks, NIPS, 2012.\n",
            "Cosine Similarity : 0.5086979269981384\n",
            "\n",
            "\n",
            "Document: The training data consists of several LDC corpora and is described in detail in [18] (320M words, 82K vocabulary).\n",
            "Cosine Similarity : 0.5084385871887207\n",
            "\n",
            "\n",
            "Document: Linguistic Regularities in Continuous Space Word Representations.\n",
            "Cosine Similarity : 0.5068914294242859\n",
            "\n",
            "\n",
            "Document: The framework allows us to run multiple replicas of the same model in parallel, and each replica synchronizes its gradient updates through a centralized server that keeps all the parameters.\n",
            "Cosine Similarity : 0.5065144896507263\n",
            "\n",
            "\n",
            "Document: We also include results on a test set introduced in [20] that focuses on syntactic similarity between words .\n",
            "Cosine Similarity : 0.5037089586257935\n",
            "\n",
            "\n",
            "Document: Hierarchical Probabilistic Neural Network Language Model.\n",
            "Cosine Similarity : 0.5035409927368164\n",
            "\n",
            "\n",
            "Document: The results using the CBOW architecture with different choice of word vector dimensionality and increasing amount of the training data are shown in Table 2.\n",
            "Cosine Similarity : 0.503190279006958\n",
            "\n",
            "\n",
            "Document: By using ten examples instead of one to form the relationship vector (we average the individual vectors together), we have observed improvement of accuracy of our best models by about 10% absolutely on the semantic-syntactic test.\n",
            "Cosine Similarity : 0.5004463791847229\n",
            "\n",
            "\n",
            "Document: The NNLM vectors perform significantly better than the RNN - this is not surprising, as the word vectors in the RNNLM are directly connected to a non-linear hidden layer.\n",
            "Cosine Similarity : 0.49912258982658386\n",
            "\n",
            "\n",
            "Document: For example when the vocabulary size is one million words, this results in about two times speedup in evaluation.\n",
            "Cosine Similarity : 0.49831366539001465\n",
            "\n",
            "\n",
            "Document: Learning word vectors for sentiment analysis.\n",
            "Cosine Similarity : 0.49729448556900024\n",
            "\n",
            "\n",
            "Document: Journal of Machine Learning Research, 12:24932537, 2011.\n",
            "Cosine Similarity : 0.49532049894332886\n",
            "\n",
            "\n",
            "Document: Training a model on twice as much data using one epoch gives comparable or better results than iterating over the same data for three epochs, as is shown in Table 5, and provides additional small speedup.\n",
            "Cosine Similarity : 0.49522334337234497\n",
            "\n",
            "\n",
            "Document: Note that due to the overhead of the distributed framework, the CPU usage of the CBOW model and the Skip-gram model are much closer to each other than their single-machine implementations.\n",
            "Cosine Similarity : 0.49366676807403564\n",
            "\n",
            "\n",
            "Document: The training complexity of this architecture is proportional to\n",
            " \tQ = C × (D + D × log2(V )),\t(5)\n",
            " where C is the maximum distance of the words.\n",
            "Cosine Similarity : 0.4922415614128113\n",
            "\n",
            "\n",
            "Document: For example, we made a list of 68 large American cities and the states they belong to, and formed about 2.5K questions by picking two word pairs at random.\n",
            "Cosine Similarity : 0.4914901554584503\n",
            "\n",
            "\n",
            "Document: Moreover, we discuss how training time and accuracy depends on the dimensionality of the word vectors and on the amount of the training data.\n",
            "Cosine Similarity : 0.48907703161239624\n",
            "\n",
            "\n",
            "Document: In International Conference on Machine Learning, ICML, 2008.\n",
            "Cosine Similarity : 0.48696255683898926\n",
            "\n",
            "\n",
            "Document: Association for Computational Linguistics, 2010.\n",
            "Cosine Similarity : 0.4859180450439453\n",
            "\n",
            "\n",
            "Document: A neural probabilistic language model.\n",
            "Cosine Similarity : 0.4853484332561493\n",
            "\n",
            "\n",
            "Document: The CBOW architecture predicts the current word based on the context, and the Skip-gram predicts surrounding words given the current word.\n",
            "Cosine Similarity : 0.4835830330848694\n",
            "\n",
            "\n",
            "Document: Another interesting architecture of NNLM was presented in [13, 14], where the word vectors are first learned using neural network with a single hidden layer.\n",
            "Cosine Similarity : 0.4815746545791626\n",
            "\n",
            "\n",
            "Document: A Scalable Hierarchical Distributed Language Model.\n",
            "Cosine Similarity : 0.4780651926994324\n",
            "\n",
            "\n",
            "Document: We evaluate the overall accuracy for all question types, and for each question type separately (semantic, syntactic).\n",
            "Cosine Similarity : 0.4756277799606323\n",
            "\n",
            "\n",
            "Document: International Joint Conference on Artificial Intelligence, 2005.\n",
            "Cosine Similarity : 0.47557854652404785\n",
            "\n",
            "\n",
            "Document: What is special for this type of model is the recurrent matrix that connects hidden layer to itself, using time-delayed connections.\n",
            "Cosine Similarity : 0.47511836886405945\n",
            "\n",
            "\n",
            "Document: Because of the much lower computational complexity, it is possible to compute very accurate high dimensional word vectors from a much larger data set.\n",
            "Cosine Similarity : 0.4746166169643402\n",
            "\n",
            "\n",
            "Document: While there has been later substantial amount of work that focuses on learning word vectors, we consider the approach proposed in [13] to be the simplest one.\n",
            "Cosine Similarity : 0.47158676385879517\n",
            "\n",
            "\n",
            "Document: Overall, there are 8869 semantic and 10675 syntactic questions.\n",
            "Cosine Similarity : 0.4701477289199829\n",
            "\n",
            "\n",
            "Document: Note that the weight matrix between the input and the projection layer is shared for all word positions in the same way as in the NNLM.\n",
            "Cosine Similarity : 0.4695628881454468\n",
            "\n",
            "\n",
            "Document: We have explored the performance of Skip-gram architecture on this task.\n",
            "Cosine Similarity : 0.46911129355430603\n",
            "\n",
            "\n",
            "Document: The CBOW architecture works better than the NNLM on the syntactic tasks, and about the same on the semantic one.\n",
            "Cosine Similarity : 0.4691014289855957\n",
            "\n",
            "\n",
            "Document: Extensions of recurrent neural´ network language model, In: Proceedings of ICASSP 2011.\n",
            "Cosine Similarity : 0.46639955043792725\n",
            "\n",
            "\n",
            "Document: Clearly, there is still a lot of discoveries to be made using these techniques.\n",
            "Cosine Similarity : 0.46616941690444946\n",
            "\n",
            "\n",
            "Document: In: Parallel distributed processing: Explorations in the microstructure of cognition.\n",
            "Cosine Similarity : 0.4638737142086029\n",
            "\n",
            "\n",
            "Document: 1.2\tPrevious Work\n",
            " Representation of words as continuous vectors has a long history [10, 26, 8].\n",
            "Cosine Similarity : 0.4628114104270935\n",
            "\n",
            "\n",
            "Document: Association for Computational Linguistics, 2012.\n",
            "Cosine Similarity : 0.46277230978012085\n",
            "\n",
            "\n",
            "Document: At the input layer, N previous words are encoded using 1-of-V coding, where V is size of the vocabulary.\n",
            "Cosine Similarity : 0.4622572660446167\n",
            "\n",
            "\n",
            "Document: It can be expected that these applications can benefit from the model architectures described in this paper.\n",
            "Cosine Similarity : 0.46222826838493347\n",
            "\n",
            "\n",
            "Document: This task consists of 1040 sentences, where one word is missing in each sentence and the goal is to select word that is the most coherent with the rest of the sentence, given a list of five reasonable choices.\n",
            "Cosine Similarity : 0.4580868184566498\n",
            "\n",
            "\n",
            "Document: Note that training of NNLM with 1000-dimensional vectors would take too long to complete.\n",
            "Cosine Similarity : 0.4574417471885681\n",
            "\n",
            "\n",
            "Document: Huffman trees assign short binary codes to frequent words, and this further reduces the number of output units that need to be evaluated: while balanced binary tree would require log2(V ) outputs to be evaluated, the Huffman tree based hierarchical softmax requires only about log2(Unigram perplexity(V )).\n",
            "Cosine Similarity : 0.4566797614097595\n",
            "\n",
            "\n",
            "Document: Combining Heterogeneous Models for Measuring Relational Similarity.\n",
            "Cosine Similarity : 0.45566821098327637\n",
            "\n",
            "\n",
            "Document: The number of CPU cores is an Table 6: Comparison of models trained using the DistBelief distributed framework.\n",
            "Cosine Similarity : 0.45245125889778137\n",
            "\n",
            "\n",
            "Document: Table 1: Examples of five types of semantic and nine types of syntactic questions in the SemanticSyntactic Word Relationship test set.\n",
            "Cosine Similarity : 0.45137330889701843\n",
            "\n",
            "\n",
            "Document: Somewhat surprisingly, it was found that similarity of word representations goes beyond simple syntactic regularities.\n",
            "Cosine Similarity : 0.4506218433380127\n",
            "\n",
            "\n",
            "Document: Using the sentence scores, we choose the most likely sentence.\n",
            "Cosine Similarity : 0.4489284157752991\n",
            "\n",
            "\n",
            "Document: Semeval-2012 task 2: Measuring degrees of relational similarity.\n",
            "Cosine Similarity : 0.44870954751968384\n",
            "\n",
            "\n",
            "Document: Finally, the Skip-gram architecture works slightly worse on the syntactic task than the CBOW model (but still better than the NNLM), and much better on the semantic part of the test than all the other models.\n",
            "Cosine Similarity : 0.448149710893631\n",
            "\n",
            "\n",
            "Document: In the future, it would be also interesting to compare our techniques to Latent Relational Analysis [30] and others.\n",
            "Cosine Similarity : 0.4472295641899109\n",
            "\n",
            "\n",
            "Document: The publicly available RNN vectors were used together with other techniques to achieve over 50% increase in Spearman’s rank correlation over the previous best result [31].\n",
            "Cosine Similarity : 0.4468814432621002\n",
            "\n",
            "\n",
            "Document: [15]\tT. Mikolov, M. Karafiat, L. Burget, J.´ Cernockˇ y, S. Khudanpur.\n",
            "Cosine Similarity : 0.444038450717926\n",
            "\n",
            "\n",
            "Document: The training speed is significantly higher than reported earlier in this paper, i.e.\n",
            "Cosine Similarity : 0.44332921504974365\n",
            "\n",
            "\n",
            "Document: Measuring Semantic Similarity by Latent Relational Analysis.\n",
            "Cosine Similarity : 0.4423941969871521\n",
            "\n",
            "\n",
            "Document: An interesting task where the word vectors have recently been shown to significantly outperform the previous state of the art is the SemEval-2012 Task 2 [11].\n",
            "Cosine Similarity : 0.4418867528438568\n",
            "\n",
            "\n",
            "Document: it is in the order of billions of words per hour for typical hyperparameter choices.\n",
            "Cosine Similarity : 0.4413698613643646\n",
            "\n",
            "\n",
            "Document: Statistical Language Models based on Neural Networks.\n",
            "Cosine Similarity : 0.44124019145965576\n",
            "\n",
            "\n",
            "Document: Next, we will try to maximize the accuracy, while minimizing the computational complexity.\n",
            "Cosine Similarity : 0.4401335120201111\n",
            "\n",
            "\n",
            "Document: For the experiments reported in Tables 2 and 4, we used three training epochs with stochastic gradient descent and backpropagation.\n",
            "Cosine Similarity : 0.4400547742843628\n",
            "\n",
            "\n",
            "Document: First, we train the 640dimensional model on 50M words provided in [32].\n",
            "Cosine Similarity : 0.4392668604850769\n",
            "\n",
            "\n",
            "Document: Question is assumed to be correctly answered only if the closest word to the vector computed using the above method is exactly the same as the correct word in the question; synonyms are thus counted as mistakes.\n",
            "Cosine Similarity : 0.4392404556274414\n",
            "\n",
            "\n",
            "Document: [16]\tT. Mikolov, S. Kombrink, L. Burget, J. Cernockˇ y, S. Khudanpur.\n",
            "Cosine Similarity : 0.4381093382835388\n",
            "\n",
            "\n",
            "Document: Full vocabularies are used.\n",
            "Cosine Similarity : 0.4367649555206299\n",
            "\n",
            "\n",
            "Document: Training complexity is then\n",
            " \tQ = N × D + D × log2(V ).\n",
            "Cosine Similarity : 0.4366205334663391\n",
            "\n",
            "\n",
            "Document: We follow the approach described above: the relationship is defined by subtracting two word vectors, and the result is added to another word.\n",
            "Cosine Similarity : 0.43617039918899536\n",
            "\n",
            "\n",
            "Document: The word vectors are then used to train the NNLM.\n",
            "Cosine Similarity : 0.4331796169281006\n",
            "\n",
            "\n",
            "Document: PhD thesis, Brno University of Technology, 2012.\n",
            "Cosine Similarity : 0.43312564492225647\n",
            "\n",
            "\n",
            "Document: When the word vectors are well trained, it is possible to find the correct answer (word smallest) using this method.\n",
            "Cosine Similarity : 0.4321632385253906\n",
            "\n",
            "\n",
            "Document: Computer Speech and Language, vol.\n",
            "Cosine Similarity : 0.42944270372390747\n",
            "\n",
            "\n",
            "Document: Next, we evaluated our models trained using one CPU only and compared the results against publicly available word vectors.\n",
            "Cosine Similarity : 0.42872241139411926\n",
            "\n",
            "\n",
            "Document: For example, neural network based language models significantly outperform N-gram models [1, 27, 17].\n",
            "Cosine Similarity : 0.42701661586761475\n",
            "\n",
            "\n",
            "Document: That is several orders of magnitude larger than the best previously published results for similar models.\n",
            "Cosine Similarity : 0.42377862334251404\n",
            "\n",
            "\n",
            "Document: Cognitive Science, 14, 179-211, 1990.\n",
            "Cosine Similarity : 0.4235783815383911\n",
            "\n",
            "\n",
            "Document: We observe large improvements in accuracy at much lower computational cost, i.e.\n",
            "Cosine Similarity : 0.42146310210227966\n",
            "\n",
            "\n",
            "Document: Common choice is E = 3−50 and T up to one billion.\n",
            "Cosine Similarity : 0.4197723865509033\n",
            "\n",
            "\n",
            "Document: The input layer is then projected to a projection layer P that has dimensionality N × D, using a shared projection matrix.\n",
            "Cosine Similarity : 0.4186450242996216\n",
            "\n",
            "\n",
            "Document: Another way to improve accuracy is to provide more than one example of the relationship.\n",
            "Cosine Similarity : 0.4175006151199341\n",
            "\n",
            "\n",
            "Document: R words from the future of the current word as correct labels.\n",
            "Cosine Similarity : 0.40891218185424805\n",
            "\n",
            "\n",
            "Document: It is also possible to apply the vector operations to solve different tasks.\n",
            "Cosine Similarity : 0.4071838855743408\n",
            "\n",
            "\n",
            "Document: Journal of Machine Learning Research, 2011.\n",
            "Cosine Similarity : 0.4065617322921753\n",
            "\n",
            "\n",
            "Document: While this is not crucial speedup for neural network LMs as the computational bottleneck is in the N×D×H term, we will later propose architectures that do not have hidden layers and thus depend heavily on the efficiency of the softmax normalization.\n",
            "Cosine Similarity : 0.40391165018081665\n",
            "\n",
            "\n",
            "Document: it takes less than a day to learn high quality word vectors from a 1.6 billion words data set.\n",
            "Cosine Similarity : 0.40142229199409485\n",
            "\n",
            "\n",
            "Document: Thus for example, Paris - France + Italy = Rome.\n",
            "Cosine Similarity : 0.3993549644947052\n",
            "\n",
            "\n",
            "Document: Continuous space language models.\n",
            "Cosine Similarity : 0.39715075492858887\n",
            "\n",
            "\n",
            "Document: This allows the recurrent model to form some kind of short term memory, as information from the past can be represented by the hidden layer state that gets updated based on the current input and the state of the hidden layer in the previous time step.\n",
            "Cosine Similarity : 0.39628058671951294\n",
            "\n",
            "\n",
            "Document: 5\tExamples of the Learned Relationships\n",
            " Table 8 shows words that follow various relationships.\n",
            "Cosine Similarity : 0.3947139084339142\n",
            "\n",
            "\n",
            "Document: For a common choice of N = 10, the size of the projection layer (P) might be 500 to 2000, while the hidden layer size H is typically 500 to 1000 units.\n",
            "Cosine Similarity : 0.3891286253929138\n",
            "\n",
            "\n",
            "Document: Automatic Speech Recognition and Understanding, 2011.\n",
            "Cosine Similarity : 0.38862496614456177\n",
            "\n",
            "\n",
            "Document: However, several practical solutions were proposed for avoiding it; either using hierarchical versions of the softmax [25, 23, 18], or avoiding normalized models completely by using models that are not normalized during training [4, 9].\n",
            "Cosine Similarity : 0.38749292492866516\n",
            "\n",
            "\n",
            "Document: The main observation from the previous section was that most of the complexity is caused by the non-linear hidden layer in the model.\n",
            "Cosine Similarity : 0.3853116035461426\n",
            "\n",
            "\n",
            "Document: Example of another type of relationship can be word pairs big - biggest and small - smallest [20].\n",
            "Cosine Similarity : 0.38510021567344666\n",
            "\n",
            "\n",
            "Document: Note that related models have been proposed also much earlier [26, 8].\n",
            "Cosine Similarity : 0.37991487979888916\n",
            "\n",
            "\n",
            "Document: In: Proceedings of the 6th International Workshop on Semantic Evaluation (SemEval 2012), 2012.\n",
            "Cosine Similarity : 0.37957584857940674\n",
            "\n",
            "\n",
            "Document: [26]\tD. E. Rumelhart, G. E. Hinton, R. J. Williams.\n",
            "Cosine Similarity : 0.37798380851745605\n",
            "\n",
            "\n",
            "Document: Volume 1: Foundations, MIT Press, 1986.\n",
            "Cosine Similarity : 0.3776105046272278\n",
            "\n",
            "\n",
            "Document: [3]\tT. Brants, A. C. Popat, P. Xu, F. J. Och, and J.\n",
            "Cosine Similarity : 0.3718760311603546\n",
            "\n",
            "\n",
            "Document: The final sentence score is then the sum of these individual predictions.\n",
            "Cosine Similarity : 0.3711232542991638\n",
            "\n",
            "\n",
            "Document: We used these data to provide a comparison to a previously trained recurrent neural network language model that took about 8 weeks to train on a single CPU.\n",
            "Cosine Similarity : 0.36577552556991577\n",
            "\n",
            "\n",
            "Document: [9]\tEric H. Huang, R. Socher, C. D. Manning and Andrew Y. Ng.\n",
            "Cosine Similarity : 0.3651368021965027\n",
            "\n",
            "\n",
            "Document: Thus, there are situations where simple scaling up of the basic techniques will not result in any significant progress, and we have to focus on more advanced techniques.\n",
            "Cosine Similarity : 0.3651042580604553\n",
            "\n",
            "\n",
            "Document: With binary tree representations of the vocabulary, the number of output units that need to be evaluated can go down to around log2(V ).\n",
            "Cosine Similarity : 0.3649998903274536\n",
            "\n",
            "\n",
            "Document: It can be seen that after some point, adding more dimensions or adding more training data provides diminishing improvements.\n",
            "Cosine Similarity : 0.36162269115448\n",
            "\n",
            "\n",
            "Document: In machine translation, the existing corpora for many languages contain only a few billions of words or less.\n",
            "Cosine Similarity : 0.3608698546886444\n",
            "\n",
            "\n",
            "Document: Natural Language Processing (Almost) from Scratch.\n",
            "Cosine Similarity : 0.36084744334220886\n",
            "\n",
            "\n",
            "Document: [32]\tG. Zweig, C.J.C.\n",
            "Cosine Similarity : 0.35663819313049316\n",
            "\n",
            "\n",
            "Document: We have restricted the vocabulary size to 1 million most frequent words.\n",
            "Cosine Similarity : 0.3559234142303467\n",
            "\n",
            "\n",
            "Document: Learning internal representations by backpropagating errors.\n",
            "Cosine Similarity : 0.3539748787879944\n",
            "\n",
            "\n",
            "Document: Word Representations: A Simple and General Method for Semi-Supervised Learning.\n",
            "Cosine Similarity : 0.3535982370376587\n",
            "\n",
            "\n",
            "Document: [18]\tT. Mikolov, A. Deoras, D. Povey, L. Burget, J. Cernockˇ y.\n",
            "Cosine Similarity : 0.3532436490058899\n",
            "\n",
            "\n",
            "Document: In the following experiments, we use C = 10.\n",
            "Cosine Similarity : 0.3503314256668091\n",
            "\n",
            "\n",
            "Document: [5]\tR. Collobert, J. Weston, L. Bottou, M. Karlen, K. Kavukcuoglu and P. Kuksa.\n",
            "Cosine Similarity : 0.33332133293151855\n",
            "\n",
            "\n",
            "Document: We call this architecture a bag-of-words model as the order of words in the history does not influence the projection.\n",
            "Cosine Similarity : 0.33325445652008057\n",
            "\n",
            "\n",
            "Document: References\n",
            " [1]\tY. Bengio, R. Ducharme, P. Vincent.\n",
            "Cosine Similarity : 0.33265262842178345\n",
            "\n",
            "\n",
            "Document: We used 50 to 100 model replicas during the training.\n",
            "Cosine Similarity : 0.3310413360595703\n",
            "\n",
            "\n",
            "Document: Yih, C. Meek, G. Zweig, T. Mikolov.\n",
            "Cosine Similarity : 0.32976794242858887\n",
            "\n",
            "\n",
            "Document: The model architecture is shown at Figure 1.\n",
            "Cosine Similarity : 0.3296181559562683\n",
            "\n",
            "\n",
            "Document: Thus, the word vectors are learned even without constructing the full NNLM.\n",
            "Cosine Similarity : 0.3245707154273987\n",
            "\n",
            "\n",
            "Document: [25]\tF. Morin, Y. Bengio.\n",
            "Cosine Similarity : 0.32140934467315674\n",
            "\n",
            "\n",
            "Document: Distributed Representations of Words and Phrases and their Compositionality.\n",
            "Cosine Similarity : 0.3212302029132843\n",
            "\n",
            "\n",
            "Document: 1\tIntroduction\n",
            " Many current NLP systems and techniques treat words as atomic units - there is no notion of similarity between words, as these are represented as indices in a vocabulary.\n",
            "Cosine Similarity : 0.31622886657714844\n",
            "\n",
            "\n",
            "Document: Holyoak.\n",
            "Cosine Similarity : 0.30829107761383057\n",
            "\n",
            "\n",
            "Document: [28]\tR. Socher, E.H. Huang, J. Pennington, A.Y.\n",
            "Cosine Similarity : 0.30805060267448425\n",
            "\n",
            "\n",
            "Document: Yih, G. Zweig.\n",
            "Cosine Similarity : 0.3072924315929413\n",
            "\n",
            "\n",
            "Document: Pham, D. Huang, A.Y.\n",
            "Cosine Similarity : 0.30703598260879517\n",
            "\n",
            "\n",
            "Document: As far as we know, none of the previously proposed architectures has been successfully trained on more than a few hundred of millions of words, with a modest dimensionality of the word vectors between 50 - 100.\n",
            "Cosine Similarity : 0.3012414574623108\n",
            "\n",
            "\n",
            "Document: unrestricted to the 30k vocabulary.\n",
            "Cosine Similarity : 0.30118224024772644\n",
            "\n",
            "\n",
            "Document: [21]\tT. Mikolov, I. Sutskever, K. Chen, G. Corrado, and J.\n",
            "Cosine Similarity : 0.2983017563819885\n",
            "\n",
            "\n",
            "Document: [4]\tR. Collobert and J. Weston.\n",
            "Cosine Similarity : 0.29814112186431885\n",
            "\n",
            "\n",
            "Document: Two examples from each category are shown in Table 1.\n",
            "Cosine Similarity : 0.2975305914878845\n",
            "\n",
            "\n",
            "Document: Corrado, R. Monga, K. Chen, M. Devin, Q.V.\n",
            "Cosine Similarity : 0.29692697525024414\n",
            "\n",
            "\n",
            "Document: ICML, 2007.\n",
            "Cosine Similarity : 0.2896750569343567\n",
            "\n",
            "\n",
            "Document: [29]\tJ. Turian, L. Ratinov, Y. Bengio.\n",
            "Cosine Similarity : 0.287531316280365\n",
            "\n",
            "\n",
            "Document: [24]\tA. Mnih, Y.W.\n",
            "Cosine Similarity : 0.28655368089675903\n",
            "\n",
            "\n",
            "Document: [7]\tJ.C. Duchi, E. Hazan, and Y.\n",
            "Cosine Similarity : 0.2836742401123047\n",
            "\n",
            "\n",
            "Document: [23]\tA. Mnih, G. Hinton.\n",
            "Cosine Similarity : 0.28265294432640076\n",
            "\n",
            "\n",
            "Document: [31]\tA. Zhila, W.T.\n",
            "Cosine Similarity : 0.28130874037742615\n",
            "\n",
            "\n",
            "Document: Dean, G.S.\n",
            "Cosine Similarity : 0.2800489664077759\n",
            "\n",
            "\n",
            "Document: This also means that reaching 100% accuracy is likely to be impossible, as the current models do not have any input information about word morphology.\n",
            "Cosine Similarity : 0.28001782298088074\n",
            "\n",
            "\n",
            "Document: The result are reported in Table 6.\n",
            "Cosine Similarity : 0.2795773148536682\n",
            "\n",
            "\n",
            "Document: This work has been followed by many others.\n",
            "Cosine Similarity : 0.2753438353538513\n",
            "\n",
            "\n",
            "Document: Nature, 323:533.536, 1986.\n",
            "Cosine Similarity : 0.27531611919403076\n",
            "\n",
            "\n",
            "Document: [2]\tY. Bengio, Y. LeCun.\n",
            "Cosine Similarity : 0.27484989166259766\n",
            "\n",
            "\n",
            "Document: Some of our follow-up work will be published in an upcoming NIPS 2013 paper [21].\n",
            "Cosine Similarity : 0.27317357063293457\n",
            "\n",
            "\n",
            "Document: It consists of input, projection, hidden and output layers.\n",
            "Cosine Similarity : 0.2729419469833374\n",
            "\n",
            "\n",
            "Document: AISTATS, 2005.\n",
            "Cosine Similarity : 0.2726563513278961\n",
            "\n",
            "\n",
            "Document: The comparison is given in Table 4.\n",
            "Cosine Similarity : 0.27098265290260315\n",
            "\n",
            "\n",
            "Document: We have included in our test set only single token words, thus multi-word entities are not present (such as New York).\n",
            "Cosine Similarity : 0.2703903913497925\n",
            "\n",
            "\n",
            "Document: A short summary of some previous results together with the new results is presented in Table 7.\n",
            "Cosine Similarity : 0.2702811062335968\n",
            "\n",
            "\n",
            "Document: This corpus contains about 6B tokens.\n",
            "Cosine Similarity : 0.2694113254547119\n",
            "\n",
            "\n",
            "Document: Only questions containing words from the most frequent 30k words are used.\n",
            "Cosine Similarity : 0.2660485506057739\n",
            "\n",
            "\n",
            "Document: Senior, P. Tucker, K. Yang, A. Y.\n",
            "Cosine Similarity : 0.26604583859443665\n",
            "\n",
            "\n",
            "Document: Turney, K.J.\n",
            "Cosine Similarity : 0.26525044441223145\n",
            "\n",
            "\n",
            "Document: [22]\tA. Mnih, G. Hinton.\n",
            "Cosine Similarity : 0.26423296332359314\n",
            "\n",
            "\n",
            "Document: Since the more distant words are usually less related to the current word than those close to it, we give less weight to the distant words by sampling less from those words in our training examples.\n",
            "Cosine Similarity : 0.2629006505012512\n",
            "\n",
            "\n",
            "Document: NAACL HLT 2013.\n",
            "Cosine Similarity : 0.26008182764053345\n",
            "\n",
            "\n",
            "Document: NAACL HLT 2013.\n",
            "Cosine Similarity : 0.26008182764053345\n",
            "\n",
            "\n",
            "Document: [27]\tH. Schwenk.\n",
            "Cosine Similarity : 0.25883761048316956\n",
            "\n",
            "\n",
            "Document: Rumelhart.\n",
            "Cosine Similarity : 0.25619184970855713\n",
            "\n",
            "\n",
            "Document: For experiments reported further, we used just one training epoch (again, we decrease the learning rate linearly so that it approaches zero at the end of training).\n",
            "Cosine Similarity : 0.2551851272583008\n",
            "\n",
            "\n",
            "Document: Hinton, J.L.\n",
            "Cosine Similarity : 0.2547524571418762\n",
            "\n",
            "\n",
            "Document: Le, M.Z.\n",
            "Cosine Similarity : 0.2539858818054199\n",
            "\n",
            "\n",
            "Document: As only N inputs are active at any given time, composition of the projection layer is a relatively cheap operation.\n",
            "Cosine Similarity : 0.24974650144577026\n",
            "\n",
            "\n",
            "Document: Finding Structure in Time.\n",
            "Cosine Similarity : 0.24859759211540222\n",
            "\n",
            "\n",
            "Document: Ng, and C. Potts.\n",
            "Cosine Similarity : 0.2437811642885208\n",
            "\n",
            "\n",
            "Document: France is to Paris as Germany is to Berlin.\n",
            "Cosine Similarity : 0.243088960647583\n",
            "\n",
            "\n",
            "Document: Daly, P.T.\n",
            "Cosine Similarity : 0.24177798628807068\n",
            "\n",
            "\n",
            "Document: McClelland, D.E.\n",
            "Cosine Similarity : 0.24096015095710754\n",
            "\n",
            "\n",
            "Document: Ranzato, A.\n",
            "Cosine Similarity : 0.23901256918907166\n",
            "\n",
            "\n",
            "Document: ICASSP 2009.\n",
            "Cosine Similarity : 0.23542740941047668\n",
            "\n",
            "\n",
            "Document: [20]\tT. Mikolov, W.T.\n",
            "Cosine Similarity : 0.232744038105011\n",
            "\n",
            "\n",
            "Document: In: Proc.\n",
            "Cosine Similarity : 0.23025304079055786\n",
            "\n",
            "\n",
            "Document: In: Proc.\n",
            "Cosine Similarity : 0.23025304079055786\n",
            "\n",
            "\n",
            "Document: In: Proc.\n",
            "Cosine Similarity : 0.23025304079055786\n",
            "\n",
            "\n",
            "Document: [10]\tG.E.\n",
            "Cosine Similarity : 0.22976389527320862\n",
            "\n",
            "\n",
            "Document: [13]\tT. Mikolov.\n",
            "Cosine Similarity : 0.2293711155653\n",
            "\n",
            "\n",
            "Document: [30]\tP. D. Turney.\n",
            "Cosine Similarity : 0.22378301620483398\n",
            "\n",
            "\n",
            "Document: Accepted to NIPS 2013.\n",
            "Cosine Similarity : 0.22068685293197632\n",
            "\n",
            "\n",
            "Document: Mao, M.A.\n",
            "Cosine Similarity : 0.22047081589698792\n",
            "\n",
            "\n",
            "Document: ICML, 2012.\n",
            "Cosine Similarity : 0.22031234204769135\n",
            "\n",
            "\n",
            "Document: Dean.\n",
            "Cosine Similarity : 0.22012367844581604\n",
            "\n",
            "\n",
            "Document: Dean.\n",
            "Cosine Similarity : 0.22012367844581604\n",
            "\n",
            "\n",
            "Document: In Proceedings of ACL, 2011.\n",
            "Cosine Similarity : 0.2164452224969864\n",
            "\n",
            "\n",
            "Document: Maas, R.E.\n",
            "Cosine Similarity : 0.21598955988883972\n",
            "\n",
            "\n",
            "Document: [19]\tT. Mikolov.\n",
            "Cosine Similarity : 0.2159072309732437\n",
            "\n",
            "\n",
            "Document: We chose starting learning rate 0.025 and decreased it linearly, so that it approaches zero at the end of the last training epoch.\n",
            "Cosine Similarity : 0.2142694890499115\n",
            "\n",
            "\n",
            "Document: Ng, and C.D.\n",
            "Cosine Similarity : 0.20833362638950348\n",
            "\n",
            "\n",
            "Document: [6]\tJ.\n",
            "Cosine Similarity : 0.19779923558235168\n",
            "\n",
            "\n",
            "Document: 21, 2007.\n",
            "Cosine Similarity : 0.1954953670501709\n",
            "\n",
            "\n",
            "Document: However, the simple techniques are at their limits in many tasks.\n",
            "Cosine Similarity : 0.1944892853498459\n",
            "\n",
            "\n",
            "Document: Burges.\n",
            "Cosine Similarity : 0.19377149641513824\n",
            "\n",
            "\n",
            "Document: Mohammad, P.D.\n",
            "Cosine Similarity : 0.18916283547878265\n",
            "\n",
            "\n",
            "Document: Manning.\n",
            "Cosine Similarity : 0.17994138598442078\n",
            "\n",
            "\n",
            "Document: [8]\tJ. Elman.\n",
            "Cosine Similarity : 0.17750635743141174\n",
            "\n",
            "\n",
            "Document: In NIPS, 2011.\n",
            "Cosine Similarity : 0.17371466755867004\n",
            "\n",
            "\n",
            "Document: Teh.\n",
            "Cosine Similarity : 0.17336535453796387\n",
            "\n",
            "\n",
            "Document: Singer.\n",
            "Cosine Similarity : 0.16149672865867615\n",
            "\n",
            "\n",
            "Document: [12]\tA.L.\n",
            "Cosine Similarity : 0.15174376964569092\n",
            "\n",
            "\n",
            "Document: Jurgens, S.M.\n",
            "Cosine Similarity : 0.1492825597524643\n",
            "\n",
            "\n",
            "Document: Distributed representations.\n",
            "Cosine Similarity : 0.13562795519828796\n",
            "\n",
            "\n",
            "Document: [11]\tD.A.\n",
            "Cosine Similarity : 0.12523597478866577\n",
            "\n",
            "\n",
            "Document: The RNN model does not have a projection layer; only input, hidden and output layer.\n",
            "Cosine Similarity : 0.10632213950157166\n",
            "\n",
            "\n",
            "Document: 4.2\tMaximization of Accuracy\n",
            " We have used a Google News corpus for training the word vectors.\n",
            "Euclidean Distance : 10.444738388061523\n",
            "\n",
            "\n",
            "Document: Finally, we found that when we train high dimensional word vectors on a large amount of data, the resulting vectors can be used to answer very subtle semantic relationships between words, such as a city and the country it belongs to, e.g.\n",
            "Euclidean Distance : 10.6636381149292\n",
            "\n",
            "\n",
            "Document: 1.1\tGoals of the Paper\n",
            " The main goal of this paper is to introduce techniques that can be used for learning high-quality word vectors from huge data sets with billions of words, and with millions of words in the vocabulary.\n",
            "Euclidean Distance : 11.091798782348633\n",
            "\n",
            "\n",
            "Document: Model\tVector\tTraining\tAccuracy [%]\t\tTraining time\n",
            " \tDimensionality\twords\t\t\t[days x CPU cores]\n",
            " \t\t\tSemantic\tSyntactic\tTotal\t\n",
            " NNLM\t100\t6B\t34.2\t64.5\t50.8\t14 x 180\n",
            " CBOW\t1000\t6B\t57.3\t68.9\t63.7\t2 x 140\n",
            " Skip-gram\t1000\t6B\t66.1\t65.1\t65.6\t2.5 x 125\n",
            " Table 7: Comparison and combination of models on the Microsoft Sentence Completion Challenge.\n",
            "Euclidean Distance : 11.318404197692871\n",
            "\n",
            "\n",
            "Document: We design a new comprehensive test set for measuring both syntactic and semantic regularities , and show that many such regularities can be learned with high accuracy.\n",
            "Euclidean Distance : 11.696615219116211\n",
            "\n",
            "\n",
            "Document: Below we report the results of several models trained on the Google News 6B data set, with mini-batch asynchronous gradient descent and the adaptive learning rate procedure called Adagrad [7].\n",
            "Euclidean Distance : 11.79327392578125\n",
            "\n",
            "\n",
            "Document: Dimensionality / Training words\t24M\t49M\t98M\t196M\t391M\t783M\n",
            " 50\t13.4\t15.7\t18.6\t19.1\t22.5\t23.2\n",
            " 100\t19.4\t23.1\t27.8\t28.7\t33.4\t32.2\n",
            " 300\t23.2\t29.2\t35.3\t38.6\t43.7\t45.9\n",
            " 600\t24.0\t30.1\t36.5\t40.8\t46.6\t50.4\n",
            " Table 3: Comparison of architectures using models trained on the same data, with 640-dimensional word vectors.\n",
            "Euclidean Distance : 11.929051399230957\n",
            "\n",
            "\n",
            "Document: Model\tVector\tTraining\tAccuracy [%]\t\n",
            " \tDimensionality\twords\t\t\n",
            " \t\t\tSemantic\tSyntactic\tTotal\n",
            " Collobert-Weston NNLM\t50\t660M\t9.3\t12.3\t11.0\n",
            " Turian NNLM\t50\t37M\t1.4\t2.6\t2.1\n",
            " Turian NNLM\t200\t37M\t1.4\t2.2\t1.8\n",
            " Mnih NNLM\t50\t37M\t1.8\t9.1\t5.8\n",
            " Mnih NNLM\t100\t37M\t3.3\t13.2\t8.8\n",
            " Mikolov RNNLM\t80\t320M\t4.9\t18.4\t12.7\n",
            " Mikolov RNNLM\t640\t320M\t8.6\t36.5\t24.6\n",
            " Huang NNLM\t50\t990M\t13.3\t11.6\t12.3\n",
            " Our NNLM\t20\t6B\t12.9\t26.4\t20.3\n",
            " Our NNLM\t50\t6B\t27.9\t55.8\t43.2\n",
            " Our NNLM\t100\t6B\t34.2\t64.5\t50.8\n",
            " CBOW\t300\t783M\t15.5\t53.1\t36.1\n",
            " Skip-gram\t300\t783M\t50.0\t55.9\t53.3\n",
            " Table 5: Comparison of models trained for three epochs on the same data and models trained for one epoch.\n",
            "Euclidean Distance : 12.017752647399902\n",
            "\n",
            "\n",
            "Document: As it can be seen, accuracy is quite good, although there is clearly a lot of room for further improvements (note that using our accuracy metric that Table 8: Examples of the word pair relationships, using the best word vectors from Table 4 (Skipgram model trained on 783M words with 300 dimensionality).\n",
            "Euclidean Distance : 12.04330825805664\n",
            "\n",
            "\n",
            "Document: [14]\tT. Mikolov, J. Kopecky, L. Burget, O. Glembek and J.´ Cernockˇ y. Neural network based lan-´ guage models for higly inflective languages, In: Proc.\n",
            "Euclidean Distance : 12.059144973754883\n",
            "\n",
            "\n",
            "Document: A very popular model architecture for estimating neural network language model (NNLM) was proposed in [1], where a feedforward neural network with a linear projection layer and a non-linear hidden layer was used to learn jointly the word vector representation and a statistical language model.\n",
            "Euclidean Distance : 12.185312271118164\n",
            "\n",
            "\n",
            "Document: An example is the popular N-gram model used for statistical language modeling - today, it is possible to train N-grams on virtually all available data (trillions of words [3]).\n",
            "Euclidean Distance : 12.18710994720459\n",
            "\n",
            "\n",
            "Document: Using the DistBelief distributed framework, it should be possible to train the CBOW and Skip-gram models even on corpora with one trillion words, for basically unlimited size of the vocabulary.\n",
            "Euclidean Distance : 12.187378883361816\n",
            "\n",
            "\n",
            "Document: 4.5\tMicrosoft Research Sentence Completion Challenge\n",
            " The Microsoft Sentence Completion Challenge has been recently introduced as a task for advancing language modeling and other NLP techniques [32].\n",
            "Euclidean Distance : 12.329833984375\n",
            "\n",
            "\n",
            "Document: Most of the complexity then comes from H × H.\n",
            " 2.3\tParallel Training of Neural Networks\n",
            " To train models on huge data sets, we have implemented several models on top of a large-scale distributed framework called DistBelief [6], including the feedforward NNLM and the new models proposed in this paper.\n",
            "Euclidean Distance : 12.374923706054688\n",
            "\n",
            "\n",
            "Document: 2.2\tRecurrent Neural Net Language Model (RNNLM)\n",
            " Recurrent neural network based language model has been proposed to overcome certain limitations of the feedforward NNLM, such as the need to specify the context length (the order of the model N), and because theoretically RNNs can efficiently represent more complex patterns than the shallow neural networks [15, 2].\n",
            "Euclidean Distance : 12.42641830444336\n",
            "\n",
            "\n",
            "Document: To find a word that is similar to small in the same sense as biggest is similar to big, we can simply compute vector X = vector(”biggest”)−vector(”big”)+ vector(”small”).\n",
            "Euclidean Distance : 12.466090202331543\n",
            "\n",
            "\n",
            "Document: The complexity per training example of the RNN model is\n",
            " \tQ = H × H + H × V,\t(3)\n",
            " where the word representations D have the same dimensionality as the hidden layer H. Again, the term H × V can be efficiently reduced to H × log2(V ) by using hierarchical softmax.\n",
            "Euclidean Distance : 12.709166526794434\n",
            "\n",
            "\n",
            "Document: The new architectures directly follow those proposed in our earlier work [13, 14], where it was found that neural network language model can be successfully trained in two steps: first, continuous word vectors are learned using simple model, and then the N-gram NNLM is trained on top of these distributed representations of words.\n",
            "Euclidean Distance : 12.72337818145752\n",
            "\n",
            "\n",
            "Document: We further denote two pairs of words with the same relationship as a question, as we can ask: ”What is the word that is similar to small in the same sense as biggest is similar to big?”\n",
            " Somewhat surprisingly, these questions can be answered by performing simple algebraic operations with the vector representation of words.\n",
            "Euclidean Distance : 12.762174606323242\n",
            "\n",
            "\n",
            "Document: This follows previous observations that the frequency of words works well for obtaining classes in neural net language models [16].\n",
            "Euclidean Distance : 12.832956314086914\n",
            "\n",
            "\n",
            "Document: 3\tNew Log-linear Models\n",
            " In this section, we propose two new model architectures for learning distributed representations of words that try to minimize computational complexity.\n",
            "Euclidean Distance : 12.90353012084961\n",
            "\n",
            "\n",
            "Document: Architecture\tAccuracy [%]\n",
            " 4-gram [32]\t39\n",
            " Average LSA similarity [32]\t49\n",
            " Log-bilinear model [24]\t54.8\n",
            " RNNLMs [19]\t55.4\n",
            " Skip-gram\t48.0\n",
            " Skip-gram + RNNLMs\t58.9\n",
            " estimate since the data center machines are shared with other production tasks, and the usage can fluctuate quite a bit.\n",
            "Euclidean Distance : 12.922435760498047\n",
            "\n",
            "\n",
            "Document: 4\tResults\n",
            " To compare the quality of different versions of word vectors, previous papers typically use a table showing example words and their most similar words, and understand them intuitively.\n",
            "Euclidean Distance : 12.958083152770996\n",
            "\n",
            "\n",
            "Document: In the further experiments, we use full set of questions in the new Semantic-Syntactic Word Relationship test set, i.e.\n",
            "Euclidean Distance : 12.96212387084961\n",
            "\n",
            "\n",
            "Document: Using a word offset technique where simple algebraic operations are performed on the word vectors, it was shown for example that vector(”King”) - vector(”Man”) + vector(”Woman”) results in a vector that is closest to the vector representation of the word Queen [20].\n",
            "Euclidean Distance : 13.032560348510742\n",
            "\n",
            "\n",
            "Document: In this paper, we try to maximize accuracy of these vector operations by developing new model architectures that preserve the linear regularities among words.\n",
            "Euclidean Distance : 13.09510612487793\n",
            "\n",
            "\n",
            "Document: Model\tVector\tTraining\tAccuracy [%]\t\tTraining time\n",
            " \tDimensionality\twords\t\t\t[days]\n",
            " \t\t\tSemantic\tSyntactic\tTotal\t\n",
            " 3 epoch CBOW\t300\t783M\t15.5\t53.1\t36.1\t1\n",
            " 3 epoch Skip-gram\t300\t783M\t50.0\t55.9\t53.3\t3\n",
            " 1 epoch CBOW\t300\t783M\t13.8\t49.9\t33.6\t0.3\n",
            " 1 epoch CBOW\t300\t1.6B\t16.1\t52.6\t36.1\t0.6\n",
            " 1 epoch CBOW\t600\t783M\t15.4\t53.3\t36.2\t0.7\n",
            " 1 epoch Skip-gram\t300\t783M\t45.6\t52.2\t49.2\t1\n",
            " 1 epoch Skip-gram\t300\t1.6B\t52.2\t55.1\t53.8\t2\n",
            " 1 epoch Skip-gram\t600\t783M\t56.7\t54.5\t55.5\t2.5\n",
            " of the Google News data in about a day, while training time for the Skip-gram model was about three days.\n",
            "Euclidean Distance : 13.098112106323242\n",
            "\n",
            "\n",
            "Document: Furthermore, we show that these vectors provide state-of-the-art performance on our test set for measuring syntactic and semantic word similarities.\n",
            "Euclidean Distance : 13.22626781463623\n",
            "\n",
            "\n",
            "Document: Relationship\tExample 1\tExample 2\tExample 3\n",
            " France - Paris\tItaly: Rome\tJapan: Tokyo\tFlorida: Tallahassee\n",
            " big - bigger\tsmall: larger\tcold: colder\tquick: quicker\n",
            " Miami - Florida\tBaltimore: Maryland\tDallas: Texas\tKona: Hawaii\n",
            " Einstein - scientist\tMessi: midfielder\tMozart: violinist\tPicasso: painter\n",
            " Sarkozy - France\tBerlusconi: Italy\tMerkel: Germany\tKoizumi: Japan\n",
            " copper - Cu\tzinc: Zn\tgold: Au\turanium: plutonium\n",
            " Berlusconi - Silvio\tSarkozy: Nicolas\tPutin: Medvedev\tObama: Barack\n",
            " Microsoft - Windows\tGoogle: Android\tIBM: Linux\tApple: iPhone\n",
            " Microsoft - Ballmer\tGoogle: Yahoo\tIBM: McNealy\tApple: Jobs\n",
            " Japan - sushi\tGermany: bratwurst\tFrance: tapas\tUSA: pizza\n",
            " assumes exact match, the results in Table 8 would score only about 60%).\n",
            "Euclidean Distance : 13.261391639709473\n",
            "\n",
            "\n",
            "Document: Large language models in machine translation.\n",
            "Euclidean Distance : 13.308095932006836\n",
            "\n",
            "\n",
            "Document: The accuracies are reported on our Semantic-Syntactic Word Relationship test set, and on the syntactic relationship test set of [20]\n",
            " Model\n",
            " Architecture\tSemantic-Syntactic Word Relationship test set\tMSR Word Relatedness\n",
            " Test Set [20]\n",
            " \tSemantic Accuracy [%]\tSyntactic Accuracy [%]\t\n",
            " RNNLM\t9\t36\t35\n",
            " NNLM\t23\t53\t47\n",
            " CBOW\t24\t64\t61\n",
            " Skip-gram\t55\t59\t56\n",
            " (such as 50 - 100).\n",
            "Euclidean Distance : 13.318432807922363\n",
            "\n",
            "\n",
            "Document: A Unified Architecture for Natural Language Processing: Deep Neural Networks with Multitask Learning.\n",
            "Euclidean Distance : 13.362717628479004\n",
            "\n",
            "\n",
            "Document: We believe that our comprehensive test set will help the research community to improve the existing techniques for estimating the word vectors.\n",
            "Euclidean Distance : 13.417351722717285\n",
            "\n",
            "\n",
            "Document: While this observation might seem trivial, it must be noted that it is currently popular to train word vectors on relatively large amounts of data, but with insufficient size Table 2: Accuracy on subset of the Semantic-Syntactic Word Relationship test set, using word vectors from the CBOW architecture with limited vocabulary.\n",
            "Euclidean Distance : 13.42747688293457\n",
            "\n",
            "\n",
            "Document: For example, we have observed good accuracy for selecting out-of-the-list words, by computing average vector for a list of words, and finding the most distant word vector.\n",
            "Euclidean Distance : 13.44601058959961\n",
            "\n",
            "\n",
            "Document: Furthermore, we also use words from the future; we have obtained the best performance on the task introduced in the next section by building a log-linear classifier with four future and four history words at the input, where the training criterion is to correctly classify the current (middle) word.\n",
            "Euclidean Distance : 13.460872650146484\n",
            "\n",
            "\n",
            "Document: Strategies for Training Large Scale´ Neural Network Language Models, In: Proc.\n",
            "Euclidean Distance : 13.47437572479248\n",
            "\n",
            "\n",
            "Document: The CBOW model was trained on subset Table 4: Comparison of publicly available word vectors on the Semantic-Syntactic Word Relationship test set, and word vectors from our models.\n",
            "Euclidean Distance : 13.514318466186523\n",
            "\n",
            "\n",
            "Document: We believe that word vectors trained on even larger data sets with larger dimensionality will perform significantly better, and will enable the development of new innovative applications.\n",
            "Euclidean Distance : 13.515299797058105\n",
            "\n",
            "\n",
            "Document: The Microsoft Research Sentence Completion Challenge, Microsoft Research Technical Report MSR-TR-2011-129, 2011.\n",
            "Euclidean Distance : 13.521716117858887\n",
            "\n",
            "\n",
            "Document: This has been observed earlier in the context of inflectional languages - for example, nouns can have multiple word endings, and if we search for similar words in a subspace of the original vector space, it is possible to find words that have similar endings [13, 14].\n",
            "Euclidean Distance : 13.565898895263672\n",
            "\n",
            "\n",
            "Document: 2\tModel Architectures\n",
            " Many different types of models were proposed for estimating continuous representations of words, including the well-known Latent Semantic Analysis (LSA) and Latent Dirichlet Allocation (LDA).\n",
            "Euclidean Distance : 13.580973625183105\n",
            "\n",
            "\n",
            "Document: [17]\tT. Mikolov, A. Deoras, S. Kombrink, L. Burget, J. Cernockˇ\ty. Empirical Evaluation and Com-´ bination of Advanced Language Modeling Techniques, In: Proceedings of Interspeech, 2011.\n",
            "Euclidean Distance : 13.584979057312012\n",
            "\n",
            "\n",
            "Document: In this paper, we focus on distributed representations of words learned by neural networks, as it was previously shown that they perform significantly better than LSA for preserving linear regularities among words [20, 31]; LDA moreover becomes computationally very expensive on large data sets.\n",
            "Euclidean Distance : 13.619778633117676\n",
            "\n",
            "\n",
            "Document: Then, we search in the vector space for the word closest to X measured by cosine distance, and use it as the answer to the question (we discard the input question words during this search).\n",
            "Euclidean Distance : 13.667179107666016\n",
            "\n",
            "\n",
            "Document: We observed that it is possible to train high quality word vectors using very simple model architectures, compared to the popular neural network models (both feedforward and recurrent).\n",
            "Euclidean Distance : 13.668889045715332\n",
            "\n",
            "\n",
            "Document: Clearly, we are facing time constrained optimization problem, as it can be expected that both using more data and higher dimensional word vectors will improve the accuracy.\n",
            "Euclidean Distance : 13.695496559143066\n",
            "\n",
            "\n",
            "Document: Our ongoing work shows that the word vectors can be successfully applied to automatic extension of facts in Knowledge Bases, and also for verification of correctness of existing facts.\n",
            "Euclidean Distance : 13.69642448425293\n",
            "\n",
            "\n",
            "Document: 6\tConclusion\n",
            " In this paper we studied the quality of vector representations of words derived by various models on a collection of syntactic and semantic language tasks.\n",
            "Euclidean Distance : 13.707531929016113\n",
            "\n",
            "\n",
            "Document: We found that increasing the range improves quality of the resulting word vectors, but it also increases the computational complexity.\n",
            "Euclidean Distance : 13.73445987701416\n",
            "\n",
            "\n",
            "Document: Thus, most of the complexity is caused by the term N × D × H.\n",
            " In our models, we use hierarchical softmax where the vocabulary is represented as a Huffman binary tree.\n",
            "Euclidean Distance : 13.753223419189453\n",
            "\n",
            "\n",
            "Document: In Table 3, it can be seen that the word vectors from the RNN (as used in [20]) perform well mostly on the syntactic questions.\n",
            "Euclidean Distance : 13.7719144821167\n",
            "\n",
            "\n",
            "Document: 4.3\tComparison of Model Architectures\n",
            " First we compare different model architectures for deriving the word vectors using the same training data and using the same dimensionality of 640 of the word vectors.\n",
            "Euclidean Distance : 13.796059608459473\n",
            "\n",
            "\n",
            "Document: For all the following models, the training complexity is proportional to\n",
            " \tO = E × T × Q,\t(1)\n",
            " where E is number of the training epochs, T is the number of the words in the training set and Q is defined further for each model architecture.\n",
            "Euclidean Distance : 13.811626434326172\n",
            "\n",
            "\n",
            "Document: It was later shown that the word vectors can be used to significantly improve and simplify many NLP applications [4, 5, 29].\n",
            "Euclidean Distance : 13.83976936340332\n",
            "\n",
            "\n",
            "Document: Word vectors with such semantic relationships could be used to improve many existing NLP applications, such as machine translation, information retrieval and question answering systems, and may enable other future applications yet to be invented.\n",
            "Euclidean Distance : 13.877030372619629\n",
            "\n",
            "\n",
            "Document: Language Modeling for Speech Recognition in Czech, Masters thesis, Brno University of Technology, 2007.\n",
            "Euclidean Distance : 13.926878929138184\n",
            "\n",
            "\n",
            "Document: Performance of several techniques has been already reported on this set, including N-gram models, LSA-based model [32], log-bilinear model [24] and a combination of recurrent neural networks that currently holds the state of the art performance of 55.4% accuracy on this benchmark [19].\n",
            "Euclidean Distance : 13.94106674194336\n",
            "\n",
            "\n",
            "Document: 4.4\tLarge Scale Parallel Training of Models\n",
            " As mentioned earlier, we have implemented various models in a distributed framework called DistBelief.\n",
            "Euclidean Distance : 13.941106796264648\n",
            "\n",
            "\n",
            "Document: Thus, if we choose C = 5, for each training word we will select randomly a number R in range < 1;C >, and then use R words from history and\n",
            " \t       INPUT         PROJECTION         OUTPUT\t          INPUT         PROJECTION      OUTPUT\n",
            " w(t-2)w(t-2)\n",
            " w(t-1)w(t-1)\n",
            " \tw(t)\tw(t)\n",
            " w(t+1)w(t+1)\n",
            " w(t+2)w(t+2)\n",
            "                    CBOW                                                   Skip-gram\n",
            " Figure 1: New model architectures.\n",
            "Euclidean Distance : 14.044635772705078\n",
            "\n",
            "\n",
            "Document: To estimate the best choice of model architecture for obtaining as good as possible results quickly, we have first evaluated models trained on subsets of the training data, with vocabulary restricted to the most frequent 30k words.\n",
            "Euclidean Distance : 14.054998397827148\n",
            "\n",
            "\n",
            "Document: Journal of Machine Learning Research, 3:1137-1155, 2003.\n",
            "Euclidean Distance : 14.116700172424316\n",
            "\n",
            "\n",
            "Document: Improving Word Representations via Global Context and Multiple Word Prototypes.\n",
            "Euclidean Distance : 14.13975715637207\n",
            "\n",
            "\n",
            "Document: We follow previous observation that there can be many different types of similarities between words, for example, word big is similar to bigger in the same sense that small is similar to smaller.\n",
            "Euclidean Distance : 14.139995574951172\n",
            "\n",
            "\n",
            "Document: Type of relationship\tWord Pair 1\tWord Pair 2\n",
            " Common capital city\tAthens\tGreece\tOslo\tNorway\n",
            " All capital cities\tAstana\tKazakhstan\tHarare\tZimbabwe\n",
            " Currency\tAngola\tkwanza\tIran\trial\n",
            " City-in-state\tChicago\tIllinois\tStockton\tCalifornia\n",
            " Man-Woman\tbrother\tsister\tgrandson\tgranddaughter\n",
            " Adjective to adverb\tapparent\tapparently\trapid\trapidly\n",
            " Opposite\tpossibly\timpossibly\tethical\tunethical\n",
            " Comparative\tgreat\tgreater\ttough\ttougher\n",
            " Superlative\teasy\teasiest\tlucky\tluckiest\n",
            " Present Participle\tthink\tthinking\tread\treading\n",
            " Nationality adjective\tSwitzerland\tSwiss\tCambodia\tCambodian\n",
            " Past tense\twalking\twalked\tswimming\tswam\n",
            " Plural nouns\tmouse\tmice\tdollar\tdollars\n",
            " Plural verbs\twork\tworks\tspeak\tspeaks\n",
            " 4.1\tTask Description\n",
            " To measure quality of the word vectors, we define a comprehensive test set that contains five types of semantic questions, and nine types of syntactic questions.\n",
            "Euclidean Distance : 14.14451789855957\n",
            "\n",
            "\n",
            "Document: 3.2\tContinuous Skip-gram Model\n",
            " The second architecture is similar to CBOW, but instead of predicting the current word based on the context, it tries to maximize classification of a word based on another word in the same sentence.\n",
            "Euclidean Distance : 14.160436630249023\n",
            "\n",
            "\n",
            "Document: The quality of these representations is measured in a word similarity task, and the results are compared to the previously best performing techniques based on different types of neural networks.\n",
            "Euclidean Distance : 14.170543670654297\n",
            "\n",
            "\n",
            "Document: This will require us to do R × 2 word classifications, with the current word as input, and each of the R + R words as output.\n",
            "Euclidean Distance : 14.181190490722656\n",
            "\n",
            "\n",
            "Document: In: Large-Scale Kernel Machines, MIT Press, 2007.\n",
            "Euclidean Distance : 14.190459251403809\n",
            "\n",
            "\n",
            "Document: Dynamic Pooling and Unfolding Recursive Autoencoders for Paraphrase Detection.\n",
            "Euclidean Distance : 14.211971282958984\n",
            "\n",
            "\n",
            "Document: More precisely, we use each current word as an input to a log-linear classifier with continuous projection layer, and predict words within a certain range before and after the current word.\n",
            "Euclidean Distance : 14.272055625915527\n",
            "\n",
            "\n",
            "Document: The neural network based word vectors were previously applied to many other NLP tasks, for example sentiment analysis [12] and paraphrase detection [28].\n",
            "Euclidean Distance : 14.319437026977539\n",
            "\n",
            "\n",
            "Document: We also expect that high quality word vectors will become an important building block for future NLP applications.\n",
            "Euclidean Distance : 14.347527503967285\n",
            "\n",
            "\n",
            "Document: We use recently proposed techniques for measuring the quality of the resulting vector representations, with the expectation that not only will similar words tend to be close to each other, but that words can have multiple degrees of similarity [20].\n",
            "Euclidean Distance : 14.348443984985352\n",
            "\n",
            "\n",
            "Document: 3.1\tContinuous Bag-of-Words Model\n",
            " The first proposed architecture is similar to the feedforward NNLM, where the non-linear hidden layer is removed and the projection layer is shared for all words (not just the projection matrix); thus, all words get projected into the same position (their vectors are averaged).\n",
            "Euclidean Distance : 14.377571105957031\n",
            "\n",
            "\n",
            "Document: 7\tFollow-Up Work\n",
            " After the initial version of this paper was written, we published single-machine multi-threaded C++ code for computing the word vectors, using both the continuous bag-of-words and skip-gram architectures .\n",
            "Euclidean Distance : 14.386446952819824\n",
            "\n",
            "\n",
            "Document: Given Equation 4, increasing amount of training data twice results in about the same increase of computational complexity as increasing vector size twice.\n",
            "Euclidean Distance : 14.405689239501953\n",
            "\n",
            "\n",
            "Document: (4)\n",
            " We denote this model further as CBOW, as unlike standard bag-of-words model, it uses continuous distributed representation of the context.\n",
            "Euclidean Distance : 14.44005012512207\n",
            "\n",
            "\n",
            "Document: This choice has several good reasons - simplicity, robustness and the observation that simple models trained on huge amounts of data outperform complex systems trained on less data.\n",
            "Euclidean Distance : 14.444040298461914\n",
            "\n",
            "\n",
            "Document: Adaptive subgradient methods for online learning and stochastic optimization.\n",
            "Euclidean Distance : 14.478663444519043\n",
            "\n",
            "\n",
            "Document: Thus, the computational complexity per each training example is\n",
            " \tQ = N × D + N × D × H + H × V,\t(2)\n",
            " where the dominating term is H × V .\n",
            "Euclidean Distance : 14.486832618713379\n",
            "\n",
            "\n",
            "Document: For this parallel training, we use mini-batch asynchronous gradient descent with an adaptive learning rate procedure called Adagrad [7].\n",
            "Euclidean Distance : 14.500414848327637\n",
            "\n",
            "\n",
            "Document: Estimation of the word vectors itself was performed using different model architectures and trained on various corpora [4, 29, 23, 19, 9], and some of the resulting word vectors were made available for future research and comparison .\n",
            "Euclidean Distance : 14.503366470336914\n",
            "\n",
            "\n",
            "Document: In Proceedings of the Joint Conference on Empirical Methods in Natural Language Processing and Computational Language Learning, 2007.\n",
            "Euclidean Distance : 14.544085502624512\n",
            "\n",
            "\n",
            "Document: With progress of machine learning techniques in recent years, it has become possible to train more complex models on much larger data set, and they typically outperform the simple models.\n",
            "Euclidean Distance : 14.59271240234375\n",
            "\n",
            "\n",
            "Document: We trained a feedforward NNLM with the same number of 640 hidden units using the DistBelief parallel training [6], using a history of 8 previous words (thus, the NNLM has more parameters than the RNNLM, as the projection layer has size 640 × 8).\n",
            "Euclidean Distance : 14.602818489074707\n",
            "\n",
            "\n",
            "Document: Then, a large list of questions is formed by connecting two word pairs.\n",
            "Euclidean Distance : 14.636691093444824\n",
            "\n",
            "\n",
            "Document: Moreover, the hidden layer is used to compute probability distribution over all the words in the vocabulary, resulting in an output layer with dimensionality V .\n",
            "Euclidean Distance : 14.644509315490723\n",
            "\n",
            "\n",
            "Document: Recurrent neural network´ based language model, In: Proceedings of Interspeech, 2010.\n",
            "Euclidean Distance : 14.668444633483887\n",
            "\n",
            "\n",
            "Document: The training data consists of several LDC corpora and is described in detail in [18] (320M words, 82K vocabulary).\n",
            "Euclidean Distance : 14.716398239135742\n",
            "\n",
            "\n",
            "Document: Similar to [18], to compare different model architectures we define first the computational complexity of a model as the number of parameters that need to be accessed to fully train the model.\n",
            "Euclidean Distance : 14.718367576599121\n",
            "\n",
            "\n",
            "Document: The questions in each category were created in two steps: first, a list of similar word pairs was created manually.\n",
            "Euclidean Distance : 14.74571418762207\n",
            "\n",
            "\n",
            "Document: We also include results on a test set introduced in [20] that focuses on syntactic similarity between words .\n",
            "Euclidean Distance : 14.75478744506836\n",
            "\n",
            "\n",
            "Document: All models are trained using stochastic gradient descent and backpropagation [26].\n",
            "Euclidean Distance : 14.780763626098633\n",
            "\n",
            "\n",
            "Document: However, we believe that usefulness of the word vectors for certain applications should be positively correlated with this accuracy metric.\n",
            "Euclidean Distance : 14.796217918395996\n",
            "\n",
            "\n",
            "Document: Further progress can be achieved by incorporating information about structure of words, especially for the syntactic questions.\n",
            "Euclidean Distance : 14.814562797546387\n",
            "\n",
            "\n",
            "Document: So, we have to increase both vector dimensionality and the amount of the training data together.\n",
            "Euclidean Distance : 14.817792892456055\n",
            "\n",
            "\n",
            "Document: Then, we compute score of each sentence in the test set by using the unknown word at the input, and predict all surrounding words in a sentence.\n",
            "Euclidean Distance : 14.8604154586792\n",
            "\n",
            "\n",
            "Document: The results using the CBOW architecture with different choice of word vector dimensionality and increasing amount of the training data are shown in Table 2.\n",
            "Euclidean Distance : 14.872721672058105\n",
            "\n",
            "\n",
            "Document: A fast and simple algorithm for training neural probabilistic language models.\n",
            "Euclidean Distance : 14.880208969116211\n",
            "\n",
            "\n",
            "Document: Advances in Neural Information Processing Systems 21, MIT Press, 2009.\n",
            "Euclidean Distance : 14.885119438171387\n",
            "\n",
            "\n",
            "Document: Three new graphical models for statistical language modelling.\n",
            "Euclidean Distance : 14.914896965026855\n",
            "\n",
            "\n",
            "Document: While the Skip-gram model itself does not perform on this task better than LSA similarity, the scores from this model are complementary to scores obtained with RNNLMs, and a weighted combination leads to a new state of the art result 58.9% accuracy (59.2% on the development part of the set and 58.7% on the test part of the set).\n",
            "Euclidean Distance : 14.93038272857666\n",
            "\n",
            "\n",
            "Document: 2.1\tFeedforward Neural Net Language Model (NNLM)\n",
            " The probabilistic feedforward neural network language model has been proposed in [1].\n",
            "Euclidean Distance : 14.935108184814453\n",
            "\n",
            "\n",
            "Document: In this work, we directly extend this architecture, and focus just on the first step where the word vectors are learned using a simple model.\n",
            "Euclidean Distance : 14.94632339477539\n",
            "\n",
            "\n",
            "Document: Although it is easy to show that word France is similar to Italy and perhaps some other countries, it is much more challenging when subjecting those vectors in a more complex similarity task, as follows.\n",
            "Euclidean Distance : 14.977049827575684\n",
            "\n",
            "\n",
            "Document: Journal of Machine Learning Research, 12:24932537, 2011.\n",
            "Euclidean Distance : 15.046161651611328\n",
            "\n",
            "\n",
            "Document: While this is what makes neural networks so attractive, we decided to explore simpler models that might not be able to represent the data as precisely as neural networks, but can possibly be trained on much more data efficiently.\n",
            "Euclidean Distance : 15.055145263671875\n",
            "\n",
            "\n",
            "Document: The CBOW architecture predicts the current word based on the context, and the Skip-gram predicts surrounding words given the current word.\n",
            "Euclidean Distance : 15.076750755310059\n",
            "\n",
            "\n",
            "Document: For example, the amount of relevant in-domain data for automatic speech recognition is limited - the performance is usually dominated by the size of high quality transcribed speech data (often just millions of words).\n",
            "Euclidean Distance : 15.097968101501465\n",
            "\n",
            "\n",
            "Document: Ng., Large Scale Distributed Deep Networks, NIPS, 2012.\n",
            "Euclidean Distance : 15.111274719238281\n",
            "\n",
            "\n",
            "Document: Training a model on twice as much data using one epoch gives comparable or better results than iterating over the same data for three epochs, as is shown in Table 5, and provides additional small speedup.\n",
            "Euclidean Distance : 15.119040489196777\n",
            "\n",
            "\n",
            "Document: While there has been later substantial amount of work that focuses on learning word vectors, we consider the approach proposed in [13] to be the simplest one.\n",
            "Euclidean Distance : 15.172937393188477\n",
            "\n",
            "\n",
            "Document: The training complexity of this architecture is proportional to\n",
            " \tQ = C × (D + D × log2(V )),\t(5)\n",
            " where C is the maximum distance of the words.\n",
            "Euclidean Distance : 15.175687789916992\n",
            "\n",
            "\n",
            "Document: [15]\tT. Mikolov, M. Karafiat, L. Burget, J.´ Cernockˇ y, S. Khudanpur.\n",
            "Euclidean Distance : 15.185075759887695\n",
            "\n",
            "\n",
            "Document: This is a popular type of problems in certain human intelligence tests.\n",
            "Euclidean Distance : 15.204482078552246\n",
            "\n",
            "\n",
            "Document: The framework allows us to run multiple replicas of the same model in parallel, and each replica synchronizes its gradient updates through a centralized server that keeps all the parameters.\n",
            "Euclidean Distance : 15.230541229248047\n",
            "\n",
            "\n",
            "Document: We also published more than 1.4 million vectors that represent named entities, trained on more than 100 billion words.\n",
            "Euclidean Distance : 15.242920875549316\n",
            "\n",
            "\n",
            "Document: However, as far as we know, these architectures were significantly more computationally expensive for training than the one proposed in [13], with the exception of certain version of log-bilinear model where diagonal weight matrices are used [23].\n",
            "Euclidean Distance : 15.285164833068848\n",
            "\n",
            "\n",
            "Document: For example, we made a list of 68 large American cities and the states they belong to, and formed about 2.5K questions by picking two word pairs at random.\n",
            "Euclidean Distance : 15.298802375793457\n",
            "\n",
            "\n",
            "Document: Probably the most successful concept is to use distributed representations of words [10].\n",
            "Euclidean Distance : 15.313851356506348\n",
            "\n",
            "\n",
            "Document: Learning word vectors for sentiment analysis.\n",
            "Euclidean Distance : 15.321474075317383\n",
            "\n",
            "\n",
            "Document: Scaling learning algorithms towards AI.\n",
            "Euclidean Distance : 15.331671714782715\n",
            "\n",
            "\n",
            "Document: [16]\tT. Mikolov, S. Kombrink, L. Burget, J. Cernockˇ y, S. Khudanpur.\n",
            "Euclidean Distance : 15.333549499511719\n",
            "\n",
            "\n",
            "Document: Accuracy is reported on the full Semantic-Syntactic data set.\n",
            "Euclidean Distance : 15.341720581054688\n",
            "\n",
            "\n",
            "Document: Another interesting architecture of NNLM was presented in [13, 14], where the word vectors are first learned using neural network with a single hidden layer.\n",
            "Euclidean Distance : 15.34948444366455\n",
            "\n",
            "\n",
            "Document: The NNLM architecture becomes complex for computation between the projection and the hidden layer, as values in the projection layer are dense.\n",
            "Euclidean Distance : 15.391060829162598\n",
            "\n",
            "\n",
            "Document: We evaluate the overall accuracy for all question types, and for each question type separately (semantic, syntactic).\n",
            "Euclidean Distance : 15.39812183380127\n",
            "\n",
            "\n",
            "Document: The NNLM vectors perform significantly better than the RNN - this is not surprising, as the word vectors in the RNNLM are directly connected to a non-linear hidden layer.\n",
            "Euclidean Distance : 15.424861907958984\n",
            "\n",
            "\n",
            "Document: What is special for this type of model is the recurrent matrix that connects hidden layer to itself, using time-delayed connections.\n",
            "Euclidean Distance : 15.43307113647461\n",
            "\n",
            "\n",
            "Document: Linguistic Regularities in Continuous Space Word Representations.\n",
            "Euclidean Distance : 15.433791160583496\n",
            "\n",
            "\n",
            "Document: Note that due to the overhead of the distributed framework, the CPU usage of the CBOW model and the Skip-gram model are much closer to each other than their single-machine implementations.\n",
            "Euclidean Distance : 15.44148063659668\n",
            "\n",
            "\n",
            "Document: Under this framework, it is common to use one hundred or more model replicas, each using many CPU cores at different machines in a data center.\n",
            "Euclidean Distance : 15.446123123168945\n",
            "\n",
            "\n",
            "Document: At the input layer, N previous words are encoded using 1-of-V coding, where V is size of the vocabulary.\n",
            "Euclidean Distance : 15.456282615661621\n",
            "\n",
            "\n",
            "Document: By using ten examples instead of one to form the relationship vector (we average the individual vectors together), we have observed improvement of accuracy of our best models by about 10% absolutely on the semantic-syntactic test.\n",
            "Euclidean Distance : 15.476218223571777\n",
            "\n",
            "\n",
            "Document: Extensions of recurrent neural´ network language model, In: Proceedings of ICASSP 2011.\n",
            "Euclidean Distance : 15.519148826599121\n",
            "\n",
            "\n",
            "Document: We have explored the performance of Skip-gram architecture on this task.\n",
            "Euclidean Distance : 15.594531059265137\n",
            "\n",
            "\n",
            "Document: Hierarchical Probabilistic Neural Network Language Model.\n",
            "Euclidean Distance : 15.601634979248047\n",
            "\n",
            "\n",
            "Document: Moreover, we discuss how training time and accuracy depends on the dimensionality of the word vectors and on the amount of the training data.\n",
            "Euclidean Distance : 15.615988731384277\n",
            "\n",
            "\n",
            "Document: 1.2\tPrevious Work\n",
            " Representation of words as continuous vectors has a long history [10, 26, 8].\n",
            "Euclidean Distance : 15.616637229919434\n",
            "\n",
            "\n",
            "Document: Huffman trees assign short binary codes to frequent words, and this further reduces the number of output units that need to be evaluated: while balanced binary tree would require log2(V ) outputs to be evaluated, the Huffman tree based hierarchical softmax requires only about log2(Unigram perplexity(V )).\n",
            "Euclidean Distance : 15.62179183959961\n",
            "\n",
            "\n",
            "Document: Somewhat surprisingly, it was found that similarity of word representations goes beyond simple syntactic regularities.\n",
            "Euclidean Distance : 15.647238731384277\n",
            "\n",
            "\n",
            "Document: For example when the vocabulary size is one million words, this results in about two times speedup in evaluation.\n",
            "Euclidean Distance : 15.651368141174316\n",
            "\n",
            "\n",
            "Document: Results from machine translation experiments also look very promising.\n",
            "Euclidean Distance : 15.666494369506836\n",
            "\n",
            "\n",
            "Document: Note that the weight matrix between the input and the projection layer is shared for all word positions in the same way as in the NNLM.\n",
            "Euclidean Distance : 15.677502632141113\n",
            "\n",
            "\n",
            "Document: Association for Computational Linguistics, 2010.\n",
            "Euclidean Distance : 15.715909957885742\n",
            "\n",
            "\n",
            "Document: Overall, there are 8869 semantic and 10675 syntactic questions.\n",
            "Euclidean Distance : 15.732139587402344\n",
            "\n",
            "\n",
            "Document: Semeval-2012 task 2: Measuring degrees of relational similarity.\n",
            "Euclidean Distance : 15.736207008361816\n",
            "\n",
            "\n",
            "Document: In International Conference on Machine Learning, ICML, 2008.\n",
            "Euclidean Distance : 15.776275634765625\n",
            "\n",
            "\n",
            "Document: Next, we evaluated our models trained using one CPU only and compared the results against publicly available word vectors.\n",
            "Euclidean Distance : 15.860010147094727\n",
            "\n",
            "\n",
            "Document: We follow the approach described above: the relationship is defined by subtracting two word vectors, and the result is added to another word.\n",
            "Euclidean Distance : 15.86902904510498\n",
            "\n",
            "\n",
            "Document: This task consists of 1040 sentences, where one word is missing in each sentence and the goal is to select word that is the most coherent with the rest of the sentence, given a list of five reasonable choices.\n",
            "Euclidean Distance : 15.89319896697998\n",
            "\n",
            "\n",
            "Document: Association for Computational Linguistics, 2012.\n",
            "Euclidean Distance : 15.897497177124023\n",
            "\n",
            "\n",
            "Document: The CBOW architecture works better than the NNLM on the syntactic tasks, and about the same on the semantic one.\n",
            "Euclidean Distance : 15.923697471618652\n",
            "\n",
            "\n",
            "Document: A neural probabilistic language model.\n",
            "Euclidean Distance : 15.92850399017334\n",
            "\n",
            "\n",
            "Document: International Joint Conference on Artificial Intelligence, 2005.\n",
            "Euclidean Distance : 15.92963695526123\n",
            "\n",
            "\n",
            "Document: The number of CPU cores is an Table 6: Comparison of models trained using the DistBelief distributed framework.\n",
            "Euclidean Distance : 15.995386123657227\n",
            "\n",
            "\n",
            "Document: For the experiments reported in Tables 2 and 4, we used three training epochs with stochastic gradient descent and backpropagation.\n",
            "Euclidean Distance : 16.01731300354004\n",
            "\n",
            "\n",
            "Document: Question is assumed to be correctly answered only if the closest word to the vector computed using the above method is exactly the same as the correct word in the question; synonyms are thus counted as mistakes.\n",
            "Euclidean Distance : 16.031997680664062\n",
            "\n",
            "\n",
            "Document: Table 1: Examples of five types of semantic and nine types of syntactic questions in the SemanticSyntactic Word Relationship test set.\n",
            "Euclidean Distance : 16.03641700744629\n",
            "\n",
            "\n",
            "Document: Because of the much lower computational complexity, it is possible to compute very accurate high dimensional word vectors from a much larger data set.\n",
            "Euclidean Distance : 16.0679874420166\n",
            "\n",
            "\n",
            "Document: Using the sentence scores, we choose the most likely sentence.\n",
            "Euclidean Distance : 16.083431243896484\n",
            "\n",
            "\n",
            "Document: In the future, it would be also interesting to compare our techniques to Latent Relational Analysis [30] and others.\n",
            "Euclidean Distance : 16.085567474365234\n",
            "\n",
            "\n",
            "Document: The word vectors are then used to train the NNLM.\n",
            "Euclidean Distance : 16.110830307006836\n",
            "\n",
            "\n",
            "Document: A Scalable Hierarchical Distributed Language Model.\n",
            "Euclidean Distance : 16.130762100219727\n",
            "\n",
            "\n",
            "Document: Training complexity is then\n",
            " \tQ = N × D + D × log2(V ).\n",
            "Euclidean Distance : 16.138290405273438\n",
            "\n",
            "\n",
            "Document: An interesting task where the word vectors have recently been shown to significantly outperform the previous state of the art is the SemEval-2012 Task 2 [11].\n",
            "Euclidean Distance : 16.143007278442383\n",
            "\n",
            "\n",
            "Document: However, several practical solutions were proposed for avoiding it; either using hierarchical versions of the softmax [25, 23, 18], or avoiding normalized models completely by using models that are not normalized during training [4, 9].\n",
            "Euclidean Distance : 16.15892219543457\n",
            "\n",
            "\n",
            "Document: Combining Heterogeneous Models for Measuring Relational Similarity.\n",
            "Euclidean Distance : 16.166980743408203\n",
            "\n",
            "\n",
            "Document: Note that training of NNLM with 1000-dimensional vectors would take too long to complete.\n",
            "Euclidean Distance : 16.16834831237793\n",
            "\n",
            "\n",
            "Document: In: Parallel distributed processing: Explorations in the microstructure of cognition.\n",
            "Euclidean Distance : 16.171667098999023\n",
            "\n",
            "\n",
            "Document: Finally, the Skip-gram architecture works slightly worse on the syntactic task than the CBOW model (but still better than the NNLM), and much better on the semantic part of the test than all the other models.\n",
            "Euclidean Distance : 16.182096481323242\n",
            "\n",
            "\n",
            "Document: Clearly, there is still a lot of discoveries to be made using these techniques.\n",
            "Euclidean Distance : 16.23988151550293\n",
            "\n",
            "\n",
            "Document: Cognitive Science, 14, 179-211, 1990.\n",
            "Euclidean Distance : 16.243093490600586\n",
            "\n",
            "\n",
            "Document: First, we train the 640dimensional model on 50M words provided in [32].\n",
            "Euclidean Distance : 16.250516891479492\n",
            "\n",
            "\n",
            "Document: It can be expected that these applications can benefit from the model architectures described in this paper.\n",
            "Euclidean Distance : 16.279647827148438\n",
            "\n",
            "\n",
            "Document: The input layer is then projected to a projection layer P that has dimensionality N × D, using a shared projection matrix.\n",
            "Euclidean Distance : 16.326358795166016\n",
            "\n",
            "\n",
            "Document: When the word vectors are well trained, it is possible to find the correct answer (word smallest) using this method.\n",
            "Euclidean Distance : 16.37933921813965\n",
            "\n",
            "\n",
            "Document: Next, we will try to maximize the accuracy, while minimizing the computational complexity.\n",
            "Euclidean Distance : 16.414304733276367\n",
            "\n",
            "\n",
            "Document: Full vocabularies are used.\n",
            "Euclidean Distance : 16.429487228393555\n",
            "\n",
            "\n",
            "Document: The training speed is significantly higher than reported earlier in this paper, i.e.\n",
            "Euclidean Distance : 16.4332332611084\n",
            "\n",
            "\n",
            "Document: Example of another type of relationship can be word pairs big - biggest and small - smallest [20].\n",
            "Euclidean Distance : 16.452417373657227\n",
            "\n",
            "\n",
            "Document: The publicly available RNN vectors were used together with other techniques to achieve over 50% increase in Spearman’s rank correlation over the previous best result [31].\n",
            "Euclidean Distance : 16.453310012817383\n",
            "\n",
            "\n",
            "Document: 5\tExamples of the Learned Relationships\n",
            " Table 8 shows words that follow various relationships.\n",
            "Euclidean Distance : 16.479276657104492\n",
            "\n",
            "\n",
            "Document: For example, neural network based language models significantly outperform N-gram models [1, 27, 17].\n",
            "Euclidean Distance : 16.499835968017578\n",
            "\n",
            "\n",
            "Document: PhD thesis, Brno University of Technology, 2012.\n",
            "Euclidean Distance : 16.5009708404541\n",
            "\n",
            "\n",
            "Document: Measuring Semantic Similarity by Latent Relational Analysis.\n",
            "Euclidean Distance : 16.50188446044922\n",
            "\n",
            "\n",
            "Document: This allows the recurrent model to form some kind of short term memory, as information from the past can be represented by the hidden layer state that gets updated based on the current input and the state of the hidden layer in the previous time step.\n",
            "Euclidean Distance : 16.5382137298584\n",
            "\n",
            "\n",
            "Document: Common choice is E = 3−50 and T up to one billion.\n",
            "Euclidean Distance : 16.553186416625977\n",
            "\n",
            "\n",
            "Document: [26]\tD. E. Rumelhart, G. E. Hinton, R. J. Williams.\n",
            "Euclidean Distance : 16.574491500854492\n",
            "\n",
            "\n",
            "Document: That is several orders of magnitude larger than the best previously published results for similar models.\n",
            "Euclidean Distance : 16.58060646057129\n",
            "\n",
            "\n",
            "Document: Statistical Language Models based on Neural Networks.\n",
            "Euclidean Distance : 16.60750389099121\n",
            "\n",
            "\n",
            "Document: While this is not crucial speedup for neural network LMs as the computational bottleneck is in the N×D×H term, we will later propose architectures that do not have hidden layers and thus depend heavily on the efficiency of the softmax normalization.\n",
            "Euclidean Distance : 16.62667465209961\n",
            "\n",
            "\n",
            "Document: [18]\tT. Mikolov, A. Deoras, D. Povey, L. Burget, J. Cernockˇ y.\n",
            "Euclidean Distance : 16.634254455566406\n",
            "\n",
            "\n",
            "Document: it is in the order of billions of words per hour for typical hyperparameter choices.\n",
            "Euclidean Distance : 16.646602630615234\n",
            "\n",
            "\n",
            "Document: Computer Speech and Language, vol.\n",
            "Euclidean Distance : 16.675798416137695\n",
            "\n",
            "\n",
            "Document: R words from the future of the current word as correct labels.\n",
            "Euclidean Distance : 16.733823776245117\n",
            "\n",
            "\n",
            "Document: Journal of Machine Learning Research, 2011.\n",
            "Euclidean Distance : 16.73403549194336\n",
            "\n",
            "\n",
            "Document: The main observation from the previous section was that most of the complexity is caused by the non-linear hidden layer in the model.\n",
            "Euclidean Distance : 16.734331130981445\n",
            "\n",
            "\n",
            "Document: [5]\tR. Collobert, J. Weston, L. Bottou, M. Karlen, K. Kavukcuoglu and P. Kuksa.\n",
            "Euclidean Distance : 16.78400230407715\n",
            "\n",
            "\n",
            "Document: [3]\tT. Brants, A. C. Popat, P. Xu, F. J. Och, and J.\n",
            "Euclidean Distance : 16.80328369140625\n",
            "\n",
            "\n",
            "Document: Another way to improve accuracy is to provide more than one example of the relationship.\n",
            "Euclidean Distance : 16.817684173583984\n",
            "\n",
            "\n",
            "Document: Thus, the word vectors are learned even without constructing the full NNLM.\n",
            "Euclidean Distance : 16.857784271240234\n",
            "\n",
            "\n",
            "Document: [9]\tEric H. Huang, R. Socher, C. D. Manning and Andrew Y. Ng.\n",
            "Euclidean Distance : 16.877159118652344\n",
            "\n",
            "\n",
            "Document: Automatic Speech Recognition and Understanding, 2011.\n",
            "Euclidean Distance : 16.894567489624023\n",
            "\n",
            "\n",
            "Document: With binary tree representations of the vocabulary, the number of output units that need to be evaluated can go down to around log2(V ).\n",
            "Euclidean Distance : 16.931316375732422\n",
            "\n",
            "\n",
            "Document: We observe large improvements in accuracy at much lower computational cost, i.e.\n",
            "Euclidean Distance : 16.966886520385742\n",
            "\n",
            "\n",
            "Document: Note that related models have been proposed also much earlier [26, 8].\n",
            "Euclidean Distance : 16.974716186523438\n",
            "\n",
            "\n",
            "Document: References\n",
            " [1]\tY. Bengio, R. Ducharme, P. Vincent.\n",
            "Euclidean Distance : 16.982587814331055\n",
            "\n",
            "\n",
            "Document: it takes less than a day to learn high quality word vectors from a 1.6 billion words data set.\n",
            "Euclidean Distance : 17.004764556884766\n",
            "\n",
            "\n",
            "Document: Thus for example, Paris - France + Italy = Rome.\n",
            "Euclidean Distance : 17.034679412841797\n",
            "\n",
            "\n",
            "Document: In: Proceedings of the 6th International Workshop on Semantic Evaluation (SemEval 2012), 2012.\n",
            "Euclidean Distance : 17.084491729736328\n",
            "\n",
            "\n",
            "Document: Yih, C. Meek, G. Zweig, T. Mikolov.\n",
            "Euclidean Distance : 17.126747131347656\n",
            "\n",
            "\n",
            "Document: It is also possible to apply the vector operations to solve different tasks.\n",
            "Euclidean Distance : 17.13796615600586\n",
            "\n",
            "\n",
            "Document: For a common choice of N = 10, the size of the projection layer (P) might be 500 to 2000, while the hidden layer size H is typically 500 to 1000 units.\n",
            "Euclidean Distance : 17.141172409057617\n",
            "\n",
            "\n",
            "Document: Continuous space language models.\n",
            "Euclidean Distance : 17.153745651245117\n",
            "\n",
            "\n",
            "Document: Natural Language Processing (Almost) from Scratch.\n",
            "Euclidean Distance : 17.178064346313477\n",
            "\n",
            "\n",
            "Document: We have restricted the vocabulary size to 1 million most frequent words.\n",
            "Euclidean Distance : 17.218759536743164\n",
            "\n",
            "\n",
            "Document: The final sentence score is then the sum of these individual predictions.\n",
            "Euclidean Distance : 17.21969985961914\n",
            "\n",
            "\n",
            "Document: [32]\tG. Zweig, C.J.C.\n",
            "Euclidean Distance : 17.245847702026367\n",
            "\n",
            "\n",
            "Document: Thus, there are situations where simple scaling up of the basic techniques will not result in any significant progress, and we have to focus on more advanced techniques.\n",
            "Euclidean Distance : 17.292072296142578\n",
            "\n",
            "\n",
            "Document: Volume 1: Foundations, MIT Press, 1986.\n",
            "Euclidean Distance : 17.298295974731445\n",
            "\n",
            "\n",
            "Document: [25]\tF. Morin, Y. Bengio.\n",
            "Euclidean Distance : 17.342260360717773\n",
            "\n",
            "\n",
            "Document: We call this architecture a bag-of-words model as the order of words in the history does not influence the projection.\n",
            "Euclidean Distance : 17.38874626159668\n",
            "\n",
            "\n",
            "Document: [28]\tR. Socher, E.H. Huang, J. Pennington, A.Y.\n",
            "Euclidean Distance : 17.446659088134766\n",
            "\n",
            "\n",
            "Document: Learning internal representations by backpropagating errors.\n",
            "Euclidean Distance : 17.467571258544922\n",
            "\n",
            "\n",
            "Document: In machine translation, the existing corpora for many languages contain only a few billions of words or less.\n",
            "Euclidean Distance : 17.492952346801758\n",
            "\n",
            "\n",
            "Document: It can be seen that after some point, adding more dimensions or adding more training data provides diminishing improvements.\n",
            "Euclidean Distance : 17.528337478637695\n",
            "\n",
            "\n",
            "Document: Only questions containing words from the most frequent 30k words are used.\n",
            "Euclidean Distance : 17.552379608154297\n",
            "\n",
            "\n",
            "Document: [21]\tT. Mikolov, I. Sutskever, K. Chen, G. Corrado, and J.\n",
            "Euclidean Distance : 17.553239822387695\n",
            "\n",
            "\n",
            "Document: [2]\tY. Bengio, Y. LeCun.\n",
            "Euclidean Distance : 17.555633544921875\n",
            "\n",
            "\n",
            "Document: In the following experiments, we use C = 10.\n",
            "Euclidean Distance : 17.59218406677246\n",
            "\n",
            "\n",
            "Document: Corrado, R. Monga, K. Chen, M. Devin, Q.V.\n",
            "Euclidean Distance : 17.603946685791016\n",
            "\n",
            "\n",
            "Document: [29]\tJ. Turian, L. Ratinov, Y. Bengio.\n",
            "Euclidean Distance : 17.607145309448242\n",
            "\n",
            "\n",
            "Document: We used these data to provide a comparison to a previously trained recurrent neural network language model that took about 8 weeks to train on a single CPU.\n",
            "Euclidean Distance : 17.649641036987305\n",
            "\n",
            "\n",
            "Document: Word Representations: A Simple and General Method for Semi-Supervised Learning.\n",
            "Euclidean Distance : 17.73236846923828\n",
            "\n",
            "\n",
            "Document: [4]\tR. Collobert and J. Weston.\n",
            "Euclidean Distance : 17.744186401367188\n",
            "\n",
            "\n",
            "Document: 1\tIntroduction\n",
            " Many current NLP systems and techniques treat words as atomic units - there is no notion of similarity between words, as these are represented as indices in a vocabulary.\n",
            "Euclidean Distance : 17.787490844726562\n",
            "\n",
            "\n",
            "Document: Two examples from each category are shown in Table 1.\n",
            "Euclidean Distance : 17.84168243408203\n",
            "\n",
            "\n",
            "Document: Pham, D. Huang, A.Y.\n",
            "Euclidean Distance : 17.907958984375\n",
            "\n",
            "\n",
            "Document: [31]\tA. Zhila, W.T.\n",
            "Euclidean Distance : 17.96809959411621\n",
            "\n",
            "\n",
            "Document: [23]\tA. Mnih, G. Hinton.\n",
            "Euclidean Distance : 17.991374969482422\n",
            "\n",
            "\n",
            "Document: Distributed Representations of Words and Phrases and their Compositionality.\n",
            "Euclidean Distance : 17.99690055847168\n",
            "\n",
            "\n",
            "Document: Nature, 323:533.536, 1986.\n",
            "Euclidean Distance : 18.01112174987793\n",
            "\n",
            "\n",
            "Document: We used 50 to 100 model replicas during the training.\n",
            "Euclidean Distance : 18.012609481811523\n",
            "\n",
            "\n",
            "Document: [24]\tA. Mnih, Y.W.\n",
            "Euclidean Distance : 18.052509307861328\n",
            "\n",
            "\n",
            "Document: The model architecture is shown at Figure 1.\n",
            "Euclidean Distance : 18.072629928588867\n",
            "\n",
            "\n",
            "Document: unrestricted to the 30k vocabulary.\n",
            "Euclidean Distance : 18.093990325927734\n",
            "\n",
            "\n",
            "Document: Yih, G. Zweig.\n",
            "Euclidean Distance : 18.149728775024414\n",
            "\n",
            "\n",
            "Document: [7]\tJ.C. Duchi, E. Hazan, and Y.\n",
            "Euclidean Distance : 18.160858154296875\n",
            "\n",
            "\n",
            "Document: As far as we know, none of the previously proposed architectures has been successfully trained on more than a few hundred of millions of words, with a modest dimensionality of the word vectors between 50 - 100.\n",
            "Euclidean Distance : 18.180139541625977\n",
            "\n",
            "\n",
            "Document: [22]\tA. Mnih, G. Hinton.\n",
            "Euclidean Distance : 18.205352783203125\n",
            "\n",
            "\n",
            "Document: We have included in our test set only single token words, thus multi-word entities are not present (such as New York).\n",
            "Euclidean Distance : 18.254993438720703\n",
            "\n",
            "\n",
            "Document: Holyoak.\n",
            "Euclidean Distance : 18.288835525512695\n",
            "\n",
            "\n",
            "Document: A short summary of some previous results together with the new results is presented in Table 7.\n",
            "Euclidean Distance : 18.297393798828125\n",
            "\n",
            "\n",
            "Document: It consists of input, projection, hidden and output layers.\n",
            "Euclidean Distance : 18.349634170532227\n",
            "\n",
            "\n",
            "Document: Since the more distant words are usually less related to the current word than those close to it, we give less weight to the distant words by sampling less from those words in our training examples.\n",
            "Euclidean Distance : 18.37875747680664\n",
            "\n",
            "\n",
            "Document: Turney, K.J.\n",
            "Euclidean Distance : 18.416893005371094\n",
            "\n",
            "\n",
            "Document: Senior, P. Tucker, K. Yang, A. Y.\n",
            "Euclidean Distance : 18.42473030090332\n",
            "\n",
            "\n",
            "Document: For experiments reported further, we used just one training epoch (again, we decrease the learning rate linearly so that it approaches zero at the end of training).\n",
            "Euclidean Distance : 18.470853805541992\n",
            "\n",
            "\n",
            "Document: Rumelhart.\n",
            "Euclidean Distance : 18.510942459106445\n",
            "\n",
            "\n",
            "Document: [27]\tH. Schwenk.\n",
            "Euclidean Distance : 18.546180725097656\n",
            "\n",
            "\n",
            "Document: Hinton, J.L.\n",
            "Euclidean Distance : 18.549827575683594\n",
            "\n",
            "\n",
            "Document: Dean, G.S.\n",
            "Euclidean Distance : 18.559051513671875\n",
            "\n",
            "\n",
            "Document: The result are reported in Table 6.\n",
            "Euclidean Distance : 18.608781814575195\n",
            "\n",
            "\n",
            "Document: [20]\tT. Mikolov, W.T.\n",
            "Euclidean Distance : 18.619524002075195\n",
            "\n",
            "\n",
            "Document: AISTATS, 2005.\n",
            "Euclidean Distance : 18.621463775634766\n",
            "\n",
            "\n",
            "Document: Ranzato, A.\n",
            "Euclidean Distance : 18.62676239013672\n",
            "\n",
            "\n",
            "Document: This also means that reaching 100% accuracy is likely to be impossible, as the current models do not have any input information about word morphology.\n",
            "Euclidean Distance : 18.634916305541992\n",
            "\n",
            "\n",
            "Document: NAACL HLT 2013.\n",
            "Euclidean Distance : 18.67132568359375\n",
            "\n",
            "\n",
            "Document: NAACL HLT 2013.\n",
            "Euclidean Distance : 18.67132568359375\n",
            "\n",
            "\n",
            "Document: This corpus contains about 6B tokens.\n",
            "Euclidean Distance : 18.727323532104492\n",
            "\n",
            "\n",
            "Document: ICML, 2007.\n",
            "Euclidean Distance : 18.73740005493164\n",
            "\n",
            "\n",
            "Document: Some of our follow-up work will be published in an upcoming NIPS 2013 paper [21].\n",
            "Euclidean Distance : 18.745058059692383\n",
            "\n",
            "\n",
            "Document: Ng, and C. Potts.\n",
            "Euclidean Distance : 18.754249572753906\n",
            "\n",
            "\n",
            "Document: The comparison is given in Table 4.\n",
            "Euclidean Distance : 18.763452529907227\n",
            "\n",
            "\n",
            "Document: McClelland, D.E.\n",
            "Euclidean Distance : 18.78553009033203\n",
            "\n",
            "\n",
            "Document: Daly, P.T.\n",
            "Euclidean Distance : 18.81499671936035\n",
            "\n",
            "\n",
            "Document: France is to Paris as Germany is to Berlin.\n",
            "Euclidean Distance : 18.83258819580078\n",
            "\n",
            "\n",
            "Document: Le, M.Z.\n",
            "Euclidean Distance : 18.835710525512695\n",
            "\n",
            "\n",
            "Document: [13]\tT. Mikolov.\n",
            "Euclidean Distance : 18.87462615966797\n",
            "\n",
            "\n",
            "Document: This work has been followed by many others.\n",
            "Euclidean Distance : 18.9001522064209\n",
            "\n",
            "\n",
            "Document: [19]\tT. Mikolov.\n",
            "Euclidean Distance : 19.071430206298828\n",
            "\n",
            "\n",
            "Document: ICASSP 2009.\n",
            "Euclidean Distance : 19.094388961791992\n",
            "\n",
            "\n",
            "Document: Maas, R.E.\n",
            "Euclidean Distance : 19.113332748413086\n",
            "\n",
            "\n",
            "Document: [30]\tP. D. Turney.\n",
            "Euclidean Distance : 19.165328979492188\n",
            "\n",
            "\n",
            "Document: As only N inputs are active at any given time, composition of the projection layer is a relatively cheap operation.\n",
            "Euclidean Distance : 19.168901443481445\n",
            "\n",
            "\n",
            "Document: Mohammad, P.D.\n",
            "Euclidean Distance : 19.172544479370117\n",
            "\n",
            "\n",
            "Document: Finding Structure in Time.\n",
            "Euclidean Distance : 19.195436477661133\n",
            "\n",
            "\n",
            "Document: In: Proc.\n",
            "Euclidean Distance : 19.20491600036621\n",
            "\n",
            "\n",
            "Document: In: Proc.\n",
            "Euclidean Distance : 19.20491600036621\n",
            "\n",
            "\n",
            "Document: In: Proc.\n",
            "Euclidean Distance : 19.20491600036621\n",
            "\n",
            "\n",
            "Document: In Proceedings of ACL, 2011.\n",
            "Euclidean Distance : 19.23595428466797\n",
            "\n",
            "\n",
            "Document: We chose starting learning rate 0.025 and decreased it linearly, so that it approaches zero at the end of the last training epoch.\n",
            "Euclidean Distance : 19.37176513671875\n",
            "\n",
            "\n",
            "Document: Accepted to NIPS 2013.\n",
            "Euclidean Distance : 19.383304595947266\n",
            "\n",
            "\n",
            "Document: Ng, and C.D.\n",
            "Euclidean Distance : 19.403484344482422\n",
            "\n",
            "\n",
            "Document: Burges.\n",
            "Euclidean Distance : 19.443330764770508\n",
            "\n",
            "\n",
            "Document: ICML, 2012.\n",
            "Euclidean Distance : 19.473339080810547\n",
            "\n",
            "\n",
            "Document: Mao, M.A.\n",
            "Euclidean Distance : 19.475671768188477\n",
            "\n",
            "\n",
            "Document: However, the simple techniques are at their limits in many tasks.\n",
            "Euclidean Distance : 19.50613021850586\n",
            "\n",
            "\n",
            "Document: [10]\tG.E.\n",
            "Euclidean Distance : 19.533485412597656\n",
            "\n",
            "\n",
            "Document: [6]\tJ.\n",
            "Euclidean Distance : 19.570112228393555\n",
            "\n",
            "\n",
            "Document: [8]\tJ. Elman.\n",
            "Euclidean Distance : 19.651992797851562\n",
            "\n",
            "\n",
            "Document: In NIPS, 2011.\n",
            "Euclidean Distance : 19.65627670288086\n",
            "\n",
            "\n",
            "Document: Dean.\n",
            "Euclidean Distance : 19.880529403686523\n",
            "\n",
            "\n",
            "Document: Dean.\n",
            "Euclidean Distance : 19.880529403686523\n",
            "\n",
            "\n",
            "Document: 21, 2007.\n",
            "Euclidean Distance : 19.881145477294922\n",
            "\n",
            "\n",
            "Document: [12]\tA.L.\n",
            "Euclidean Distance : 19.967641830444336\n",
            "\n",
            "\n",
            "Document: Teh.\n",
            "Euclidean Distance : 20.107404708862305\n",
            "\n",
            "\n",
            "Document: Jurgens, S.M.\n",
            "Euclidean Distance : 20.19614028930664\n",
            "\n",
            "\n",
            "Document: Manning.\n",
            "Euclidean Distance : 20.271333694458008\n",
            "\n",
            "\n",
            "Document: [11]\tD.A.\n",
            "Euclidean Distance : 20.273557662963867\n",
            "\n",
            "\n",
            "Document: The RNN model does not have a projection layer; only input, hidden and output layer.\n",
            "Euclidean Distance : 20.50300407409668\n",
            "\n",
            "\n",
            "Document: Singer.\n",
            "Euclidean Distance : 20.541976928710938\n",
            "\n",
            "\n",
            "Document: Distributed representations.\n",
            "Euclidean Distance : 20.768924713134766\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "id": "fPpunwik1dAg",
        "outputId": "8aaaf150-9a4e-4181-924a-6fffcb9a439b"
      },
      "source": [
        "documents_df.tail()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>documents</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>312</th>\n",
              "      <td>Combining Heterogeneous Models for Measuring R...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>313</th>\n",
              "      <td>NAACL HLT 2013.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>314</th>\n",
              "      <td>[32]\\tG. Zweig, C.J.C.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>315</th>\n",
              "      <td>Burges.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>316</th>\n",
              "      <td>The Microsoft Research Sentence Completion Cha...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                             documents\n",
              "312  Combining Heterogeneous Models for Measuring R...\n",
              "313                                    NAACL HLT 2013.\n",
              "314                             [32]\\tG. Zweig, C.J.C.\n",
              "315                                            Burges.\n",
              "316  The Microsoft Research Sentence Completion Cha..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T6OecQ1P6lqq",
        "outputId": "33f40d3b-5058-4381-ec36-364a29e22f4e"
      },
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NnNh5WPW51ya"
      },
      "source": [
        "ttt = nltk.tokenize.TextTilingTokenizer()\n",
        "tiles = ttt.tokenize(text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YfDza6cq6sg_",
        "outputId": "1dec29ae-d57c-43f8-fd4e-a99527db3dea"
      },
      "source": [
        "print(tiles)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['  \\n Efficient Estimation of Word Representations in\\n Vector Space\\n  \\n Tomas Mikolov\\n Google Inc., Mountain View, CA tmikolov@google.com\\tKai Chen\\n Google Inc., Mountain View, CA kaichen@google.com\\n Greg Corrado\\n Google Inc., Mountain View, CA gcorrado@google.com\\tJeffrey Dean\\n Google Inc., Mountain View, CA jeff@google.com\\n Abstract\\n We propose two novel model architectures for computing continuous vector representations of words from very large data sets. The quality of these representations is measured in a word similarity task, and the results are compared to the previously best performing techniques based on different types of neural networks. We observe large improvements in accuracy at much lower computational cost, i.e. it takes less than a day to learn high quality word vectors from a 1.6 billion words data set. Furthermore, we show that these vectors provide state-of-the-art performance on our test set for measuring syntactic and semantic word similarities.\\n 1\\tIntroduction\\n Many current NLP systems and techniques treat words as atomic units - there is no notion of similarity between words, as these are represented as indices in a vocabulary. This choice has several good reasons - simplicity, robustness and the observation that simple models trained on huge amounts of data outperform complex systems trained on less data. An example is the popular N-gram model used for statistical language modeling - today, it is possible to train N-grams on virtually all available data (trillions of words [3]).\\n However, the simple techniques are at their limits in many tasks. For example, the amount of relevant in-domain data for automatic speech recognition is limited - the performance is usually dominated by the size of high quality transcribed speech data (often just millions of words). In machine translation, the existing corpora for many languages contain only a few billions of words or less. Thus, there are situations where simple scaling up of the basic techniques will not result in any significant progress, and we have to focus on more advanced techniques.\\n With progress of machine learning techniques in recent years, it has become possible to train more complex models on much larger data set, and they typically outperform the simple models. Probably the most successful concept is to use distributed representations of words [10]. For example, neural network based language models significantly outperform N-gram models [1, 27, 17].\\n 1.1\\tGoals of the Paper\\n The main goal of this paper is to introduce techniques that can be used for learning high-quality word vectors from huge data sets with billions of words, and with millions of words in the vocabulary. As far as we know, none of the previously proposed architectures has been successfully trained on more than a few hundred of millions of words, with a modest dimensionality of the word vectors between 50 - 100.\\n We use recently proposed techniques for measuring the quality of the resulting vector representations, with the expectation that not only will similar words tend to be close to each other, but that words can have multiple degrees of similarity [20]. This has been observed earlier in the context of inflectional languages - for example, nouns can have multiple word endings, and if we search for similar words in a subspace of the original vector space, it is possible to find words that have similar endings [13, 14].\\n Somewhat surprisingly, it was found that similarity of word representations goes beyond simple syntactic regularities. Using a word offset technique where simple algebraic operations are performed on the word vectors, it was shown for example that vector(”King”) - vector(”Man”) + vector(”Woman”) results in a vector that is closest to the vector representation of the word Queen [20].\\n In this paper, we try to maximize accuracy of these vector operations by developing new model architectures that preserve the linear regularities among words. We design a new comprehensive test set for measuring both syntactic and semantic regularities , and show that many such regularities can be learned with high accuracy. Moreover, we discuss how training time and accuracy depends on the dimensionality of the word vectors and on the amount of the training data.\\n 1.2\\tPrevious Work\\n Representation of words as continuous vectors has a long history [10, 26, 8]. A very popular model architecture for estimating neural network language model (NNLM) was proposed in [1], where a feedforward neural network with a linear projection layer and a non-linear hidden layer was used to learn jointly the word vector representation and a statistical language model. This work has been followed by many others.\\n Another interesting architecture of NNLM was presented in [13, 14], where the word vectors are first learned using neural network with a single hidden layer. The word vectors are then used to train the NNLM. Thus, the word vectors are learned even without constructing the full NNLM. In this work, we directly extend this architecture, and focus just on the first step where the word vectors are learned using a simple model.\\n It was later shown that the word vectors can be used to significantly improve and simplify many NLP applications [4, 5, 29]. Estimation of the word vectors itself was performed using different model architectures and trained on various corpora [4, 29, 23, 19, 9], and some of the resulting word vectors were made available for future research and comparison . However, as far as we know, these architectures were significantly more computationally expensive for training than the one proposed in [13], with the exception of certain version of log-bilinear model where diagonal weight matrices are used [23].\\n 2\\tModel Architectures\\n Many different types of models were proposed for estimating continuous representations of words, including the well-known Latent Semantic Analysis (LSA) and Latent Dirichlet Allocation (LDA). In this paper, we focus on distributed representations of words learned by neural networks, as it was previously shown that they perform significantly better than LSA for preserving linear regularities among words [20, 31]; LDA moreover becomes computationally very expensive on large data sets.\\n Similar to [18], to compare different model architectures we define first the computational complexity of a model as the number of parameters that need to be accessed to fully train the model. Next, we will try to maximize the accuracy, while minimizing the computational complexity.\\n For all the following models, the training complexity is proportional to\\n \\tO = E × T × Q,\\t(1)\\n where E is number of the training epochs, T is the number of the words in the training set and Q is defined further for each model architecture. Common choice is E = 3−50 and T up to one billion. All models are trained using stochastic gradient descent and backpropagation [26].\\n 2.1\\tFeedforward Neural Net Language Model (NNLM)\\n The probabilistic feedforward neural network language model has been proposed in [1]. It consists of input, projection, hidden and output layers. At the input layer, N previous words are encoded using 1-of-V coding, where V is size of the vocabulary. The input layer is then projected to a projection layer P that has dimensionality N × D, using a shared projection matrix. As only N inputs are active at any given time, composition of the projection layer is a relatively cheap operation.\\n The NNLM architecture becomes complex for computation between the projection and the hidden layer, as values in the projection layer are dense. For a common choice of N = 10, the size of the projection layer (P) might be 500 to 2000, while the hidden layer size H is typically 500 to 1000 units. Moreover, the hidden layer is used to compute probability distribution over all the words in the vocabulary, resulting in an output layer with dimensionality V . Thus, the computational complexity per each training example is\\n \\tQ = N × D + N × D × H + H × V,\\t(2)\\n where the dominating term is H × V . However, several practical solutions were proposed for avoiding it; either using hierarchical versions of the softmax [25, 23, 18], or avoiding normalized models completely by using models that are not normalized during training [4, 9]. With binary tree representations of the vocabulary, the number of output units that need to be evaluated can go down to around log2(V ). Thus, most of the complexity is caused by the term N × D × H.\\n In our models, we use hierarchical softmax where the vocabulary is represented as a Huffman binary tree. This follows previous observations that the frequency of words works well for obtaining classes in neural net language models [16]. Huffman trees assign short binary codes to frequent words, and this further reduces the number of output units that need to be evaluated: while balanced binary tree would require log2(V ) outputs to be evaluated, the Huffman tree based hierarchical softmax requires only about log2(Unigram perplexity(V )). For example when the vocabulary size is one million words, this results in about two times speedup in evaluation. While this is not crucial speedup for neural network LMs as the computational bottleneck is in the N×D×H term, we will later propose architectures that do not have hidden layers and thus depend heavily on the efficiency of the softmax normalization.\\n 2.2\\tRecurrent Neural Net Language Model (RNNLM)\\n Recurrent neural network based language model has been proposed to overcome certain limitations of the feedforward NNLM, such as the need to specify the context length (the order of the model N), and because theoretically RNNs can efficiently represent more complex patterns than the shallow neural networks [15, 2]. The RNN model does not have a projection layer; only input, hidden and output layer. What is special for this type of model is the recurrent matrix that connects hidden layer to itself, using time-delayed connections. This allows the recurrent model to form some kind of short term memory, as information from the past can be represented by the hidden layer state that gets updated based on the current input and the state of the hidden layer in the previous time step.\\n The complexity per training example of the RNN model is\\n \\tQ = H × H + H × V,\\t(3)\\n where the word representations D have the same dimensionality as the hidden layer H. Again, the term H × V can be efficiently reduced to H × log2(V ) by using hierarchical softmax. Most of the complexity then comes from H × H.\\n 2.3\\tParallel Training of Neural Networks\\n To train models on huge data sets, we have implemented several models on top of a large-scale distributed framework called DistBelief [6], including the feedforward NNLM and the new models proposed in this paper. The framework allows us to run multiple replicas of the same model in parallel, and each replica synchronizes its gradient updates through a centralized server that keeps all the parameters. For this parallel training, we use mini-batch asynchronous gradient descent with an adaptive learning rate procedure called Adagrad [7]. Under this framework, it is common to use one hundred or more model replicas, each using many CPU cores at different machines in a data center.\\n 3\\tNew Log-linear Models\\n In this section, we propose two new model architectures for learning distributed representations of words that try to minimize computational complexity. The main observation from the previous section was that most of the complexity is caused by the non-linear hidden layer in the model. While this is what makes neural networks so attractive, we decided to explore simpler models that might not be able to represent the data as precisely as neural networks, but can possibly be trained on much more data efficiently.\\n The new architectures directly follow those proposed in our earlier work [13, 14], where it was found that neural network language model can be successfully trained in two steps: first, continuous word vectors are learned using simple model, and then the N-gram NNLM is trained on top of these distributed representations of words. While there has been later substantial amount of work that focuses on learning word vectors, we consider the approach proposed in [13] to be the simplest one. Note that related models have been proposed also much earlier [26, 8].\\n 3.1\\tContinuous Bag-of-Words Model\\n The first proposed architecture is similar to the feedforward NNLM, where the non-linear hidden layer is removed and the projection layer is shared for all words (not just the projection matrix); thus, all words get projected into the same position (their vectors are averaged). We call this architecture a bag-of-words model as the order of words in the history does not influence the projection. Furthermore, we also use words from the future; we have obtained the best performance on the task introduced in the next section by building a log-linear classifier with four future and four history words at the input, where the training criterion is to correctly classify the current (middle) word. Training complexity is then\\n \\tQ = N × D + D × log2(V ).\\t(4)\\n We denote this model further as CBOW, as unlike standard bag-of-words model, it uses continuous distributed representation of the context. The model architecture is shown at Figure 1. Note that the weight matrix between the input and the projection layer is shared for all word positions in the same way as in the NNLM.\\n 3.2\\tContinuous Skip-gram Model\\n The second architecture is similar to CBOW, but instead of predicting the current word based on the context, it tries to maximize classification of a word based on another word in the same sentence. More precisely, we use each current word as an input to a log-linear classifier with continuous projection layer, and predict words within a certain range before and after the current word. We found that increasing the range improves quality of the resulting word vectors, but it also increases the computational complexity. Since the more distant words are usually less related to the current word than those close to it, we give less weight to the distant words by sampling less from those words in our training examples.\\n The training complexity of this architecture is proportional to\\n \\tQ = C × (D + D × log2(V )),\\t(5)\\n where C is the maximum distance of the words. Thus, if we choose C = 5, for each training word we will select randomly a number R in range < 1;C >, and then use R words from history and\\n \\t       INPUT         PROJECTION         OUTPUT\\t          INPUT         PROJECTION      OUTPUT\\n w(t-2)w(t-2)\\n w(t-1)w(t-1)\\n \\tw(t)\\tw(t)\\n w(t+1)w(t+1)\\n w(t+2)w(t+2)\\n                    CBOW                                                   Skip-gram\\n Figure 1: New model architectures. The CBOW architecture predicts the current word based on the context, and the Skip-gram predicts surrounding words given the current word.\\n R words from the future of the current word as correct labels. This will require us to do R × 2 word classifications, with the current word as input, and each of the R + R words as output. In the following experiments, we use C = 10.\\n 4\\tResults\\n To compare the quality of different versions of word vectors, previous papers typically use a table showing example words and their most similar words, and understand them intuitively. Although it is easy to show that word France is similar to Italy and perhaps some other countries, it is much more challenging when subjecting those vectors in a more complex similarity task, as follows. We follow previous observation that there can be many different types of similarities between words, for example, word big is similar to bigger in the same sense that small is similar to smaller. Example of another type of relationship can be word pairs big - biggest and small - smallest [20]. We further denote two pairs of words with the same relationship as a question, as we can ask: ”What is the word that is similar to small in the same sense as biggest is similar to big?”\\n Somewhat surprisingly, these questions can be answered by performing simple algebraic operations with the vector representation of words. To find a word that is similar to small in the same sense as biggest is similar to big, we can simply compute vector X = vector(”biggest”)−vector(”big”)+ vector(”small”). Then, we search in the vector space for the word closest to X measured by cosine distance, and use it as the answer to the question (we discard the input question words during this search). When the word vectors are well trained, it is possible to find the correct answer (word smallest) using this method.\\n Finally, we found that when we train high dimensional word vectors on a large amount of data, the resulting vectors can be used to answer very subtle semantic relationships between words, such as a city and the country it belongs to, e.g. France is to Paris as Germany is to Berlin. Word vectors with such semantic relationships could be used to improve many existing NLP applications, such as machine translation, information retrieval and question answering systems, and may enable other future applications yet to be invented.\\n Table 1: Examples of five types of semantic and nine types of syntactic questions in the SemanticSyntactic Word Relationship test set.\\n Type of relationship\\tWord Pair 1\\tWord Pair 2\\n Common capital city\\tAthens\\tGreece\\tOslo\\tNorway\\n All capital cities\\tAstana\\tKazakhstan\\tHarare\\tZimbabwe\\n Currency\\tAngola\\tkwanza\\tIran\\trial\\n City-in-state\\tChicago\\tIllinois\\tStockton\\tCalifornia\\n Man-Woman\\tbrother\\tsister\\tgrandson\\tgranddaughter\\n Adjective to adverb\\tapparent\\tapparently\\trapid\\trapidly\\n Opposite\\tpossibly\\timpossibly\\tethical\\tunethical\\n Comparative\\tgreat\\tgreater\\ttough\\ttougher\\n Superlative\\teasy\\teasiest\\tlucky\\tluckiest\\n Present Participle\\tthink\\tthinking\\tread\\treading\\n Nationality adjective\\tSwitzerland\\tSwiss\\tCambodia\\tCambodian\\n Past tense\\twalking\\twalked\\tswimming\\tswam\\n Plural nouns\\tmouse\\tmice\\tdollar\\tdollars\\n Plural verbs\\twork\\tworks\\tspeak\\tspeaks\\n 4.1\\tTask Description\\n To measure quality of the word vectors, we define a comprehensive test set that contains five types of semantic questions, and nine types of syntactic questions. Two examples from each category are shown in Table 1. Overall, there are 8869 semantic and 10675 syntactic questions. The questions in each category were created in two steps: first, a list of similar word pairs was created manually. Then, a large list of questions is formed by connecting two word pairs. For example, we made a list of 68 large American cities and the states they belong to, and formed about 2.5K questions by picking two word pairs at random. We have included in our test set only single token words, thus multi-word entities are not present (such as New York).\\n We evaluate the overall accuracy for all question types, and for each question type separately (semantic, syntactic). Question is assumed to be correctly answered only if the closest word to the vector computed using the above method is exactly the same as the correct word in the question; synonyms are thus counted as mistakes. This also means that reaching 100% accuracy is likely to be impossible, as the current models do not have any input information about word morphology. However, we believe that usefulness of the word vectors for certain applications should be positively correlated with this accuracy metric. Further progress can be achieved by incorporating information about structure of words, especially for the syntactic questions.\\n 4.2\\tMaximization of Accuracy\\n We have used a Google News corpus for training the word vectors. This corpus contains about 6B tokens. We have restricted the vocabulary size to 1 million most frequent words. Clearly, we are facing time constrained optimization problem, as it can be expected that both using more data and higher dimensional word vectors will improve the accuracy. To estimate the best choice of model architecture for obtaining as good as possible results quickly, we have first evaluated models trained on subsets of the training data, with vocabulary restricted to the most frequent 30k words. The results using the CBOW architecture with different choice of word vector dimensionality and increasing amount of the training data are shown in Table 2.\\n It can be seen that after some point, adding more dimensions or adding more training data provides diminishing improvements. So, we have to increase both vector dimensionality and the amount of the training data together. While this observation might seem trivial, it must be noted that it is currently popular to train word vectors on relatively large amounts of data, but with insufficient size Table 2: Accuracy on subset of the Semantic-Syntactic Word Relationship test set, using word vectors from the CBOW architecture with limited vocabulary. Only questions containing words from the most frequent 30k words are used.\\n Dimensionality / Training words\\t24M\\t49M\\t98M\\t196M\\t391M\\t783M\\n 50\\t13.4\\t15.7\\t18.6\\t19.1\\t22.5\\t23.2\\n 100\\t19.4\\t23.1\\t27.8\\t28.7\\t33.4\\t32.2\\n 300\\t23.2\\t29.2\\t35.3\\t38.6\\t43.7\\t45.9\\n 600\\t24.0\\t30.1\\t36.5\\t40.8\\t46.6\\t50.4\\n Table 3: Comparison of architectures using models trained on the same data, with 640-dimensional word vectors. The accuracies are reported on our Semantic-Syntactic Word Relationship test set, and on the syntactic relationship test set of [20]\\n Model\\n Architecture\\tSemantic-Syntactic Word Relationship test set\\tMSR Word Relatedness\\n Test Set [20]\\n \\tSemantic Accuracy [%]\\tSyntactic Accuracy [%]\\t\\n RNNLM\\t9\\t36\\t35\\n NNLM\\t23\\t53\\t47\\n CBOW\\t24\\t64\\t61\\n Skip-gram\\t55\\t59\\t56\\n (such as 50 - 100). Given Equation 4, increasing amount of training data twice results in about the same increase of computational complexity as increasing vector size twice.\\n For the experiments reported in Tables 2 and 4, we used three training epochs with stochastic gradient descent and backpropagation. We chose starting learning rate 0.025 and decreased it linearly, so that it approaches zero at the end of the last training epoch.\\n 4.3\\tComparison of Model Architectures\\n First we compare different model architectures for deriving the word vectors using the same training data and using the same dimensionality of 640 of the word vectors. In the further experiments, we use full set of questions in the new Semantic-Syntactic Word Relationship test set, i.e. unrestricted to the 30k vocabulary. We also include results on a test set introduced in [20] that focuses on syntactic similarity between words .\\n The training data consists of several LDC corpora and is described in detail in [18] (320M words, 82K vocabulary). We used these data to provide a comparison to a previously trained recurrent neural network language model that took about 8 weeks to train on a single CPU. We trained a feedforward NNLM with the same number of 640 hidden units using the DistBelief parallel training [6], using a history of 8 previous words (thus, the NNLM has more parameters than the RNNLM, as the projection layer has size 640 × 8).\\n In Table 3, it can be seen that the word vectors from the RNN (as used in [20]) perform well mostly on the syntactic questions. The NNLM vectors perform significantly better than the RNN - this is not surprising, as the word vectors in the RNNLM are directly connected to a non-linear hidden layer. The CBOW architecture works better than the NNLM on the syntactic tasks, and about the same on the semantic one. Finally, the Skip-gram architecture works slightly worse on the syntactic task than the CBOW model (but still better than the NNLM), and much better on the semantic part of the test than all the other models.\\n Next, we evaluated our models trained using one CPU only and compared the results against publicly available word vectors. The comparison is given in Table 4. The CBOW model was trained on subset Table 4: Comparison of publicly available word vectors on the Semantic-Syntactic Word Relationship test set, and word vectors from our models. Full vocabularies are used.\\n Model\\tVector\\tTraining\\tAccuracy [%]\\t\\n \\tDimensionality\\twords\\t\\t\\n \\t\\t\\tSemantic\\tSyntactic\\tTotal\\n Collobert-Weston NNLM\\t50\\t660M\\t9.3\\t12.3\\t11.0\\n Turian NNLM\\t50\\t37M\\t1.4\\t2.6\\t2.1\\n Turian NNLM\\t200\\t37M\\t1.4\\t2.2\\t1.8\\n Mnih NNLM\\t50\\t37M\\t1.8\\t9.1\\t5.8\\n Mnih NNLM\\t100\\t37M\\t3.3\\t13.2\\t8.8\\n Mikolov RNNLM\\t80\\t320M\\t4.9\\t18.4\\t12.7\\n Mikolov RNNLM\\t640\\t320M\\t8.6\\t36.5\\t24.6\\n Huang NNLM\\t50\\t990M\\t13.3\\t11.6\\t12.3\\n Our NNLM\\t20\\t6B\\t12.9\\t26.4\\t20.3\\n Our NNLM\\t50\\t6B\\t27.9\\t55.8\\t43.2\\n Our NNLM\\t100\\t6B\\t34.2\\t64.5\\t50.8\\n CBOW\\t300\\t783M\\t15.5\\t53.1\\t36.1\\n Skip-gram\\t300\\t783M\\t50.0\\t55.9\\t53.3\\n Table 5: Comparison of models trained for three epochs on the same data and models trained for one epoch. Accuracy is reported on the full Semantic-Syntactic data set.\\n Model\\tVector\\tTraining\\tAccuracy [%]\\t\\tTraining time\\n \\tDimensionality\\twords\\t\\t\\t[days]\\n \\t\\t\\tSemantic\\tSyntactic\\tTotal\\t\\n 3 epoch CBOW\\t300\\t783M\\t15.5\\t53.1\\t36.1\\t1\\n 3 epoch Skip-gram\\t300\\t783M\\t50.0\\t55.9\\t53.3\\t3\\n 1 epoch CBOW\\t300\\t783M\\t13.8\\t49.9\\t33.6\\t0.3\\n 1 epoch CBOW\\t300\\t1.6B\\t16.1\\t52.6\\t36.1\\t0.6\\n 1 epoch CBOW\\t600\\t783M\\t15.4\\t53.3\\t36.2\\t0.7\\n 1 epoch Skip-gram\\t300\\t783M\\t45.6\\t52.2\\t49.2\\t1\\n 1 epoch Skip-gram\\t300\\t1.6B\\t52.2\\t55.1\\t53.8\\t2\\n 1 epoch Skip-gram\\t600\\t783M\\t56.7\\t54.5\\t55.5\\t2.5\\n of the Google News data in about a day, while training time for the Skip-gram model was about three days.\\n For experiments reported further, we used just one training epoch (again, we decrease the learning rate linearly so that it approaches zero at the end of training). Training a model on twice as much data using one epoch gives comparable or better results than iterating over the same data for three epochs, as is shown in Table 5, and provides additional small speedup.\\n 4.4\\tLarge Scale Parallel Training of Models\\n As mentioned earlier, we have implemented various models in a distributed framework called DistBelief. Below we report the results of several models trained on the Google News 6B data set, with mini-batch asynchronous gradient descent and the adaptive learning rate procedure called Adagrad [7]. We used 50 to 100 model replicas during the training. The number of CPU cores is an Table 6: Comparison of models trained using the DistBelief distributed framework. Note that training of NNLM with 1000-dimensional vectors would take too long to complete.\\n Model\\tVector\\tTraining\\tAccuracy [%]\\t\\tTraining time\\n \\tDimensionality\\twords\\t\\t\\t[days x CPU cores]\\n \\t\\t\\tSemantic\\tSyntactic\\tTotal\\t\\n NNLM\\t100\\t6B\\t34.2\\t64.5\\t50.8\\t14 x 180\\n CBOW\\t1000\\t6B\\t57.3\\t68.9\\t63.7\\t2 x 140\\n Skip-gram\\t1000\\t6B\\t66.1\\t65.1\\t65.6\\t2.5 x 125\\n Table 7: Comparison and combination of models on the Microsoft Sentence Completion Challenge.\\n Architecture\\tAccuracy [%]\\n 4-gram [32]\\t39\\n Average LSA similarity [32]\\t49\\n Log-bilinear model [24]\\t54.8\\n RNNLMs [19]\\t55.4\\n Skip-gram\\t48.0\\n Skip-gram + RNNLMs\\t58.9\\n estimate since the data center machines are shared with other production tasks, and the usage can fluctuate quite a bit. Note that due to the overhead of the distributed framework, the CPU usage of the CBOW model and the Skip-gram model are much closer to each other than their single-machine implementations. The result are reported in Table 6.\\n 4.5\\tMicrosoft Research Sentence Completion Challenge\\n The Microsoft Sentence Completion Challenge has been recently introduced as a task for advancing language modeling and other NLP techniques [32]. This task consists of 1040 sentences, where one word is missing in each sentence and the goal is to select word that is the most coherent with the rest of the sentence, given a list of five reasonable choices. Performance of several techniques has been already reported on this set, including N-gram models, LSA-based model [32], log-bilinear model [24] and a combination of recurrent neural networks that currently holds the state of the art performance of 55.4% accuracy on this benchmark [19].\\n We have explored the performance of Skip-gram architecture on this task. First, we train the 640dimensional model on 50M words provided in [32]. Then, we compute score of each sentence in the test set by using the unknown word at the input, and predict all surrounding words in a sentence. The final sentence score is then the sum of these individual predictions. Using the sentence scores, we choose the most likely sentence.\\n A short summary of some previous results together with the new results is presented in Table 7. While the Skip-gram model itself does not perform on this task better than LSA similarity, the scores from this model are complementary to scores obtained with RNNLMs, and a weighted combination leads to a new state of the art result 58.9% accuracy (59.2% on the development part of the set and 58.7% on the test part of the set).\\n 5\\tExamples of the Learned Relationships\\n Table 8 shows words that follow various relationships. We follow the approach described above: the relationship is defined by subtracting two word vectors, and the result is added to another word. Thus for example, Paris - France + Italy = Rome. As it can be seen, accuracy is quite good, although there is clearly a lot of room for further improvements (note that using our accuracy metric that Table 8: Examples of the word pair relationships, using the best word vectors from Table 4 (Skipgram model trained on 783M words with 300 dimensionality).\\n Relationship\\tExample 1\\tExample 2\\tExample 3\\n France - Paris\\tItaly: Rome\\tJapan: Tokyo\\tFlorida: Tallahassee\\n big - bigger\\tsmall: larger\\tcold: colder\\tquick: quicker\\n Miami - Florida\\tBaltimore: Maryland\\tDallas: Texas\\tKona: Hawaii\\n Einstein - scientist\\tMessi: midfielder\\tMozart: violinist\\tPicasso: painter\\n Sarkozy - France\\tBerlusconi: Italy\\tMerkel: Germany\\tKoizumi: Japan\\n copper - Cu\\tzinc: Zn\\tgold: Au\\turanium: plutonium\\n Berlusconi - Silvio\\tSarkozy: Nicolas\\tPutin: Medvedev\\tObama: Barack\\n Microsoft - Windows\\tGoogle: Android\\tIBM: Linux\\tApple: iPhone\\n Microsoft - Ballmer\\tGoogle: Yahoo\\tIBM: McNealy\\tApple: Jobs\\n Japan - sushi\\tGermany: bratwurst\\tFrance: tapas\\tUSA: pizza\\n assumes exact match, the results in Table 8 would score only about 60%). We believe that word vectors trained on even larger data sets with larger dimensionality will perform significantly better, and will enable the development of new innovative applications. Another way to improve accuracy is to provide more than one example of the relationship. By using ten examples instead of one to form the relationship vector (we average the individual vectors together), we have observed improvement of accuracy of our best models by about 10% absolutely on the semantic-syntactic test.\\n It is also possible to apply the vector operations to solve different tasks. For example, we have observed good accuracy for selecting out-of-the-list words, by computing average vector for a list of words, and finding the most distant word vector. This is a popular type of problems in certain human intelligence tests. Clearly, there is still a lot of discoveries to be made using these techniques.\\n 6\\tConclusion\\n In this paper we studied the quality of vector representations of words derived by various models on a collection of syntactic and semantic language tasks. We observed that it is possible to train high quality word vectors using very simple model architectures, compared to the popular neural network models (both feedforward and recurrent). Because of the much lower computational complexity, it is possible to compute very accurate high dimensional word vectors from a much larger data set. Using the DistBelief distributed framework, it should be possible to train the CBOW and Skip-gram models even on corpora with one trillion words, for basically unlimited size of the vocabulary. That is several orders of magnitude larger than the best previously published results for similar models.\\n An interesting task where the word vectors have recently been shown to significantly outperform the previous state of the art is the SemEval-2012 Task 2 [11]. The publicly available RNN vectors were used together with other techniques to achieve over 50% increase in Spearman’s rank correlation over the previous best result [31]. The neural network based word vectors were previously applied to many other NLP tasks, for example sentiment analysis [12] and paraphrase detection [28]. It can be expected that these applications can benefit from the model architectures described in this paper.\\n Our ongoing work shows that the word vectors can be successfully applied to automatic extension of facts in Knowledge Bases, and also for verification of correctness of existing facts. Results from machine translation experiments also look very promising. In the future, it would be also interesting to compare our techniques to Latent Relational Analysis [30] and others. We believe that our comprehensive test set will help the research community to improve the existing techniques for estimating the word vectors. We also expect that high quality word vectors will become an important building block for future NLP applications.\\n 7\\tFollow-Up Work\\n After the initial version of this paper was written, we published single-machine multi-threaded C++ code for computing the word vectors, using both the continuous bag-of-words and skip-gram architectures . The training speed is significantly higher than reported earlier in this paper, i.e. it is in the order of billions of words per hour for typical hyperparameter choices. We also published more than 1.4 million vectors that represent named entities, trained on more than 100 billion words. Some of our follow-up work will be published in an upcoming NIPS 2013 paper [21].\\n References\\n [1]\\tY. Bengio, R. Ducharme, P. Vincent. A neural probabilistic language model. Journal of Machine Learning Research, 3:1137-1155, 2003.\\n [2]\\tY. Bengio, Y. LeCun. Scaling learning algorithms towards AI. In: Large-Scale Kernel Machines, MIT Press, 2007.\\n [3]\\tT. Brants, A. C. Popat, P. Xu, F. J. Och, and J. Dean. Large language models in machine translation. In Proceedings of the Joint Conference on Empirical Methods in Natural Language Processing and Computational Language Learning, 2007.\\n [4]\\tR. Collobert and J. Weston. A Unified Architecture for Natural Language Processing: Deep Neural Networks with Multitask Learning. In International Conference on Machine Learning, ICML, 2008.\\n [5]\\tR. Collobert, J. Weston, L. Bottou, M. Karlen, K. Kavukcuoglu and P. Kuksa. Natural Language Processing (Almost) from Scratch. Journal of Machine Learning Research, 12:24932537, 2011.\\n [6]\\tJ. Dean, G.S. Corrado, R. Monga, K. Chen, M. Devin, Q.V. Le, M.Z. Mao, M.A. Ranzato, A. Senior, P. Tucker, K. Yang, A. Y. Ng., Large Scale Distributed Deep Networks, NIPS, 2012.\\n [7]\\tJ.C. Duchi, E. Hazan, and Y. Singer. Adaptive subgradient methods for online learning and stochastic optimization. Journal of Machine Learning Research, 2011.\\n [8]\\tJ. Elman. Finding Structure in Time. Cognitive Science, 14, 179-211, 1990.\\n [9]\\tEric H. Huang, R. Socher, C. D. Manning and Andrew Y. Ng. Improving Word Representations via Global Context and Multiple Word Prototypes. In: Proc. Association for Computational Linguistics, 2012.\\n [10]\\tG.E. Hinton, J.L. McClelland, D.E. Rumelhart. Distributed representations. In: Parallel distributed processing: Explorations in the microstructure of cognition. Volume 1: Foundations, MIT Press, 1986.\\n [11]\\tD.A. Jurgens, S.M. Mohammad, P.D. Turney, K.J. Holyoak. Semeval-2012 task 2: Measuring degrees of relational similarity. In: Proceedings of the 6th International Workshop on Semantic Evaluation (SemEval 2012), 2012.\\n [12]\\tA.L. Maas, R.E. Daly, P.T. Pham, D. Huang, A.Y. Ng, and C. Potts. Learning word vectors for sentiment analysis. In Proceedings of ACL, 2011.\\n [13]\\tT. Mikolov. Language Modeling for Speech Recognition in Czech, Masters thesis, Brno University of Technology, 2007.\\n [14]\\tT. Mikolov, J. Kopecky, L. Burget, O. Glembek and J.´ Cernockˇ y. Neural network based lan-´ guage models for higly inflective languages, In: Proc. ICASSP 2009.\\n [15]\\tT. Mikolov, M. Karafiat, L. Burget, J.´ Cernockˇ y, S. Khudanpur. Recurrent neural network´ based language model, In: Proceedings of Interspeech, 2010.\\n [16]\\tT. Mikolov, S. Kombrink, L. Burget, J. Cernockˇ y, S. Khudanpur. Extensions of recurrent neural´ network language model, In: Proceedings of ICASSP 2011.\\n [17]\\tT. Mikolov, A. Deoras, S. Kombrink, L. Burget, J. Cernockˇ\\ty. Empirical Evaluation and Com-´ bination of Advanced Language Modeling Techniques, In: Proceedings of Interspeech, 2011.\\n [18]\\tT. Mikolov, A. Deoras, D. Povey, L. Burget, J. Cernockˇ y. Strategies for Training Large Scale´ Neural Network Language Models, In: Proc. Automatic Speech Recognition and Understanding, 2011.\\n [19]\\tT. Mikolov. Statistical Language Models based on Neural Networks. PhD thesis, Brno University of Technology, 2012.\\n [20]\\tT. Mikolov, W.T. Yih, G. Zweig. Linguistic Regularities in Continuous Space Word Representations. NAACL HLT 2013.\\n [21]\\tT. Mikolov, I. Sutskever, K. Chen, G. Corrado, and J. Dean. Distributed Representations of Words and Phrases and their Compositionality. Accepted to NIPS 2013.\\n [22]\\tA. Mnih, G. Hinton. Three new graphical models for statistical language modelling. ICML, 2007.\\n [23]\\tA. Mnih, G. Hinton. A Scalable Hierarchical Distributed Language Model. Advances in Neural Information Processing Systems 21, MIT Press, 2009.\\n [24]\\tA. Mnih, Y.W. Teh. A fast and simple algorithm for training neural probabilistic language models. ICML, 2012.\\n [25]\\tF. Morin, Y. Bengio. Hierarchical Probabilistic Neural Network Language Model. AISTATS, 2005.\\n [26]\\tD. E. Rumelhart, G. E. Hinton, R. J. Williams. Learning internal representations by backpropagating errors. Nature, 323:533.536, 1986.\\n [27]\\tH. Schwenk. Continuous space language models. Computer Speech and Language, vol. 21, 2007.\\n [28]\\tR. Socher, E.H. Huang, J. Pennington, A.Y. Ng, and C.D. Manning. Dynamic Pooling and Unfolding Recursive Autoencoders for Paraphrase Detection. In NIPS, 2011.\\n [29]\\tJ. Turian, L. Ratinov, Y. Bengio. Word Representations: A Simple and General Method for Semi-Supervised Learning. In: Proc. Association for Computational Linguistics, 2010.\\n [30]\\tP. D. Turney. Measuring Semantic Similarity by Latent Relational Analysis. In: Proc. International Joint Conference on Artificial Intelligence, 2005.\\n [31]\\tA. Zhila, W.T. Yih, C. Meek, G. Zweig, T. Mikolov. Combining Heterogeneous Models for Measuring Relational Similarity. NAACL HLT 2013.\\n [32]\\tG. Zweig, C.J.C. Burges. The Microsoft Research Sentence Completion Challenge, Microsoft Research Technical Report MSR-TR-2011-129, 2011.\\n']\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}